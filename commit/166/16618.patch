From a593dc40e2b0f23d20b64f4bb1c0905f05f07f08 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:49:00 +0000
Subject: [PATCH] feat: Squash PR #16618 changes

---
 .github/workflows/pr-test-rust.yml            |  2 +-
 sgl-model-gateway/e2e_test/conftest.py        | 39 ++++++++++++-------
 .../e2e_test/infra/gpu_allocator.py           | 30 +++++++++-----
 .../e2e_test/infra/model_pool.py              | 17 +++++---
 4 files changed, 58 insertions(+), 30 deletions(-)

diff --git a/.github/workflows/pr-test-rust.yml b/.github/workflows/pr-test-rust.yml
index 7f2e94bf1..5218123d2 100644
--- a/.github/workflows/pr-test-rust.yml
+++ b/.github/workflows/pr-test-rust.yml
@@ -186,7 +186,7 @@ jobs:
             extra_deps: ""
             env_vars: "SHOW_ROUTER_LOGS=1"
             reruns: "--reruns 3 --reruns-delay 2"
-          - name: router-embeddings
+          - name: e2e
             timeout: 45
             test_dirs: "e2e_test/router e2e_test/embeddings"
             extra_deps: ""
diff --git a/sgl-model-gateway/e2e_test/conftest.py b/sgl-model-gateway/e2e_test/conftest.py
index 96eac4db5..8d3de4dfd 100644
--- a/sgl-model-gateway/e2e_test/conftest.py
+++ b/sgl-model-gateway/e2e_test/conftest.py
@@ -181,9 +181,9 @@ from infra import (
 # Test collection: scan for required backends
 # ---------------------------------------------------------------------------
 
-# Global storage for scanned requirements
+# Global storage for scanned requirements (ordered by test collection order)
 _scanned_backends: set[str] = set()  # {"grpc", "http", "openai", ...}
-_scanned_models: set[str] = set()  # Models needed by tests
+_scanned_models: list[str] = []  # Models needed by tests, in test order
 _needs_default_model: bool = False  # True if any e2e test lacks explicit model marker
 
 
@@ -215,16 +215,19 @@ def pytest_collection_modifyitems(
                         _scanned_backends.update(param_values)
 
                 elif param_name == PARAM_MODEL or PARAM_MODEL in param_name:
-                    # Extract model names from parametrize
+                    # Extract model names from parametrize (preserve order)
                     if isinstance(param_values, (list, tuple)):
-                        _scanned_models.update(param_values)
+                        for model in param_values:
+                            if model not in _scanned_models:
+                                _scanned_models.append(model)
                         has_model_marker = True
 
         # Check for @pytest.mark.model("name") markers
         model_marker = item.get_closest_marker(PARAM_MODEL)
         if model_marker and model_marker.args:
             model_name = model_marker.args[0]
-            _scanned_models.add(model_name)
+            if model_name not in _scanned_models:
+                _scanned_models.append(model_name)
             has_model_marker = True
 
         # Check if this is an e2e test without an explicit model marker
@@ -233,9 +236,9 @@ def pytest_collection_modifyitems(
             _needs_default_model = True
 
     logger.info(
-        "Scanned test requirements - backends: %s, models: %s, needs default: %s",
+        "Scanned test requirements - backends: %s, models (in test order): %s, needs default: %s",
         _scanned_backends or {"(none)"},
-        _scanned_models or {"(none)"},
+        _scanned_models or ["(none)"],
         _needs_default_model,
     )
 
@@ -245,32 +248,38 @@ def get_pool_requirements() -> list[tuple[str, ConnectionMode]]:
 
     Returns:
         List of (model_id, ConnectionMode) tuples to try to pre-launch.
-        Models that don't fit will be launched on-demand.
+        Models are ordered by first appearance in test collection order,
+        so models needed by earlier tests are launched first.
     """
-    models = set(_scanned_models)
+    # Preserve order from test collection (list, not set)
+    models = list(_scanned_models)
 
     # Add DEFAULT_MODEL if any e2e test lacks an explicit model marker,
     # or if no models were specified at all
-    if _needs_default_model or not models:
-        models.add(DEFAULT_MODEL)
+    if _needs_default_model and DEFAULT_MODEL not in models:
+        # Prepend default model since tests without explicit markers run first
+        models.insert(0, DEFAULT_MODEL)
+    elif not models:
+        models = [DEFAULT_MODEL]
 
     # Convert scanned string backends to ConnectionMode enums
     # Filter to local backends only (grpc, http) - cloud backends don't need workers
-    local_modes: set[ConnectionMode] = set()
+    local_modes: list[ConnectionMode] = []
     for backend in _scanned_backends:
         try:
             mode = ConnectionMode(backend)
-            if mode in LOCAL_MODES:
-                local_modes.add(mode)
+            if mode in LOCAL_MODES and mode not in local_modes:
+                local_modes.append(mode)
         except ValueError:
             # Not a ConnectionMode (e.g., "openai", "xai", "pd") - skip
             pass
 
     # Default to HTTP if no local backends specified
     if not local_modes:
-        local_modes = {ConnectionMode.HTTP}
+        local_modes = [ConnectionMode.HTTP]
 
     # Build requirements: each model needs each mode
+    # Order: model order takes priority (models used first are launched first)
     requirements: list[tuple[str, ConnectionMode]] = []
     for model in models:
         for mode in local_modes:
diff --git a/sgl-model-gateway/e2e_test/infra/gpu_allocator.py b/sgl-model-gateway/e2e_test/infra/gpu_allocator.py
index 9e5d63371..c6a690dc4 100644
--- a/sgl-model-gateway/e2e_test/infra/gpu_allocator.py
+++ b/sgl-model-gateway/e2e_test/infra/gpu_allocator.py
@@ -244,19 +244,27 @@ class GPUAllocator:
             logger.warning("Failed to detect GPUs: %s", e)
             return []
 
-    def allocate_slots(self, model_specs: dict[str, dict]) -> list[GPUSlot]:
+    def allocate_slots(
+        self, model_specs: dict[str, dict], preserve_order: bool = False
+    ) -> list[GPUSlot]:
         """Allocate GPU slots based on model memory requirements.
 
-        Uses a first-fit decreasing bin-packing algorithm:
+        Uses a first-fit decreasing bin-packing algorithm by default:
         1. Sort models by memory requirement (largest first)
         2. For each model, find the first GPU(s) that can fit it
         3. For multi-GPU models, find consecutive GPUs
 
+        When preserve_order=True, processes models in dict insertion order
+        (test collection order) instead of sorting by memory. This ensures
+        models needed by earlier tests are allocated first.
+
         Note: This method tracks used GPUs across multiple calls, so subsequent
         allocations will use different GPUs than previous ones.
 
         Args:
             model_specs: Dict of model_id -> spec dict with 'memory_gb' and 'tp' keys
+            preserve_order: If True, allocate in dict order (test order) instead
+                           of sorting by memory size. Default False.
 
         Returns:
             List of GPUSlots with assigned models (only the newly allocated slots)
@@ -265,17 +273,21 @@ class GPUAllocator:
             logger.warning("No GPUs available for allocation")
             return []
 
-        # Sort models by memory requirement (largest first for better packing)
-        sorted_models = sorted(
-            model_specs.items(),
-            key=lambda x: x[1].get("memory_gb", 0),
-            reverse=True,
-        )
+        if preserve_order:
+            # Process in dict insertion order (test collection order)
+            ordered_models = list(model_specs.items())
+        else:
+            # Sort models by memory requirement (largest first for better packing)
+            ordered_models = sorted(
+                model_specs.items(),
+                key=lambda x: x[1].get("memory_gb", 0),
+                reverse=True,
+            )
 
         # Track new slots allocated in this call
         new_slots: list[GPUSlot] = []
 
-        for model_id, spec in sorted_models:
+        for model_id, spec in ordered_models:
             memory_gb = spec.get("memory_gb", 16)
             tp_size = spec.get("tp", 1)
 
diff --git a/sgl-model-gateway/e2e_test/infra/model_pool.py b/sgl-model-gateway/e2e_test/infra/model_pool.py
index 5b01abf12..891008776 100644
--- a/sgl-model-gateway/e2e_test/infra/model_pool.py
+++ b/sgl-model-gateway/e2e_test/infra/model_pool.py
@@ -220,10 +220,17 @@ class ModelPool:
         if requirements is None:
             requirements = [(DEFAULT_MODEL, ConnectionMode.HTTP)]
 
-        # Deduplicate and validate
-        requirements = list(set(requirements))
+        # Deduplicate while preserving order (first occurrence wins)
+        seen: set[tuple[str, ConnectionMode]] = set()
+        unique_requirements = []
+        for req in requirements:
+            if req not in seen:
+                seen.add(req)
+                unique_requirements.append(req)
+
+        # Validate
         valid_requirements = []
-        for model_id, mode in requirements:
+        for model_id, mode in unique_requirements:
             if model_id not in MODEL_SPECS:
                 logger.warning("Unknown model %s, skipping", model_id)
                 continue
@@ -250,8 +257,8 @@ class ModelPool:
                 "tp": spec.get("tp", 1),
             }
 
-        # Allocate GPU slots
-        slots = self.allocator.allocate_slots(allocation_specs)
+        # Allocate GPU slots (preserve test order so earlier tests get models first)
+        slots = self.allocator.allocate_slots(allocation_specs, preserve_order=True)
 
         # Track which models got slots
         launched_keys = set()
-- 
2.52.0

