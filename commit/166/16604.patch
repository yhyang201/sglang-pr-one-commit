From e814aff0c747ed70976916e704b7f5e540400646 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:49:34 +0000
Subject: [PATCH] feat: Squash PR #16604 changes

---
 .../workflows/release-docker-npu-nightly.yml  |   3 +-
 .github/workflows/release-docker-npu.yml      |   3 +-
 docker/npu.Dockerfile                         |  13 +-
 docs/platforms/ascend_npu.md                  | 124 ++++-
 docs/platforms/ascend_npu_deepseek_example.md |  48 +-
 docs/platforms/ascend_npu_qwen3_examples.md   |  12 +-
 docs/platforms/ascend_npu_support.rst         |   2 +
 docs/platforms/ascend_npu_support_features.md | 400 +++++++++++++++
 docs/platforms/ascend_npu_support_models.md   |  97 ++++
 python/sglang/srt/configs/model_config.py     |   7 +
 python/sglang/srt/entrypoints/EngineBase.py   |   1 +
 python/sglang/srt/entrypoints/engine.py       |   4 +
 python/sglang/srt/entrypoints/http_server.py  |   6 +-
 .../sglang/srt/entrypoints/openai/protocol.py |   1 +
 .../srt/entrypoints/openai/serving_chat.py    |   2 +
 python/sglang/srt/environ.py                  |   3 +
 python/sglang/srt/eplb/async_eplb_manager.py  | 157 ++++++
 .../srt/hardware_backend/npu/allocator_npu.py |  18 +
 .../npu/attention/ascend_backend.py           | 454 +++++++++++++-----
 .../eagle_draft_npu_graph_runner.py           |  21 +-
 .../npu/graph_runner/npu_graph_runner.py      |  39 +-
 .../modules/deepseek_v2_attention_mla_npu.py  |  71 +--
 .../srt/hardware_backend/npu/moe/topk.py      |   2 +-
 python/sglang/srt/layers/activation.py        |   4 +
 .../layers/attention/torch_native_backend.py  | 126 ++++-
 python/sglang/srt/layers/attention/vision.py  |   2 +-
 .../srt/layers/moe/fused_moe_triton/layer.py  |   1 +
 python/sglang/srt/layers/rotary_embedding.py  |  51 +-
 .../srt/layers/vocab_parallel_embedding.py    |   4 +-
 .../srt/lora/backend/chunked_backend.py       |  21 +-
 .../sglang/srt/lora/backend/torch_backend.py  | 276 +++++------
 python/sglang/srt/lora/torch_ops/__init__.py  |   7 +-
 python/sglang/srt/lora/torch_ops/lora_ops.py  | 188 ++++----
 python/sglang/srt/lora/utils.py               |  29 ++
 .../srt/managers/data_parallel_controller.py  |  27 +-
 python/sglang/srt/managers/io_struct.py       |  12 +
 python/sglang/srt/managers/schedule_batch.py  |   2 +
 python/sglang/srt/managers/scheduler.py       |  97 +++-
 .../sglang/srt/managers/scheduler_enhancer.py |  26 +-
 .../srt/managers/scheduler_metrics_mixin.py   |   9 +-
 .../sglang/srt/managers/tokenizer_manager.py  |   1 +
 .../srt/model_executor/cuda_graph_runner.py   |  16 +
 .../sglang/srt/model_executor/model_runner.py |  29 +-
 python/sglang/srt/models/baichuan.py          | 116 +++--
 python/sglang/srt/models/dbrx.py              |  67 ++-
 python/sglang/srt/models/deepseek_v2.py       |   1 +
 python/sglang/srt/models/ernie4.py            |   1 +
 python/sglang/srt/models/glm4v.py             |   4 +-
 python/sglang/srt/models/grok.py              |  39 +-
 python/sglang/srt/models/llama.py             |  97 +++-
 python/sglang/srt/models/llama4.py            |   5 +
 python/sglang/srt/models/llama_eagle3.py      |  41 +-
 python/sglang/srt/models/minicpm3.py          |   2 +-
 python/sglang/srt/models/minicpmo.py          |   4 +-
 python/sglang/srt/models/minicpmv.py          |   6 +-
 python/sglang/srt/models/olmoe.py             |   1 +
 python/sglang/srt/models/phi.py               |  15 +-
 python/sglang/srt/models/qwen3.py             |  14 +-
 python/sglang/srt/models/qwen3_moe.py         |   8 +-
 python/sglang/srt/models/qwen3_next.py        |  24 +-
 python/sglang/srt/models/qwen3_omni_moe.py    |   5 +-
 python/sglang/srt/models/qwen3_vl.py          |   8 +-
 python/sglang/srt/models/xverse_moe.py        |  46 +-
 .../multimodal/processors/base_processor.py   |   2 +
 .../srt/parser/code_completion_parser.py      |   4 +-
 python/sglang/srt/server_args.py              | 124 ++++-
 .../srt/speculative/cpp_ngram/ngram.cpp       |  51 +-
 python/sglang/srt/speculative/ngram_info.py   |  18 +-
 python/sglang/srt/speculative/ngram_worker.py |  65 ++-
 python/sglang/srt/speculative/spec_info.py    |  16 +-
 .../srt/speculative/suffix_cache_adapter.py   | 340 +++++++++++++
 python/sglang/srt/speculative/suffix_info.py  | 109 +++++
 .../sglang/srt/speculative/suffix_worker.py   | 106 ++++
 python/sglang/srt/utils/common.py             |   5 +-
 python/sglang/test/lora_utils.py              |   2 +-
 python/sglang/test/runners.py                 |  14 +-
 python/sglang/test/test_utils.py              |   1 +
 scripts/ci/npu_ci_install_dependency.sh       |  21 +-
 .../benches/request_processing.rs             |   1 +
 .../bindings/python/sglang_router/mini_lb.py  |  44 +-
 .../bindings/python/sglang_router/router.py   |   2 +
 .../python/sglang_router/router_args.py       |  13 +
 sgl-model-gateway/bindings/python/src/lib.rs  |  10 +
 .../py_test/fixtures/router_manager.py        |   2 +
 .../load_balancing/test_power_of_two.py       |   2 +-
 sgl-model-gateway/src/app_context.rs          |   8 +-
 sgl-model-gateway/src/config/builder.rs       |   9 +
 sgl-model-gateway/src/config/types.rs         |   4 +
 sgl-model-gateway/src/core/worker_manager.rs  |  53 +-
 sgl-model-gateway/src/main.rs                 |   8 +
 sgl-model-gateway/src/policies/bucket.rs      |  16 +-
 sgl-model-gateway/src/policies/cache_aware.rs |  22 +-
 sgl-model-gateway/src/policies/mod.rs         | 158 +++++-
 .../src/policies/power_of_two.rs              |  16 +-
 sgl-model-gateway/src/policies/random.rs      |  24 +-
 sgl-model-gateway/src/policies/registry.rs    |  54 ++-
 sgl-model-gateway/src/policies/round_robin.rs |  25 +-
 sgl-model-gateway/src/protocols/generate.rs   |   3 +
 .../src/protocols/worker_spec.rs              |   2 +
 .../src/routers/http/pd_router.rs             |  71 +++
 .../tests/test_openai_routing.rs              |   2 +
 test/manual/lora/__init__.py                  |   6 +
 test/manual/lora/test_chunked_sgmv_backend.py | 305 ++++--------
 test/manual/lora/test_lora_cuda_graph.py      |   3 +-
 test/manual/lora/test_lora_ops.py             | 218 +++++++++
 test/manual/lora/test_lora_qwen3_vl.py        |   8 +-
 test/manual/lora/test_lora_spec_decoding.py   |   3 +-
 test/manual/lora/test_torch_backend.py        | 244 ++++++++++
 test/manual/lora/utils.py                     | 148 ++++++
 test/manual/test_lora_ops.py                  | 287 -----------
 test/manual/test_torch_backend.py             | 224 ---------
 111 files changed, 4222 insertions(+), 1571 deletions(-)
 create mode 100644 docs/platforms/ascend_npu_support_features.md
 create mode 100644 docs/platforms/ascend_npu_support_models.md
 create mode 100644 python/sglang/srt/eplb/async_eplb_manager.py
 create mode 100644 python/sglang/srt/speculative/suffix_cache_adapter.py
 create mode 100644 python/sglang/srt/speculative/suffix_info.py
 create mode 100644 python/sglang/srt/speculative/suffix_worker.py
 create mode 100644 test/manual/lora/__init__.py
 create mode 100644 test/manual/lora/test_lora_ops.py
 create mode 100644 test/manual/lora/test_torch_backend.py
 create mode 100644 test/manual/lora/utils.py
 delete mode 100644 test/manual/test_lora_ops.py
 delete mode 100644 test/manual/test_torch_backend.py

diff --git a/.github/workflows/release-docker-npu-nightly.yml b/.github/workflows/release-docker-npu-nightly.yml
index 1778038ac..f1c3c03fc 100644
--- a/.github/workflows/release-docker-npu-nightly.yml
+++ b/.github/workflows/release-docker-npu-nightly.yml
@@ -73,6 +73,7 @@ jobs:
           push: ${{ github.repository == 'sgl-project/sglang' && github.event_name != 'pull_request' }}
           provenance: false
           build-args: |
-            SGLANG_KERNEL_NPU_TAG=20251206
+            SGLANG_KERNEL_NPU_TAG=2025.12.29
             CANN_VERSION=${{ matrix.cann_version }}
             DEVICE_TYPE=${{ matrix.device_type }}
+            SGLANG_TAG=$GITHUB_REF_NAME
diff --git a/.github/workflows/release-docker-npu.yml b/.github/workflows/release-docker-npu.yml
index 804420743..a6d970b28 100644
--- a/.github/workflows/release-docker-npu.yml
+++ b/.github/workflows/release-docker-npu.yml
@@ -70,6 +70,7 @@ jobs:
           push: ${{ github.repository == 'sgl-project/sglang' && github.event_name != 'pull_request' }}
           provenance: false
           build-args: |
-            SGLANG_KERNEL_NPU_TAG=20251206
+            SGLANG_KERNEL_NPU_TAG=2025.12.29
             CANN_VERSION=${{ matrix.cann_version }}
             DEVICE_TYPE=${{ matrix.device_type }}
+            SGLANG_TAG=$GITHUB_REF_NAME
diff --git a/docker/npu.Dockerfile b/docker/npu.Dockerfile
index d028f4170..26eaee04e 100644
--- a/docker/npu.Dockerfile
+++ b/docker/npu.Dockerfile
@@ -10,7 +10,7 @@ ARG PIP_INDEX_URL="https://pypi.org/simple/"
 ARG APTMIRROR=""
 ARG PYTORCH_VERSION="2.8.0"
 ARG TORCHVISION_VERSION="0.23.0"
-ARG PTA_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/torch_npu-2.8.0.post2.dev20251113-cp311-cp311-manylinux_2_28_aarch64.whl"
+ARG PTA_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/torch_npu-2.8.0.post2.dev20251224-cp311-cp311-manylinux_2_28_aarch64.whl"
 ARG TRITON_ASCEND_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/triton_ascend/triton_ascend-3.2.0.dev2025112116-cp311-cp311-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl"
 ARG BISHENG_NAME="Ascend-BiSheng-toolkit_aarch64_20251121.run"
 ARG BISHENG_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/triton_ascend/${BISHENG_NAME}"
@@ -78,19 +78,20 @@ RUN git clone https://github.com/sgl-project/sglang --branch $SGLANG_TAG && \
 
 # Install Deep-ep
 # pin wheel to 0.45.1 ref: https://github.com/pypa/wheel/issues/662
-RUN ${PIP_INSTALL} wheel==0.45.1 && git clone --branch $SGLANG_KERNEL_NPU_TAG https://github.com/sgl-project/sgl-kernel-npu.git \
+RUN ${PIP_INSTALL} wheel==0.45.1 && mkdir sgl-kernel-npu \
     && export LD_LIBRARY_PATH=${ASCEND_CANN_PATH}/latest/runtime/lib64/stub:$LD_LIBRARY_PATH && \
     source ${ASCEND_CANN_PATH}/set_env.sh && \
     cd sgl-kernel-npu && \
-    bash build.sh \
+    wget https://github.com/sgl-project/sgl-kernel-npu/releases/download/${SGLANG_KERNEL_NPU_TAG}/sgl-kernel-npu_${SGLANG_KERNEL_NPU_TAG}_8.3.rc2_910b.zip \
+    && unzip sgl-kernel-npu_${SGLANG_KERNEL_NPU_TAG}_8.3.rc2_910b.zip \
     && ${PIP_INSTALL} output/deep_ep*.whl output/sgl_kernel_npu*.whl \
     && cd .. && rm -rf sgl-kernel-npu \
     && cd "$(python3 -m pip show deep-ep | awk '/^Location:/ {print $2}')" && ln -s deep_ep/deep_ep_cpp*.so
 
 # Install CustomOps
-RUN wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run && \
-    chmod a+x ./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run && \
-    ./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run --quiet --install-path=/usr/local/Ascend/ascend-toolkit/latest/opp && \
+RUN wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run && \
+    chmod a+x ./CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run && \
+    ./CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run --quiet --install-path=/usr/local/Ascend/ascend-toolkit/latest/opp && \
     wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl && \
     ${PIP_INSTALL} ./custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl
 
diff --git a/docs/platforms/ascend_npu.md b/docs/platforms/ascend_npu.md
index 95e889e7a..7baac89c9 100644
--- a/docs/platforms/ascend_npu.md
+++ b/docs/platforms/ascend_npu.md
@@ -3,7 +3,7 @@
 
 You can install SGLang using any of the methods below. Please go through `System Settings` section to ensure the clusters are roaring at max performance. Feel free to leave an issue [here at sglang](https://github.com/sgl-project/sglang/issues) if you encounter any issues or have any problems.
 
-## Installing SGLang
+## Preparing the Running Environment
 
 ### Method 1: Installing from source with prerequisites
 
@@ -31,6 +31,7 @@ pip install mf-adapter==1.0.0
 #### Pytorch and Pytorch Framework Adaptor on Ascend
 
 At the moment NPUGraph optimizations are supported only in `torch_npu==2.6.0.post3` that requires 'torch==2.6.0'.
+
 _TODO: NPUGraph optimizations will be supported in future releases of 'torch_npu' 2.7.1, 2.8.0 and 2.9.0_
 
 ```shell
@@ -41,7 +42,7 @@ pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --index-ur
 pip install torch_npu==$TORCH_NPU_VERSION
 ```
 
-While there is no resleased versions of 'torch_npu' for 'torch==2.7.1' and 'torch==2.8.0' we provide custom builds of 'torch_npu'. PLATFORM can be 'aarch64' or 'x86_64'
+While there is no released versions of 'torch_npu' for 'torch==2.7.1' and 'torch==2.8.0' we provide custom builds of 'torch_npu'. PLATFORM can be 'aarch64' or 'x86_64'
 
 ```shell
 PLATFORM="aarch64"
@@ -52,7 +53,7 @@ wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/torc
 pip install torch_npu-${PYTORCH_VERSION}.post2.dev20251120-cp311-cp311-manylinux_2_28_${PLATFORM}.whl
 ```
 
-If you are using other versions of 'torch' install 'torch_npu' from sources, check [installation guide](https://github.com/Ascend/pytorch/blob/master/README.md)
+If you are using other versions of `torch` and install `torch_npu`, check [installation guide](https://github.com/Ascend/pytorch/blob/master/README.md)
 
 #### Triton on Ascend
 
@@ -69,7 +70,7 @@ pip install triton-ascend==3.2.0rc4
 For installation of Triton on Ascend nightly builds or from sources, follow [installation guide](https://gitcode.com/Ascend/triton-ascend/blob/master/docs/sources/getting-started/installation.md)
 
 #### SGLang Kernels NPU
-We provide our own set of SGL kernels, check [installation guide](https://github.com/sgl-project/sgl-kernel-npu/blob/main/python/sgl_kernel_npu/README.md).
+We provide SGL kernels for Ascend NPU, check [installation guide](https://github.com/sgl-project/sgl-kernel-npu/blob/main/python/sgl_kernel_npu/README.md).
 
 #### DeepEP-compatible Library
 We provide a DeepEP-compatible Library as a drop-in replacement of deepseek-ai's DeepEP library, check the [installation guide](https://github.com/sgl-project/sgl-kernel-npu/blob/main/python/deep_ep/README.md).
@@ -97,12 +98,18 @@ mv python/pyproject_other.toml python/pyproject.toml
 pip install -e python[srt_npu]
 ```
 
-### Method 2: Using docker
-
-__Notice:__ `--privileged` and `--network=host` are required by RDMA, which is typically needed by Ascend NPU clusters.
-
-__Notice:__ The following docker command is based on Atlas 800I A3 machines. If you are using Atlas 800I A2, make sure only `davinci[0-7]` are mapped into container.
-
+### Method 2: Using Docker Image
+#### Obtain Image
+You can download the SGLang image or build an image based on Dockerfile to obtain the Ascend NPU image.
+1. Download SGLang image
+```angular2html
+dockerhub: docker.io/lmsysorg/sglang:$tag
+# Main-based tag, change main to specific version like v0.5.6,
+# you can get image for specific version
+Atlas 800I A3 : {main}-cann8.3.rc2-a3
+Atlas 800I A2: {main}-cann8.3.rc2-910b
+```
+2. Build an image based on Dockerfile
 ```shell
 # Clone the SGLang repository
 git clone https://github.com/sgl-project/sglang.git
@@ -110,6 +117,14 @@ cd sglang/docker
 
 # Build the docker image
 docker build -t <image_name> -f npu.Dockerfile .
+```
+
+#### Create Docker
+__Notice:__ `--privileged` and `--network=host` are required by RDMA, which is typically needed by Ascend NPU clusters.
+
+__Notice:__ The following docker command is based on Atlas 800I A3 machines. If you are using Atlas 800I A2, make sure only `davinci[0-7]` are mapped into container.
+
+```shell
 
 alias drun='docker run -it --rm --privileged --network=host --ipc=host --shm-size=16g \
     --device=/dev/davinci0 --device=/dev/davinci1 --device=/dev/davinci2 --device=/dev/davinci3 \
@@ -122,9 +137,10 @@ alias drun='docker run -it --rm --privileged --network=host --ipc=host --shm-siz
     --volume /etc/ascend_install.info:/etc/ascend_install.info \
     --volume /var/queue_schedule:/var/queue_schedule --volume ~/.cache/:/root/.cache/'
 
+# Add HF_TOKEN env for download model by SGLang.
 drun --env "HF_TOKEN=<secret>" \
     <image_name> \
-    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --attention-backend ascend --host 0.0.0.0 --port 30000
+    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --attention-backend ascend
 ```
 
 ## System Settings
@@ -140,13 +156,15 @@ echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governo
 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor # shows performance
 ```
 
-### Disable NUMA balancing
+### Disable NUMA balancing and enable cpu affinity
 
 ```shell
 sudo sysctl -w kernel.numa_balancing=0
-
 # Check
 cat /proc/sys/kernel/numa_balancing # shows 0
+
+# Enabling CPU Affinity
+export SGLANG_SET_CPU_AFFINITY=1
 ```
 
 ### Prevent swapping out system memory
@@ -157,3 +175,83 @@ sudo sysctl -w vm.swappiness=10
 # Check
 cat /proc/sys/vm/swappiness # shows 10
 ```
+
+## Running SGLang Service
+### Running Service For Large Language Models
+#### PD Mixed Scene
+```shell
+# Enabling CPU Affinity
+export SGLANG_SET_CPU_AFFINITY=1
+python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --attention-backend ascend
+```
+
+#### PD Separation Scene
+1. Launch Prefill Server
+```shell
+# Enabling CPU Affinity
+export SGLANG_SET_CPU_AFFINITY=1
+
+# PIP: recommended to config first Prefill Server IP
+# PORT: one free port
+# all sglang servers need to be config the same PIP and PORT,
+export ASCEND_MF_STORE_URL="tcp://PIP:PORT"
+# if you are Atlas 800I A2 hardware and use rdma for kv cache transfer, add this parameter
+export ASCEND_MF_TRANSFER_PROTOCOL="device_rdma"
+python3 -m sglang.launch_server \
+    --model-path meta-llama/Llama-3.1-8B-Instruct \
+    --disaggregation-mode prefill \
+    --disaggregation-transfer-backend ascend \
+    --disaggregation-bootstrap-port 8995 \
+    --attention-backend ascend \
+    --device npu \
+    --base-gpu-id 0 \
+    --tp-size 1 \
+```
+
+2. Launch Decode Server
+```shell
+# PIP: recommended to config first Prefill Server IP
+# PORT: one free port
+# all sglang servers need to be config the same PIP and PORT,
+export ASCEND_MF_STORE_URL="tcp://PIP:PORT"
+# if you are Atlas 800I A2 hardware and use rdma for kv cache transfer, add this parameter
+export ASCEND_MF_TRANSFER_PROTOCOL="device_rdma"
+python3 -m sglang.launch_server \
+    --model-path meta-llama/Llama-3.1-8B-Instruct \
+    --disaggregation-mode decode \
+    --disaggregation-transfer-backend ascend \
+    --attention-backend ascend \
+    --device npu \
+    --base-gpu-id 1 \
+    --tp-size 1 \
+    --host 127.0.0.1 \
+    --port 8001
+```
+
+3. Launch Router
+```shell
+python3 -m sglang_router.launch_router \
+    --pd-disaggregation \
+    --policy cache_aware \
+    --prefill http://127.0.0.1:8000 8995 \
+    --decode http://127.0.0.1:8001 \
+    --host 127.0.0.1 \
+    --port 6688
+```
+
+### Running Service For Multimodal Language Models
+#### PD Mixed Scene
+```shell
+python3 -m sglang.launch_server \
+    --model-path Qwen3-VL-30B-A3B-Instruct \
+    --host 127.0.0.1 \
+    --port 8000 \
+    --tp 4 \
+    --device npu \
+    --attention-backend ascend \
+    --mm-attention-backend ascend_attn \
+    --disable-radix-cache \
+    --trust-remote-code \
+    --enable-multimodal \
+    --sampling-backend ascend
+```
diff --git a/docs/platforms/ascend_npu_deepseek_example.md b/docs/platforms/ascend_npu_deepseek_example.md
index acb864ef5..101b971da 100644
--- a/docs/platforms/ascend_npu_deepseek_example.md
+++ b/docs/platforms/ascend_npu_deepseek_example.md
@@ -2,7 +2,7 @@
 
 ### Running DeepSeek-V3
 
-#### Running DeepSeek on 1 x Atlas 800I A3.
+#### Running DeepSeek in PD mixed mode on 1 x Atlas 800I A3.
 
 W4A8 Model weights could be found [here](https://modelers.cn/models/Modelers_Park/DeepSeek-R1-0528-w4a8).
 
@@ -32,15 +32,13 @@ python3 -m sglang.launch_server \
     --device npu \
     --quantization modelslim \
     --watchdog-timeout 9000 \
-    --host 127.0.0.1 \
-    --port 6688 \
     --cuda-graph-bs 8 16 24 28 32 \
     --mem-fraction-static 0.68 \
     --max-running-requests 128 \
     --context-length 8188 \
     --disable-radix-cache \
     --chunked-prefill-size -1 \
-    --max-prefill-tokens 6000 \
+    --max-prefill-tokens 16384 \
     --moe-a2a-backend deepep \
     --deepep-mode auto \
     --enable-dp-attention \
@@ -53,17 +51,17 @@ python3 -m sglang.launch_server \
     --dtype bfloat16
 ```
 
-#### Running DeepSeek with PD disaggregation on 2 x Atlas 800I A3.
+#### Running DeepSeek with PD disaggregation mode on 2 x Atlas 800I A3.
 
 W4A8 Model weights could be found [here](https://modelers.cn/models/Modelers_Park/DeepSeek-R1-0528-w4a8).
 
-
-Prefill:
+1. Prefill:
 
 ```shell
 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
 export STREAMS_PER_DEVICE=32
-#PD
+
+#memfabric config store
 export ASCEND_MF_STORE_URL="tcp://<PREFILL_HOST_IP>:<PORT>"
 
 #Deepep communication settings
@@ -78,10 +76,11 @@ export TASK_QUEUE_ENABLE=2
 
 python -m sglang.launch_server \
     --model-path ${MODEL_PATH} \
-    --disaggregation-mode prefill \
     --host $PREFILL_HOST_IP \
     --port 8000 \
+    --disaggregation-mode prefill \
     --disaggregation-bootstrap-port 8996 \
+    --disaggregation-transfer-backend ascend \
     --trust-remote-code \
     --nnodes 1 \
     --node-rank 0 \
@@ -90,7 +89,6 @@ python -m sglang.launch_server \
     --attention-backend ascend \
     --device npu \
     --quantization modelslim \
-    --disaggregation-transfer-backend ascend \
     --max-running-requests 8 \
     --context-length 8192 \
     --disable-radix-cache \
@@ -108,12 +106,13 @@ python -m sglang.launch_server \
     --dtype bfloat16
 ```
 
-Decode:
+2. Decode:
 
 ```shell
 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
 export STREAMS_PER_DEVICE=32
-#PD
+
+#memfabric config store
 export ASCEND_MF_STORE_URL="tcp://<PREFILL_HOST_IP>:<PORT>"
 
 #Deepep communication settings
@@ -164,7 +163,7 @@ python -m sglang.launch_server \
     --tokenizer-worker-num 4
 ```
 
-sglang router:
+3. SGLang Router
 
 ```shell
 python -m sglang_router.launch_router \
@@ -180,13 +179,13 @@ python -m sglang_router.launch_router \
 
 W8A8 Model weights could be found [here](https://modelers.cn/models/State_Cloud/Deepseek-R1-bf16-hfd-w8a8).
 
-Prefill:
+1. Prefill:
 
 ```shell
 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
 export STREAMS_PER_DEVICE=32
-#PD
-P_HOST_IP=('xx,xx,xx,xx' 'xx,xx,xx,xx')
+
+#memfabric config store
 export ASCEND_MF_STORE_URL="tcp://<P_HOST_IP[0]>:<PORT>"
 
 #Deepep communication settings
@@ -199,14 +198,18 @@ export SGLANG_USE_FIA_NZ=1
 export ENABLE_MOE_NZ=1
 export TASK_QUEUE_ENABLE=2
 
+#Please list all host ips of Prefill instance
+P_HOST_IP=('xx,xx,xx,xx' 'xx,xx,xx,xx')
+
 for i in "${!P_HOST_IP[@]}";
 do
   python -m sglang.launch_server \
       --model-path ${MODEL_PATH} \
-      --disaggregation-mode prefill \
       --host ${P_HOST_IP[$i]} \
       --port 8000 \
+      --disaggregation-mode prefill \
       --disaggregation-bootstrap-port $((8996+$i)) \
+      --disaggregation-transfer-backend ascend \
       --trust-remote-code \
       --nnodes 1 \
       --node-rank 0 \
@@ -215,7 +218,6 @@ do
       --attention-backend ascend \
       --device npu \
       --quantization modelslim \
-      --disaggregation-transfer-backend ascend \
       --max-running-requests 8 \
       --context-length 8192 \
       --disable-radix-cache \
@@ -234,12 +236,13 @@ do
 done
 ```
 
-Decode:
+2. Decode:
 
 ```shell
 export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
 export STREAMS_PER_DEVICE=32
-#PD
+
+#memfabric config store
 export ASCEND_MF_STORE_URL="tcp://<P_HOST_IP[0]>:<PORT>"
 
 #Deepep communication settings
@@ -256,6 +259,7 @@ export SGLANG_NPU_USE_MLAPO=1
 export SGLANG_USE_FIA_NZ=1
 export ENABLE_MOE_NZ=1
 
+#please list all host ips of Prefill instance
 D_HOST_IP=('xx,xx,xx,xx' 'xx,xx,xx,xx')
 
 for i in "${!D_HOST_IP[@]}";
@@ -263,6 +267,7 @@ do
   python -m sglang.launch_server
       --model-path ${MODEL_PATH} \
       --disaggregation-mode decode \
+      --disaggregation-transfer-backend ascend \
       --host ${D_HOST_IP[$i]} \
       --port 8001 \
       --trust-remote-code \
@@ -281,7 +286,6 @@ do
       --deepep-mode low_latency \
       --enable-dp-lm-head \
       --cuda-graph-bs  8 10 12 14 16 18 20 22 24 26 \
-      --disaggregation-transfer-backend ascend \
       --watchdog-timeout 9000 \
       --context-length 8192 \
       --speculative-algorithm NEXTN \
@@ -295,7 +299,7 @@ do
 done
 ```
 
-sglang router:
+3. SGLang Router:
 
 ```shell
 python -m sglang_router.launch_router \
diff --git a/docs/platforms/ascend_npu_qwen3_examples.md b/docs/platforms/ascend_npu_qwen3_examples.md
index 787dfb6a2..958ad8c97 100644
--- a/docs/platforms/ascend_npu_qwen3_examples.md
+++ b/docs/platforms/ascend_npu_qwen3_examples.md
@@ -13,13 +13,12 @@ export STREAMS_PER_DEVICE=32
 export HCCL_BUFFSIZE=1536
 export HCCL_OP_EXPANSION_MODE=AIV
 
-ASCEND_RT_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \
+python -m sglang.launch_server \
    --device npu \
    --attention-backend ascend \
    --trust-remote-code \
    --tp-size 4 \
    --model-path Qwen/Qwen3-32B \
-   --port 30111 \
    --mem-fraction-static 0.8
 ```
 
@@ -43,7 +42,6 @@ python -m sglang.launch_server \
    --trust-remote-code \
    --tp-size 4 \
    --model-path Qwen/Qwen3-32B \
-   --port 30111 \
    --mem-fraction-static 0.8 \
    --speculative-algorithm EAGLE3 \
    --speculative-draft-model-path Qwen/Qwen3-32B-Eagle3 \
@@ -66,13 +64,12 @@ export SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=32
 export SGLANG_DEEPEP_BF16_DISPATCH=1
 export ENABLE_ASCEND_MOE_NZ=1
 
-ASCEND_RT_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \
+python -m sglang.launch_server \
    --device npu \
    --attention-backend ascend \
    --trust-remote-code \
    --tp-size 4 \
    --model-path Qwen/Qwen3-30B-A3B \
-   --port 30111 \
    --mem-fraction-static 0.8
 ```
 
@@ -96,7 +93,6 @@ python -m sglang.launch_server \
    --attention-backend ascend \
    --device npu \
    --watchdog-timeout 9000 \
-   --port 30111 \
    --mem-fraction-static 0.8
 ```
 
@@ -111,14 +107,12 @@ export STREAMS_PER_DEVICE=32
 export HCCL_BUFFSIZE=1536
 export HCCL_OP_EXPANSION_MODE=AIV
 
-ASCEND_RT_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \
-   --device npu \
+python -m sglang.launch_server \
    --enable-multimodal \
    --attention-backend ascend \
    --mm-attention-backend ascend_attn \
    --trust-remote-code \
    --tp-size 4 \
    --model-path Qwen/Qwen3-VL-8B-Instruct \
-   --port 30111 \
    --mem-fraction-static 0.8
 ```
diff --git a/docs/platforms/ascend_npu_support.rst b/docs/platforms/ascend_npu_support.rst
index 1437515f8..494ace44d 100644
--- a/docs/platforms/ascend_npu_support.rst
+++ b/docs/platforms/ascend_npu_support.rst
@@ -5,5 +5,7 @@ Ascend NPUs
    :maxdepth: 1
 
    ascend_npu.md
+   ascend_npu_support_models.md
+   ascend_npu_support_features.md
    ascend_npu_deepseek_example.md
    ascend_npu_qwen3_examples.md
diff --git a/docs/platforms/ascend_npu_support_features.md b/docs/platforms/ascend_npu_support_features.md
new file mode 100644
index 000000000..457ad654f
--- /dev/null
+++ b/docs/platforms/ascend_npu_support_features.md
@@ -0,0 +1,400 @@
+# Support Features on Ascend NPU
+
+This section describes the basic functions and features supported by the Ascend NPU.If you encounter issues or have any
+questions, please [open an issue](https://github.com/sgl-project/sglang/issues).
+
+## Model and tokenizer
+
+| Argument                      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Defaults | Options                                                                          |               A2 Supported               |               A3 Supported               |
+|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|----------------------------------------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--model-path`<br/>`--model`  | The path of the model weights. This can be a local folder or a Hugging Face repo ID.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `None`   | Type: str                                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--tokenizer-path`            | The path of the tokenizer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `None`   | Type: str                                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--tokenizer-mode`            | Tokenizer mode. 'auto' will use the fast tokenizer if available, and 'slow' will always use the slow tokenizer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `auto`   | `auto`, `slow`                                                                   | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--tokenizer-worker-num`      | The worker num of the tokenizer manager.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `1`      | Type: int                                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--skip-tokenizer-init`       | If set, skip init tokenizer and pass input_ids in generate request.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `False`  | bool flag (set to enable)                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--load-format`               | The format of the model weights to load. <br/> `auto` will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available. <br/> `pt` will load the weights in the pytorch bin format. <br/> `safetensors` will load the weights in the safetensors format. <br/> `npcache` will load the weights in pytorch format and store a numpy cache to speed up the loading. <br/> `dummy` will initialize the weights with random values, which is mainly for profiling. <br/> `gguf` will load the weights in the gguf format. <br/> `bitsandbytes` will load the weights using bitsandbytes quantization.<br/>`layered` loads weights layer by layer so that one can quantize a layer before loading another to make the peak memory envelope smaller. | `auto`   | `auto`, `safetensors`                                                            | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--model-loader-extra-config` | Extra config for model loader. This will be passed to the model loader corresponding to the chosen load_format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | {}       | Type: str <br/> for example: {"enable_multithread_load": true,"num_threads": 64} | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--trust-remote-code`         | Whether or not to allow for custom models defined on the Hub in their own modeling files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `False`  | bool flag (set to enable)                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--context-length`            | The model's maximum context length. Defaults to None (will use the value from the model's config.json instead).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `None`   | Type: int                                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--is-embedding`              | Whether to use a CausalLM as an embedding model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `False`  | bool flag (set to enable)                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-multimodal`         | Enable the multimodal functionality for the served model. If the model being served is not multimodal, nothing will happen                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `None`   | bool flag (set to enable)                                                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--revision`                  | The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | `None`   | Type: str                                                                        |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--model-impl`                | Which implementation of the model to use. <br/> `auto` will try to use the SGLang implementation if it exists and fall back to the Transformers implementation if no SGLang implementation is available. <br/> `sglang` will use the sglang model implementation. <br/> `transformers` will use the transformers model implementation. <br/> `mindspore` will use the MindSpore model implementation.                                                                                                                                                                                                                                                                                                                                                                                                               | `None`   | `auto`, `sglang`, `transformers`                                                 |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+
+## HTTP server
+
+| Argument               | Description                                                                                                                                                                                                                                 | Defaults    | Options                   |               A2 Supported               |               A3 Supported               |
+|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--host`               | The host of the HTTP server.                                                                                                                                                                                                                | `127.0.0.1` | Type: str                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--port`               | The port of the HTTP server.                                                                                                                                                                                                                | `30000`     | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--skip-server-warmup` | If set, skip warmup.                                                                                                                                                                                                                        | `False`     | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--warmups`            | Specify custom warmup functions (csv) to run before server starts eg. `--warmups=warmup_name1,warmup_name2` will run the functions `warmup_name1` and `warmup_name2` specified in warmup.py before the server starts listening for requests | `None`      | Type: str                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--nccl-port`          | The port for NCCL distributed environment setup. Defaults to a random port.                                                                                                                                                                 | `None`      | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Quantization and data type
+
+| Argument                    | Description                                                                                                                                                                                                                                                                                 | Defaults | Options                       |               A2 Supported               |               A3 Supported               |
+|-----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--dtype`                   | Data type for model weights and activations. * "auto" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models. * "half" for FP16. Recommended for AWQ quantization. * "float16" is the same as "half". * "bfloat16" for a balance between precision and range. | `auto`   | `auto`, `float16`, `bfloat16` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--quantization`            | The quantization method.                                                                                                                                                                                                                                                                    | `None`   | `modelslim`                   | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--quantization-param-path` | Path to the JSON file containing the KV cache scaling factors. This should generally be supplied, when KV cache dtype is FP8. Otherwise, KV cache scaling factors default to 1.0, which may cause accuracy issues.                                                                          | `None`   | Type: Optional[str]           |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--kv-cache-dtype`          | Data type for kv cache storage. "auto" will use model data type. "fp8_e5m2" and "fp8_e4m3" is supported for CUDA 11.8+.                                                                                                                                                                     | `auto`   | `auto`                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Memory and scheduling
+
+| Argument                                     | Description                                                                                                                                                                                                              | Defaults | Options                   |               A2 Supported               |               A3 Supported               |
+|----------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--mem-fraction-static`                      | The fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.                                                                     | `None`   | Type: float               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-running-requests`                     | The maximum number of running requests.                                                                                                                                                                                  | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--prefill-max-requests`                     | The maximum number of requests in a prefill batch. If not specified, there is no limit..                                                                                                                                 | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-queued-requests`                      | The maximum number of queued requests. This option is ignored when using disaggregation-mode.                                                                                                                            | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-total-tokens`                         | The maximum number of tokens in the memory pool. If not specified, it will be automatically calculated based on the memory usage fraction. This option is typically used for development and debugging purposes.         | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--chunked-prefill-size`                     | The maximum number of tokens in a chunk for the chunked prefill. Setting this to -1 means disabling chunked prefill.                                                                                                     | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-prefill-tokens`                       | The maximum number of tokens in a prefill batch. The real bound will be the maximum of this value and the model's maximum context length.                                                                                | `16384`  | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--schedule-policy`                          | The scheduling policy of the requests.                                                                                                                                                                                   | `fcfs`   | `lpm`, `fcfs`             | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-priority-scheduling`               | Enable priority scheduling. Requests with higher priority integer values will be scheduled first by default.                                                                                                             | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--schedule-low-priority-values-first`       | If specified with --enable-priority-scheduling, the scheduler will schedule requests with lower priority integer values first.                                                                                           | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--priority-scheduling-preemption-threshold` | Minimum difference in priorities for an incoming request to have to preempt running request(s).                                                                                                                          | `10`     | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--schedule-conservativeness`                | How conservative the schedule policy is. A larger value means more conservative scheduling. Use a larger value if you see requests being retracted frequently.                                                           | `1.0`    | Type: float               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--page-size`                                | The number of tokens in a page, auto set 128 to Ascend NPU.                                                                                                                                                              | `128`    | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hybrid-kvcache-ratio`                     | Mix ratio in [0,1] between uniform and hybrid kv buffers (0.0 = pure uniform: swa_size / full_size = 1)(1.0 = pure hybrid: swa_size / full_size = local_attention_size / context_length)                                 | `None`   | Optional[float]           |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--swa-full-tokens-ratio`                    | The ratio of SWA layer KV tokens / full layer KV tokens, regardless of the number of swa:full layers. It should be between 0 and 1. E.g. 0.5 means if each swa layer has 50 tokens, then each full layer has 100 tokens. | `0.8`    | Type: float               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-hybrid-swa-memory`                | Disable the hybrid SWA memory.                                                                                                                                                                                           | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Runtime options
+
+| Argument                                    | Description                                                                                                                                                                                                          | Defaults | Options                   |               A2 Supported               |               A3 Supported               |
+|---------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--device`                                  | The device to use ('cuda', 'xpu', 'hpu', 'npu', 'cpu'). Defaults to auto-detection if not specified.                                                                                                                 | `None`   | Type: str                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--tensor-parallel-size`<br/>`--tp-size`    | The tensor parallelism size.                                                                                                                                                                                         | `1`      | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--pipeline-parallel-size`<br/>`--pp-size`  | The pipeline parallelism size.                                                                                                                                                                                       | `1`      | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--pp-max-micro-batch-size`                 | The maximum micro batch size in pipeline parallelism.                                                                                                                                                                | `None`   | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--stream-interval`                         | The interval (or buffer size) for streaming in terms of the token length. A smaller value makes streaming smoother, while a larger value makes the throughput higher                                                 | `1`      | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--stream-output`                           | Whether to output as a sequence of disjoint segments.                                                                                                                                                                | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--random-seed`                             | The random seed.                                                                                                                                                                                                     | `None`   | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--constrained-json-whitespace-pattern`     | (outlines and llguidance backends only) Regex pattern for syntactic whitespaces allowed in JSON constrained output. For example, to allow the model to generate consecutive whitespaces, set the pattern to [\n\t ]* | `None`   | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--constrained-json-disable-any-whitespace` | (xgrammar and llguidance backends only) Enforce compact representation in JSON constrained output.                                                                                                                   | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--watchdog-timeout`                        | Set watchdog timeout in seconds. If a forward batch takes longer than this, the server will crash to prevent hanging.                                                                                                | `300`    | Type: float               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--soft-watchdog-timeout`                   | Set soft watchdog timeout in seconds. If a forward batch takes longer than this, the server will dump information for debugging.                                                                                     | `300`    | Type: float               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--dist-timeout`                            | Set timeout for torch.distributed initialization.                                                                                                                                                                    | `None`   | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--base-gpu-id`                             | The base GPU ID to start allocating GPUs from. Useful when running multiple instances on the same machine.                                                                                                           | `0`      | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--gpu-id-step`                             | The delta between consecutive GPU IDs that are used. For example, setting it to 2 will use GPU 0,2,4,...                                                                                                             | `1`      | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--sleep-on-idle`                           | Reduce CPU usage when sglang is idle.                                                                                                                                                                                | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--mm-process-config`                       | A JSON string for multimodal preprocessing configuration. It can contain keys: `image`, `video`, `audio`.                                                                                                            | `{}`     |                           |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Logging
+
+| Argument                                    | Description                                                                                                                                                                                                                                                                                                                                                                                             | Defaults          | Options                   |               A2 Supported               |               A3 Supported               |
+|---------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--log-level`                               | The logging level of all loggers.                                                                                                                                                                                                                                                                                                                                                                       | `info`            | Type: str                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--log-level-http`                          | The logging level of HTTP server. If not set, reuse --log-level by default.                                                                                                                                                                                                                                                                                                                             | `None`            | Type: str                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--log-requests`                            | Log metadata, inputs, outputs of all requests. The verbosity is decided by --log-requests-level                                                                                                                                                                                                                                                                                                         | `False`           | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--log-requests-level`                      | 0: Log metadata (no sampling parameters). 1: Log metadata and sampling parameters. 2: Log metadata, sampling parameters and partial input/output. 3: Log every input/output.                                                                                                                                                                                                                            | `2`               | `0`, `1`, `2`, `3`        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--crash-dump-folder`                       | Folder path to dump requests from the last 5 min before a crash (if any). If not specified, crash dumping is disabled.                                                                                                                                                                                                                                                                                  | `None`            | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--crash-on-nan`                            | Crash the server on nan logprobs.                                                                                                                                                                                                                                                                                                                                                                       | `False`           | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-metrics`                          | Enable log prometheus metrics.                                                                                                                                                                                                                                                                                                                                                                          | `False`           | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-metrics-for-all-schedulers`       | Enable --enable-metrics-for-all-schedulers when you want schedulers on all TP ranks (not just TP 0) to record request metrics separately. This is especially useful when dp_attention is enabled, as otherwise all metrics appear to come from TP 0.                                                                                                                                                    | `False`           | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--tokenizer-metrics-custom-labels-header`  | Specify the HTTP header for passing custom labels for tokenizer metrics.                                                                                                                                                                                                                                                                                                                                | `x-custom-labels` | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--tokenizer-metrics-allowed-custom-labels` | The custom labels allowed for tokenizer metrics. The labels are specified via a dict in '--tokenizer-metrics-custom-labels-header' field in HTTP requests, e.g., {'label1': 'value1', 'label2': 'value2'} is allowed if '--tokenizer-metrics-allowed-custom-labels label1 label2' is set.                                                                                                               | `None`            | List[str]                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--bucket-time-to-first-token`              | The buckets of time to first token, specified as a list of floats.                                                                                                                                                                                                                                                                                                                                      | `None`            | List[float]               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--bucket-inter-token-latency`              | The buckets of inter-token latency, specified as a list of floats.                                                                                                                                                                                                                                                                                                                                      | `None`            | List[float]               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--bucket-e2e-request-latency`              | The buckets of end-to-end request latency, specified as a list of floats.                                                                                                                                                                                                                                                                                                                               | `None`            | List[float]               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--collect-tokens-histogram`                | Collect prompt/generation tokens histogram.                                                                                                                                                                                                                                                                                                                                                             | `False`           | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--prompt-tokens-buckets`                   | The buckets rule of prompt tokens. Supports 3 rule types: 'default' uses predefined buckets; 'tse <middle> <base> <count>' generates two sides exponential distributed buckets (e.g., 'tse 1000 2 8' generates buckets [984.0, 992.0, 996.0, 998.0, 1000.0, 1002.0, 1004.0, 1008.0, 1016.0]).); 'custom <value1> <value2> ...' uses custom bucket values (e.g., 'custom 10 50 100 500').                | `None`            | List[str]                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--generation-tokens-buckets`               | The buckets rule for generation tokens histogram. Supports 3 rule types: 'default' uses predefined buckets; 'tse <middle> <base> <count>' generates two sides exponential distributed buckets (e.g., 'tse 1000 2 8' generates buckets [984.0, 992.0, 996.0, 998.0, 1000.0, 1002.0, 1004.0, 1008.0, 1016.0]).); 'custom <value1> <value2> ...' uses custom bucket values (e.g., 'custom 10 50 100 500'). | `None`            | List[str]                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--gc-warning-threshold-secs`               | The threshold for long GC warning. If a GC takes longer than this, a warning will be logged. Set to 0 to disable.                                                                                                                                                                                                                                                                                       | `0.0`             | Type: float               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--decode-log-interval`                     | The log interval of decode batch.                                                                                                                                                                                                                                                                                                                                                                       | `40`              | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-request-time-stats-logging`       | Enable per request time stats logging                                                                                                                                                                                                                                                                                                                                                                   | `False`           | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--kv-events-config`                        | Config in json format for NVIDIA dynamo KV event publishing. Publishing will be enabled if this flag is used.                                                                                                                                                                                                                                                                                           | `None`            | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-trace`                            | Enable opentelemetry trace                                                                                                                                                                                                                                                                                                                                                                              | `False`           | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--oltp-traces-endpoint`                    | Config opentelemetry collector endpoint if --enable-trace is set. format: <ip>:<port>                                                                                                                                                                                                                                                                                                                   | `localhost:4317`  | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## API related
+
+| Argument                | Description                                                                                                                                                                                                                                     | Defaults  | Options                                                                                      |               A2 Supported               |               A3 Supported               |
+|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|----------------------------------------------------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--api-key`             | Set API key of the server. It is also used in the OpenAI API compatible server.                                                                                                                                                                 | `None`    | Type: str                                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--served-model-name`   | Override the model name returned by the v1/models endpoint in OpenAI API server.                                                                                                                                                                | `None`    | Type: str                                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--weight-version`      | Version identifier for the model weights. Defaults to 'default' if not specified.                                                                                                                                                               | `default` | Type: str                                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--chat-template`       | The buliltin chat template name or the path of the chat template file. This is only used for OpenAI-compatible API server.                                                                                                                      | `None`    | Type: str                                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--completion-template` | The buliltin completion template name or the path of the completion template file. This is only used for OpenAI-compatible API server. only for code completion currently.                                                                      | `None`    | Type: str                                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-cache-report` | Return number of cached tokens in usage.prompt_tokens_details for each openai request.                                                                                                                                                          | `True`    | bool flag (set to enable)                                                                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--reasoning-parser`    | Specify the parser for reasoning models. Supported parsers: [deepseek-r1, deepseek-v3, glm45, gpt-oss, kimi, qwen3, qwen3-thinking, step3].                                                                                                     | `None`    | `deepseek-r1`, `deepseek-v3`, `glm45`, `gpt-oss`, `kimi`, `qwen3`, `qwen3-thinking`, `step3` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--tool-call-parser`    | Specify the parser for handling tool-call interactions. Supported parsers: [ llama3, qwen].                                                                                                                                                     | `None`    | `llama3`,`qwen`                                                                              | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--sampling-defaults`   | Where to get default sampling parameters. 'openai' uses SGLang/OpenAI defaults (temperature=1.0, top_p=1.0, etc.). 'model' uses the model's generation_config.json to get the recommended sampling parameters if available. Default is 'model'. | `model`   | `openai`, `model`                                                                            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--tool-server`         | Either 'demo' or a comma-separated list of tool server urls to use for the model. If not specified, no tool server will be used.                                                                                                                | `None`    | Type: str                                                                                    |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Data parallelism
+
+| Argument                               | Description                                                                                                                                                                                                              | Defaults      | Options                                           |               A2 Supported               |               A3 Supported               |
+|----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|---------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--data-parallel-size`<br/>`--dp-size` | The data parallelism size.                                                                                                                                                                                               | `1`           | Type: int                                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--load-balance-method`                | The load balancing strategy for data parallelism. The Minimum Token algorithm can only be used when DP attention is applied. This algorithm performs load balancing based on the real-time token load of the DP workers. | `round_robin` | `round_robin`, `shortest_queue`, `minimum_tokens` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--prefill-round-robin-balance`        | Prefill is round robin balanced. This is used to promise decode server can get the correct dp rank.                                                                                                                      | `False`       | bool flag (set to enable)                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Multi-node distributed serving
+
+| Argument                                  | Description                                                                        | Defaults | Options   |               A2 Supported               |               A3 Supported               |
+|-------------------------------------------|------------------------------------------------------------------------------------|----------|-----------|:----------------------------------------:|:----------------------------------------:|
+| `--dist-init-addr`<br/>`--nccl-init-addr` | The host address for initializing distributed backend (e.g., `192.168.0.2:25000`). | `None`   | Type: str | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--nnodes`                                | The number of nodes.                                                               | `1`      | Type: int | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--node-rank`                             | The node rank.                                                                     | `0`      | Type: int | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Model override args
+
+| Argument                      | Description                                                                       | Defaults | Options   |               A2 Supported               |               A3 Supported               |
+|-------------------------------|-----------------------------------------------------------------------------------|----------|-----------|:----------------------------------------:|:----------------------------------------:|
+| `--json-model-override-args`  | A dictionary in JSON string format used to override default model configurations. | `{}`     | Type: str | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--preferred-sampling-params` | json-formatted sampling settings that will be returned in /get_model_info         | `None`   | Type: str | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## LoRA
+
+| Argument                 | Description                                                                                                                                                                                                                                                                                                                            | Defaults | Options                                                                                                        |               A2 Supported               |               A3 Supported               |
+|--------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--enable-lora`          | Enable LoRA support for the model. This argument is automatically set to `True` if `--lora-paths` is provided for backward compatibility.                                                                                                                                                                                              | `False`  | Bool flag (set to enable)                                                                                      | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-lora-rank`        | The maximum LoRA rank that should be supported. If not specified, it will be automatically inferred from the adapters provided in `--lora-paths`. This argument is needed when you expect to dynamically load adapters of larger LoRA rank after server startup.                                                                       | `None`   | Type: int                                                                                                      | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--lora-target-modules`  | The union set of all target modules where LoRA should be applied (e.g., `q_proj`, `k_proj`, `gate_proj`). If not specified, it will be automatically inferred from the adapters provided in `--lora-paths`. You can also set it to `all` to enable LoRA for all supported modules; note this may introduce minor performance overhead. | `None`   | `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, `qkv_proj`, `gate_up_proj`, `all` |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--lora-paths`           | The list of LoRA adapters to load. Each adapter must be specified in one of the following formats: `<PATH>` \| `<NAME>=<PATH>` \| JSON with schema `{"lora_name": str, "lora_path": str, "pinned": bool}`.                                                                                                                             | `None`   | Type: List[str] / JSON objects                                                                                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--max-loras-per-batch`  | Maximum number of adapters for a running batch, including base-only requests.                                                                                                                                                                                                                                                          | `8`      | Type: int                                                                                                      |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--max-loaded-loras`     | If specified, limits the maximum number of LoRA adapters loaded in CPU memory at a time. Must be ≥ `--max-loras-per-batch`.                                                                                                                                                                                                            | `None`   | Type: int                                                                                                      |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--lora-eviction-policy` | LoRA adapter eviction policy when the GPU memory pool is full.                                                                                                                                                                                                                                                                         | `lru`    | `lru`, `fifo`                                                                                                  |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--lora-backend`         | Choose the kernel backend for multi-LoRA serving.                                                                                                                                                                                                                                                                                      | `triton` | `triton`, `csgmv`                                                                                              |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--max-lora-chunk-size`  | Maximum chunk size for the ChunkedSGMV LoRA backend. Only used when `--lora-backend` is `csgmv`. Larger values may improve performance.                                                                                                                                                                                                | `16`     | `16`, `32`, `64`, `128`                                                                                        |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Kernel Backends (Attention, Sampling, Grammar, GEMM)
+
+| Argument                        | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Defaults          | Options                                                                |               A2 Supported               |               A3 Supported               |
+|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|------------------------------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--attention-backend`           | Choose the kernels for attention layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`            | `ascend`                                                               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--prefill-attention-backend`   | Choose the kernels for prefill attention layers (have priority over --attention-backend).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `None`            | `ascend`                                                               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--decode-attention-backend`    | Choose the kernels for decode attention layers (have priority over --attention-backend).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `None`            | `ascend`                                                               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--sampling-backend`            | Choose the kernels for sampling layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `None`            | `pytorch`,`ascend`                                                     | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--grammar-backend`             | Choose the backend for grammar-guided decoding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `None`            | `xgrammar`                                                             | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--mm-attention-backend`        | Set multimodal attention backend.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `None`            | `ascend_attn`                                                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--nsa-prefill-backend`         | Choose the NSA backend for the prefill stage (overrides `--attention-backend` when running DeepSeek NSA-style attention).                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `flashmla_sparse` | `flashmla_sparse`, `flashmla_decode`, `fa3`, `tilelang`, `aiter`       |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--nsa-decode-backend`          | Choose the NSA backend for the decode stage when running DeepSeek NSA-style attention. Overrides `--attention-backend` for decoding.                                                                                                                                                                                                                                                                                                                                                                                                                                               | `flashmla_kv`     | `flashmla_prefill`, `flashmla_kv`, `fa3`, `tilelang`, `aiter`          |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--fp8-gemm-backend`            | Choose the runner backend for Blockwise FP8 GEMM operations. Options: 'auto' (default, auto-selects based on hardware), 'deep_gemm' (JIT-compiled; enabled by default on NVIDIA Hopper (SM90) and Blackwell (SM100) when DeepGEMM is installed), 'flashinfer_trtllm' (optimal for Blackwell and low-latency), 'cutlass' (optimal for Hopper/Blackwell GPUs and high-throughput), 'triton' (fallback, widely compatible), 'aiter' (ROCm only). **NOTE**: This replaces the deprecated environment variables SGLANG_ENABLE_FLASHINFER_FP8_GEMM and SGLANG_SUPPORT_CUTLASS_BLOCK_FP8. | `auto`            | `auto`, `deep_gemm`, `flashinfer_trtllm`, `cutlass`, `triton`, `aiter` |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-flashinfer-autotune` | Flashinfer autotune is enabled by default. Set this flag to disable the autotune.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `False`           | bool flag (set to enable)                                              |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Speculative decoding
+
+| Argument                                                         | Description                                                                                                                                 | Defaults  | Options                                           |               A2 Supported               |               A3 Supported               |
+|------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-----------|---------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--speculative-algorithm`                                        | Speculative algorithm.                                                                                                                      | `None`    | `EAGLE`, `EAGLE3`, `NEXTN`, `STANDALONE`, `NGRAM` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-draft-model-path`<br/>`--speculative-draft-model` | The path of the draft model weights. This can be a local folder or a Hugging Face repo ID.                                                  | `None`    | Type: str                                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-draft-model-revision`                             | The specific draft model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version. | `None`    | Type: str                                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--speculative-num-steps`                                        | The number of steps sampled from draft model in Speculative Decoding.                                                                       | `None`    | Type: int                                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-eagle-topk`                                       | The number of tokens sampled from the draft model in eagle2 each step.                                                                      | `None`    | Type: int                                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-num-draft-tokens`                                 | The number of tokens sampled from the draft model in Speculative Decoding.                                                                  | `None`    | Type: int                                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-accept-threshold-single`                          | Accept a draft token if its probability in the target model is greater than this threshold.                                                 | `1.0`     | Type: float                                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-accept-threshold-acc`                             | The accept probability of a draft token is raised from its target probability p to min(1, p / threshold_acc).                               | `1.0`     | Type: float                                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-token-map`                                        | The path of the draft model's small vocab table.                                                                                            | `None`    | Type: str                                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--speculative-attention-mode`                                   | Attention backend for speculative decoding operations (both target verify and draft extend). Can be one of 'prefill' (default) or 'decode'. | `prefill` | `prefill`, `decode`                               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-moe-runner-backend`                               | MOE backend for EAGLE speculative decoding, see --moe-runner-backend for options. Same as moe runner backend if unset.                      | `None`    |                                                   | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--speculative-moe-a2a-backend`                                  | MOE A2A backend for EAGLE speculative decoding, see --moe-a2a-backend for options. Same as moe a2a backend if unset.                        | `None`    |                                                   | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Ngram speculative decoding
+
+| Argument                                    | Description                                                                       | Defaults   | Options       |              A2 Supported              |              A3 Supported              |
+|---------------------------------------------|-----------------------------------------------------------------------------------|------------|---------------|:--------------------------------------:|:--------------------------------------:|
+| `--speculative-ngram-min-match-window-size` | The minimum window size for pattern matching in ngram speculative decoding.       | `1`        | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-max-match-window-size` | The maximum window size for pattern matching in ngram speculative decoding.       | `12`       | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-min-bfs-breadth`       | The minimum breadth for BFS (Breadth-First Search) in ngram speculative decoding. | `1`        | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-max-bfs-breadth`       | The maximum breadth for BFS (Breadth-First Search) in ngram speculative decoding. | `10`       | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-match-type`            | The match type for cache tree.                                                    | `BFS`      | `BFS`, `PROB` | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-branch-length`         | The branch length for ngram speculative decoding.                                 | `18`       | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--speculative-ngram-capacity`              | The cache capacity for ngram speculative decoding.                                | `10000000` | Type: int     | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Expert parallelism
+
+| Argument                                            | Description                                                                                                                                                                                     | Defaults  | Options                           |               A2 Supported               |               A3 Supported               |
+|-----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--expert-parallel-size`<br/>`--ep-size`<br/>`--ep` | The expert parallelism size. Default equal to tp size.                                                                                                                                          | `tp-size` | Type: int                         |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--moe-a2a-backend`                                 | Select the backend for all-to-all communication for expert parallelism.                                                                                                                         | `none`    | `none`, `deepep`, `ascend_fuseep` |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--moe-runner-backend`                              | Choose the runner backend for MoE.                                                                                                                                                              | `auto`    | `auto`                            |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--flashinfer-mxfp4-moe-precision`                  | Choose the computation precision of flashinfer mxfp4 moe                                                                                                                                        | `default` | `default`, `bf16`                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-flashinfer-allreduce-fusion`              | Enable FlashInfer allreduce fusion with Residual RMSNorm.                                                                                                                                       | `False`   | bool flag (set to enable)         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--deepep-mode`                                     | Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or `auto`. Default is `auto`, which means `low_latency` for decode batch and `normal` for prefill batch.               | `auto`    | `normal`, `low_latency`, `auto`   |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--deepep-config`                                   | Tuned DeepEP config suitable for your own cluster. It can be either a string with JSON content or a file path.                                                                                  | `None`    | Type: str                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--ep-num-redundant-experts`                        | Allocate this number of redundant experts in expert parallel.                                                                                                                                   | `0`       | Type: int                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--ep-dispatch-algorithm`                           | The algorithm to choose ranks for redundant experts in expert parallel.                                                                                                                         | `None`    | Type: str                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--init-expert-location`                            | Initial location of EP experts.                                                                                                                                                                 | `trivial` | Type: str                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-eplb`                                     | Enable EPLB algorithm                                                                                                                                                                           | `False`   | bool flag (set to enable)         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--eplb-algorithm`                                  | Chosen EPLB algorithm                                                                                                                                                                           | `auto`    | Type: str                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--eplb-rebalance-layers-per-chunk`                 | Number of layers to rebalance per forward pass.                                                                                                                                                 | `None`    | Type: int                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--eplb-min-rebalancing-utilization-threshold`      | Minimum threshold for GPU average utilization to trigger EPLB rebalancing. Must be in the range [0.0, 1.0].                                                                                     | `1.0`     | Type: float                       |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--expert-distribution-recorder-mode`               | Mode of expert distribution recorder.                                                                                                                                                           | `None`    | Type: str                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--expert-distribution-recorder-buffer-size`        | Circular buffer size of expert distribution recorder. Set to -1 to denote infinite buffer.                                                                                                      | `None`    | Type: int                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-expert-distribution-metrics`              | Enable logging metrics for expert balancedness                                                                                                                                                  | `False`   | bool flag (set to enable)         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--moe-dense-tp-size`                               | TP size for MoE dense MLP layers. This flag is useful when, with large TP size, there are errors caused by weights in MLP layers having dimension smaller than the min dimension GEMM supports. | `None`    | Type: int                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--elastic-ep-backend`                              | Select the collective communication backend for elastic EP. Currently supports 'mooncake'.                                                                                                      | None      | N/A                               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--mooncake-ib-device`                              | The InfiniBand devices for Mooncake Backend, accepts multiple comma-separated devices. Default is None, which triggers automatic device detection when Mooncake Backend is enabled.             | None      | N/A                               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Mamba Cache
+
+| Argument                     | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Defaults  | Options                             |              A2 Supported              |              A3 Supported              |
+|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-------------------------------------|:--------------------------------------:|:--------------------------------------:|
+| `--max-mamba-cache-size`     | The maximum size of the mamba cache.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `None`    | Type: int                           | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--mamba-ssm-dtype`          | The data type of the SSM states in mamba cache.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `float32` | `float32`, `bfloat16`               | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--mamba-full-memory-ratio`  | The ratio of mamba state memory to full kv cache memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `0.2`     | Type: float                         | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--mamba-scheduler-strategy` | The strategy to use for mamba scheduler. <br/>`auto` currently defaults to `no_buffer`. <br/>1. `no_buffer` does not support overlap scheduler due to not allocating extra mamba state buffers. Branching point caching support is feasible but not implemented. <br/>2. `extra_buffer` supports overlap schedule by allocating extra mamba state buffers to track mamba state for caching (mamba state usage per running req becomes `2x` for non-spec; `1+(1/(2+speculative_num_draft_tokens))x` for spec dec (e.g. 1.16x if speculative_num_draft_tokens==4)). <br/>2a. `extra_buffer` is strictly better for non-KV-cache-bound cases; for KV-cache-bound cases, the tradeoff depends on whether enabling overlap outweighs reduced max running requests. <br/>2b. mamba caching at radix cache branching point is strictly better than non-branch but requires kernel support (currently only FLA backend), currently only extra_buffer supports branching. | `auto`    | `auto`, `no_buffer`, `extra_buffer` | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--mamba-track-interval`     | The interval (in tokens) to track the mamba state during decode. Only used when `--mamba-scheduler-strategy` is `extra_buffer`. Must be divisible by page_size if set, and must be >= speculative_num_draft_tokens when using speculative decoding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `256`     | Type: int                           | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Args for multi-item scoring
+
+| Argument                         | Description                                                                                                                                                                                                                              | Defaults | Options   |              A2 Supported              |              A3 Supported              |
+|----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-----------|:--------------------------------------:|:--------------------------------------:|
+| `--multi-item-scoring-delimiter` | Delimiter token ID for multi-item scoring. Used to combine Query and Items into a single sequence: Query<delimiter>Item1<delimiter>Item2<delimiter>... This enables efficient batch processing of multiple items against a single query. | `None`   | Type: int | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Hierarchical cache
+
+| Argument                                 | Description                                                                                                                                                                                                                                                                       | Defaults        | Options                                                  |               A2 Supported               |               A3 Supported               |
+|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|----------------------------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--enable-hierarchical-cache`            | Enable hierarchical cache                                                                                                                                                                                                                                                         | `False`         | bool flag (set to enable)                                | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-ratio`                        | The ratio of the size of host KV cache memory pool to the size of device pool.                                                                                                                                                                                                    | `2.0`           | Type: float                                              | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-size`                         | The size of host KV cache memory pool in gigabytes, which will override the hicache_ratio if set.                                                                                                                                                                                 | `0`             | Type: int                                                | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-write-policy`                 | The write policy of hierarchical cache.                                                                                                                                                                                                                                           | `write_through` | `write_back`, `write_through`, `write_through_selective` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--radix-eviction-policy`                | The eviction policy of radix trees. 'lru' stands for Least Recently Used, 'lfu' stands for Least Frequently Used.                                                                                                                                                                 | `lru`           | `lru`, `lfu`                                             | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-io-backend`                   | The IO backend for KV cache transfer between CPU and GPU                                                                                                                                                                                                                          | `kernel`        | `kernel_ascend`                                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-mem-layout`                   | The layout of host memory pool for hierarchical cache.                                                                                                                                                                                                                            | `layer_first`   | `page_first_direct`, `page_first_kv_split`               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-storage-backend`              | The storage backend for hierarchical KV cache. Built-in backends: file, mooncake, hf3fs, nixl, aibrix. For dynamic backend, use --hicache-storage-backend-extra-config to specify: backend_name (custom name), module_path (Python module path), class_name (backend class name). | `None`          | `file`                                                   | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--hicache-storage-prefetch-policy`      | Control when prefetching from the storage backend should stop.                                                                                                                                                                                                                    | `best_effort`   | `best_effort`, `wait_complete`, `timeout`                |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--hicache-storage-backend-extra-config` | A dictionary in JSON string format containing extra configuration for the storage backend.                                                                                                                                                                                        | `None`          | Type: str                                                |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## LMCache
+
+| Argument           | Description                                                 | Defaults | Options                   |              A2 Supported              |              A3 Supported              |
+|--------------------|-------------------------------------------------------------|----------|---------------------------|:--------------------------------------:|:--------------------------------------:|
+| `--enable-lmcache` | Using LMCache as an alternative hierarchical cache solution | `False`  | bool flag (set to enable) | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Double Sparsity
+
+| Argument                       | Description                                                                                                                                  | Defaults | Options                   |              A2 Supported              |              A3 Supported              |
+|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------|---------------------------|:--------------------------------------:|:--------------------------------------:|
+| `--enable-double-sparsity`     | Enable double sparsity attention                                                                                                             | `False`  | bool flag (set to enable) | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--ds-channel-config-path`     | The path of the double sparsity channel config                                                                                               | `None`   | Type: str                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--ds-heavy-channel-num`       | The number of heavy channels in double sparsity attention                                                                                    | `32`     | Type: int                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--ds-heavy-token-num`         | The number of heavy tokens in double sparsity attention                                                                                      | `256`    | Type: int                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--ds-heavy-channel-type`      | The type of heavy channels in double sparsity attention                                                                                      | `qk`     | Type: str                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--ds-sparse-decode-threshold` | The minimum decode sequence length required before the double-sparsity backend switches from the dense fallback to the sparse decode kernel. | `4096`   | Type: int                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Offloading
+
+| Argument                  | Description                                        | Defaults | Options   |               A2 Supported               |               A3 Supported               |
+|---------------------------|----------------------------------------------------|----------|-----------|:----------------------------------------:|:----------------------------------------:|
+| `--cpu-offload-gb`        | How many GBs of RAM to reserve for CPU offloading. | `0`      | Type: int | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--offload-group-size`    | Number of layers per group in offloading.          | `-1`     | Type: int |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--offload-num-in-group`  | Number of layers to be offloaded within a group.   | `1`      | Type: int |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--offload-prefetch-step` | Steps to prefetch in offloading.                   | `1`      | Type: int |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--offload-mode`          | Mode of offloading.                                | `cpu`    | Type: str |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Optimization/debug options
+
+| Argument                                         | Description                                                                                                                                                                                                                             | Defaults | Options                   |               A2 Supported               |               A3 Supported               |
+|--------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--disable-radix-cache`                          | Disable RadixAttention for prefix caching.                                                                                                                                                                                              | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--cuda-graph-max-bs`                            | Set the maximum batch size for cuda graph. It will extend the cuda graph capture batch size to this value.                                                                                                                              | `None`   | Type: int                 |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--cuda-graph-bs`                                | Set the list of batch sizes for cuda graph.                                                                                                                                                                                             | `None`   | List[int]                 |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--disable-cuda-graph`                           | Disable cuda graph.                                                                                                                                                                                                                     | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disable-cuda-graph-padding`                   | Disable cuda graph when padding is needed. Still uses cuda graph when padding is not needed.                                                                                                                                            | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--enable-profile-cuda-graph`                    | Enable profiling of cuda graph capture.                                                                                                                                                                                                 | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-cudagraph-gc`                          | Enable garbage collection during CUDA graph capture. If disabled (default), GC is frozen during capture to speed up the process.                                                                                                        | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-nccl-nvls`                             | Enable NCCL NVLS for prefill heavy requests when available.                                                                                                                                                                             | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-symm-mem`                              | Enable NCCL symmetric memory for fast collectives.                                                                                                                                                                                      | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-flashinfer-cutlass-moe-fp4-allgather` | Disables quantize before all-gather for flashinfer cutlass moe.                                                                                                                                                                         | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-tokenizer-batch-encode`                | Enable batch tokenization for improved performance when processing multiple text inputs. Do not use with image inputs, pre-tokenized input_ids, or input_embeds.                                                                        | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disable-outlines-disk-cache`                  | Disable disk cache of outlines to avoid possible crashes related to file system or high concurrency.                                                                                                                                    | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disable-custom-all-reduce`                    | Disable the custom all-reduce kernel and fall back to NCCL.                                                                                                                                                                             | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-mscclpp`                               | Enable using mscclpp for small messages for all-reduce kernel and fall back to NCCL.                                                                                                                                                    | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-torch-symm-mem`                        | Enable using torch symm mem for all-reduce kernel and fall back to NCCL. Only supports CUDA device SM90 and above. SM90 supports world size 4, 6, 8. SM10 supports world size 6, 8.                                                     | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-overlap-schedule`                     | Disable the overlap scheduler, which overlaps the CPU scheduler with GPU model worker.                                                                                                                                                  | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-mixed-chunk`                           | Enabling mixing prefill and decode in a batch when using chunked prefill.                                                                                                                                                               | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-dp-attention`                          | Enabling data parallelism for attention and tensor parallelism for FFN. The dp size should be equal to the tp size. Currently DeepSeek-V2 and Qwen 2/3 MoE models are supported.                                                        | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-dp-lm-head`                            | Enable vocabulary parallel across the attention TP group to avoid all-gather across DP groups, optimizing performance under DP attention.                                                                                               | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-two-batch-overlap`                     | Enabling two micro batches to overlap.                                                                                                                                                                                                  | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-single-batch-overlap`                  | Let computation and communication overlap within one micro batch.                                                                                                                                                                       | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--tbo-token-distribution-threshold`             | The threshold of token distribution between two batches in micro-batch-overlap, determines whether to two-batch-overlap or two-chunk-overlap. Set to 0 denote disable two-chunk-overlap.                                                | `0.48`   | Type: float               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-torch-compile`                         | Optimize the model with torch.compile. Experimental feature.                                                                                                                                                                            | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-torch-compile-debug-mode`              | Enable debug mode for torch compile.                                                                                                                                                                                                    | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--enable-piecewise-cuda-graph`                  | Optimize the model with piecewise cuda graph for extend/prefill only. Experimental feature.                                                                                                                                             | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--piecewise-cuda-graph-tokens`                  | Set the list of tokens when using piecewise cuda graph.                                                                                                                                                                                 | `None`   | Type: JSON list           |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--torch-compile-max-bs`                         | Set the maximum batch size when using torch compile.                                                                                                                                                                                    | `32`     | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--piecewise-cuda-graph-max-tokens`              | Set the maximum tokens when using piecewise cuda graph.                                                                                                                                                                                 | `4096`   | Type: int                 |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| `--torchao-config`                               | Optimize the model with torchao. Experimental feature. Current choices are: int8dq, int8wo, int4wo-<group_size>, fp8wo, fp8dq-per_tensor, fp8dq-per_row                                                                                 | ``       | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-nan-detection`                         | Enable the NaN detection for debugging purposes.                                                                                                                                                                                        | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-p2p-check`                             | Enable P2P check for GPU access, otherwise the p2p access is allowed by default.                                                                                                                                                        | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--triton-attention-reduce-in-fp32`              | Cast the intermediate attention results to fp32 to avoid possible crashes related to fp16. This only affects Triton attention kernels.                                                                                                  | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--triton-attention-num-kv-splits`               | The number of KV splits in flash decoding Triton kernel. Larger value is better in longer context scenarios. The default value is 8.                                                                                                    | `8`      | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--triton-attention-split-tile-size`             | The size of split KV tile in flash decoding Triton kernel. Used for deterministic inference.                                                                                                                                            | `None`   | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--num-continuous-decode-steps`                  | Run multiple continuous decoding steps to reduce scheduling overhead. This can potentially increase throughput but may also increase time-to-first-token latency. The default value is 1, meaning only run one decoding step at a time. | `1`      | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--delete-ckpt-after-loading`                    | Delete the model checkpoint after loading the model.                                                                                                                                                                                    | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-memory-saver`                          | Allow saving memory using release_memory_occupation and resume_memory_occupation                                                                                                                                                        | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-weights-cpu-backup`                    | Save model weights to CPU memory during release_weights_occupation and resume_weights_occupation                                                                                                                                        | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--allow-auto-truncate`                          | Allow automatically truncating requests that exceed the maximum input length instead of returning an error.                                                                                                                             | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-custom-logit-processor`                | Enable users to pass custom logit processors to the server (disabled by default for security)                                                                                                                                           | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--flashinfer-mla-disable-ragged`                | Not using ragged prefill wrapper when running flashinfer mla                                                                                                                                                                            | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-shared-experts-fusion`                | Disable shared experts fusion optimization for deepseek v3/r1.                                                                                                                                                                          | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-chunked-prefix-cache`                 | Disable chunked prefix cache feature for deepseek, which should save overhead for short sequences.                                                                                                                                      | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disable-fast-image-processor`                 | Adopt base image processor instead of fast image processor.                                                                                                                                                                             | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--keep-mm-feature-on-device`                    | Keep multimodal feature tensors on device after processing to save D2H copy.                                                                                                                                                            | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-return-hidden-states`                  | Enable returning hidden states with responses.                                                                                                                                                                                          | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--scheduler-recv-interval`                      | The interval to poll requests in scheduler. Can be set to >1 to reduce the overhead of this.                                                                                                                                            | `1`      | Type: int                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--numa-node`                                    | Sets the numa node for the subprocesses. i-th element corresponds to i-th subprocess.                                                                                                                                                   | `None`   | List[int]                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-layerwise-nvtx-marker`                 | Enable layerwise NVTX profiling annotations for the model. This adds NVTX markers to every layer for detailed per-layer performance analysis with Nsight Systems.                                                                       | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-attn-tp-input-scattered`               | Allow input of attention to be scattered when only using tensor parallelism, to reduce the computational load of operations such as qkv latent.                                                                                         | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-nsa-prefill-context-parallel`          | Context parallelism used in the long sequence prefill phase of DeepSeek v3.2                                                                                                                                                            | `False`  | bool flag (set to enable) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Forward hooks
+
+| Argument          | Description                                                                                                                                                                                                                                                                                                                                                                       | Defaults | Options         |              A2 Supported              |              A3 Supported              |
+|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-----------------|:--------------------------------------:|:--------------------------------------:|
+| `--forward-hooks` | JSON-formatted list of forward hook specifications. Each element must include `target_modules` (list of glob patterns matched against `model.named_modules()` names) and `hook_factory` (Python import path to a factory, e.g. `my_package.hooks:make_hook`). An optional `name` field is used for logging, and an optional `config` object is passed as a `dict` to the factory. | `None`   | Type: JSON list | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## Debug tensor dumps
+
+| Argument                                  | Description                                                                                                   | Defaults | Options                   |               A2 Supported               |               A3 Supported               |
+|-------------------------------------------|---------------------------------------------------------------------------------------------------------------|----------|---------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--debug-tensor-dump-input-file`          | The input filename for dumping tensors                                                                        | `None`   | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--debug-tensor-dump-inject`              | Inject the outputs from jax as the input of every layer.                                                      | `False`  | Type: str                 |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--enable-dynamic-batch-tokenizer`        | Enable async dynamic batch tokenizer for improved performance when multiple requests arrive concurrently.     | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--dynamic-batch-tokenizer-batch-size`    | [Only used if --enable-dynamic-batch-tokenizer is set] Maximum batch size for dynamic batch tokenizer.        | `32`     | Type: int                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--dynamic-batch-tokenizer-batch-timeout` | [Only used if --enable-dynamic-batch-tokenizer is set] Timeout in seconds for batching tokenization requests. | `0.002`  | Type: float               | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## PD disaggregation
+
+| Argument                                         | Description                                                                                                                                                                                                                                                                                            | Defaults   | Options                              |               A2 Supported               |               A3 Supported               |
+|--------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|--------------------------------------|:----------------------------------------:|:----------------------------------------:|
+| `--disaggregation-mode`                          | Only used for PD disaggregation. "prefill" for prefill-only server, and "decode" for decode-only server. If not specified, it is not PD disaggregated                                                                                                                                                  | `null`     | `null`, `prefill`, `decode`          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disaggregation-transfer-backend`              | The backend for disaggregation transfer. Default is mooncake.                                                                                                                                                                                                                                          | `mooncake` | `mooncake`, `nixl`, `ascend`, `fake` | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disaggregation-bootstrap-port`                | Bootstrap server port on the prefill server. Default is 8998.                                                                                                                                                                                                                                          | `8998`     | Type: int                            | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disaggregation-decode-tp`                     | Decode tp size. If not set, it matches the tp size of the current engine. This is only set on the prefill server.                                                                                                                                                                                      | `None`     | Type: int                            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disaggregation-decode-dp`                     | Decode dp size. If not set, it matches the dp size of the current engine. This is only set on the prefill server.                                                                                                                                                                                      | `None`     | Type: int                            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disaggregation-prefill-pp`                    | Prefill pp size. If not set, it is default to 1. This is only set on the decode server.                                                                                                                                                                                                                | `1`        | Type: int                            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disaggregation-ib-device`                     | The InfiniBand devices for disaggregation transfer, accepts single device (e.g., --disaggregation-ib-device mlx5_0) or multiple comma-separated devices (e.g., --disaggregation-ib-device mlx5_0,mlx5_1). Default is None, which triggers automatic device detection when mooncake backend is enabled. | `None`     | Type: str                            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--disaggregation-decode-enable-offload-kvcache` | Enable async KV cache offloading on decode server (PD mode).                                                                                                                                                                                                                                           | `False`    | bool flag (set to enable)            |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| `--num-reserved-decode-tokens`                   | Number of decode tokens that will have memory reserved when adding new request to the running batch.                                                                                                                                                                                                   | `512`      | Type: int                            | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--disaggregation-decode-polling-interval`       | The interval to poll requests in decode server. Can be set to >1 to reduce the overhead of this.                                                                                                                                                                                                       | `1`        | Type: int                            | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+
+## Custom weight loader
+
+| Argument                                                     | Description                                                                                                                       | Defaults | Options                   |               A2 Supported               | A3 Supported                             |
+|--------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|----------|---------------------------|:----------------------------------------:|------------------------------------------|
+| `--custom-weight-loader`                                     | The custom dataloader which used to update the model. Should be set with a valid import path, such as my_package.weight_load_func | `None`   | List[str]                 |  **<span style="color: red;">×</span>**  | **<span style="color: red;">×</span>**   |
+| `--weight-loader-disable-mmap`                               | Disable mmap while loading weight using safetensors.                                                                              | `False`  | bool flag (set to enable) | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| `--remote-instance-weight-loader-seed-instance-ip`           | The ip of the seed instance for loading weights from remote instance.                                                             | `None`   | Type: str                 |  **<span style="color: red;">×</span>**  | **<span style="color: red;">×</span>**   |
+| `--remote-instance-weight-loader-seed-instance-service-port` | The service port of the seed instance for loading weights from remote instance.                                                   | `None`   | Type: int                 |  **<span style="color: red;">×</span>**  | **<span style="color: red;">×</span>**   |
+| `--remote-instance-weight-loader-send-weights-group-ports`   | The communication group ports for loading weights from remote instance.                                                           | `None`   | Type: JSON list           |  **<span style="color: red;">×</span>**  | **<span style="color: red;">×</span>**   |
+
+## For PD-Multiplexing
+
+| Argument              | Description                                            | Defaults | Options                   |              A2 Supported              |              A3 Supported              |
+|-----------------------|--------------------------------------------------------|----------|---------------------------|:--------------------------------------:|:--------------------------------------:|
+| `--enable-pdmux`      | Enable PD-Multiplexing, PD running on greenctx stream. | `False`  | bool flag (set to enable) | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--pdmux-config-path` | The path of the PD-Multiplexing config file.           | `None`   | Type: str                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+| `--sm-group-num`      | Number of sm partition groups.                         | `8`      | Type: int                 | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
+
+## For deterministic inference
+
+| Argument                           | Description                                                   | Defaults | Options                   | A2 Supported                           | A3 Supported                           |
+|------------------------------------|---------------------------------------------------------------|----------|---------------------------|----------------------------------------|----------------------------------------|
+| `--enable-deterministic-inference` | Enable deterministic inference mode with batch invariant ops. | `False`  | bool flag (set to enable) | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
diff --git a/docs/platforms/ascend_npu_support_models.md b/docs/platforms/ascend_npu_support_models.md
new file mode 100644
index 000000000..42ad6aec7
--- /dev/null
+++ b/docs/platforms/ascend_npu_support_models.md
@@ -0,0 +1,97 @@
+# Support Models on Ascend NPU
+
+This section describes the models supported on the Ascend NPU, including Large Language Models, Multimodal Language
+Models, Embedding Models, and Rerank Models. Mainstream DeepSeek/Qwen/GLM series are included.
+You are welcome to enable various models based on your business requirements.
+
+## Large Language Models
+
+| Models                                    | Model Family                   |               A2 Supported               |               A3 Supported               |
+|-------------------------------------------|--------------------------------|:----------------------------------------:|:----------------------------------------:|
+| DeepSeek V3/V3.1                          | DeepSeek                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| DeepSeek V3.2                             | DeepSeek                       |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| DeepSeek R1                               | DeepSeek                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| DeepSeek V2                               | DeepSeek                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Qwen3                                     | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Qwen3-MoE                                 | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Qwen3-Next                                | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Qwen3-Coder                               | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Qwen2.5                                   | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| QwQ-32B                                   | Qwen                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| meta-llama/Llama-4-Scout-17B-16E-Instruct | Llama                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| AI-ModelScope/Llama-3.1-8B-Instruct       | Llama                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| LLM-Research/Llama-3.2-1B-Instruct        | Llama                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| mistralai/Mistral-7B-Instruct-v0.2        | Mistral                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| google/gemma-3-4b-it                      | Gemma                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| microsoft/Phi-4-multimodal-instruct       | Phi                            | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| allenai/OLMoE-1B-7B-0924                  | OLMoE                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| stabilityai/stablelm-2-1_6b               | StableLM                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| CohereForAI/c4ai-command-r-v01            | Command-R                      |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| huihui-ai/grok-2                          | Grok                           |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| ZhipuAI/chatglm2-6b                       | ChatGLM                        |  **<span style="color: red;">×</span>**  | **<span style="color: green;">√</span>** |
+| Shanghai_AI_Laboratory/internlm2-7b       | InternLM 2                     | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct      | ExaONE 3                       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| xverse/XVERSE-MoE-A36B                    | XVERSE                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| HuggingFaceTB/SmolLM-1.7B                 | SmolLM                         | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| ZhipuAI/glm-4-9b-chat                     | GLM-4                          | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| XiaomiMiMo/MiMo-7B-RL                     | MiMo                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| arcee-ai/AFM-4.5B-Base                    | Arcee AFM-4.5B                 | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| Howeee/persimmon-8b-chat                  | Persimmon                      | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| inclusionAI/Ling-lite                     | Ling                           | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| ibm-granite/granite-3.1-8b-instruct       | Granite                        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| ibm-granite/granite-3.0-3b-a800m-instruct | Granite MoE                    | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| databricks/dbrx-instruct                  | DBRX (Databricks)              |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| baichuan-inc/Baichuan2-13B-Chat           | Baichuan 2 (7B, 13B)           |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| baidu/ERNIE-4.5-21B-A3B-PT                | ERNIE-4.5 (4.5, 4.5MoE series) |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| openbmb/MiniCPM3-4B                       | MiniCPM (v3, 4B)               |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+| openai/gpt-oss-120b                       | GPTOSS                         |  **<span style="color: red;">×</span>**  |  **<span style="color: red;">×</span>**  |
+
+## Multimodal Language Models
+
+| Models                                        | Model Family (Variants)   | A2 Supported                             |               A3 Supported               |
+|-----------------------------------------------|---------------------------|------------------------------------------|:----------------------------------------:|
+| Qwen2.5-VL-72B-Instruct-w8a8                  | Qwen-VL                   | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| Qwen3-VL-30B-A3B-Instruct                     | Qwen-VL                   | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| Qwen3-VL-8B-Instruct                          | Qwen-VL                   | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| Qwen3-VL-4B-Instruct                          | Qwen-VL                   | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| Qwen3-VL-235B-A22B-Instruct                   | Qwen-VL                   | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| deepseek-ai/deepseek-vl2                      | DeepSeek-VL2              | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| deepseek-ai/Janus-Pro-7B                      | Janus-Pro (1B, 7B)        | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| openbmb/MiniCPM-V-2_6                         | MiniCPM-V / MiniCPM-o     | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| google/gemma-3-4b-it                          | Gemma 3 (Multimodal)      | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| mistralai/Mistral-Small-3.1-24B-Instruct-2503 | Mistral-Small-3.1-24B     | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| microsoft/Phi-4-multimodal-instruct           | Phi-4-multimodal-instruct | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| XiaomiMiMo/MiMo-VL-7B-RL                      | MiMo-VL (7B)              | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| AI-ModelScope/llava-v1.6-34b                  | LLaVA (v1.5 & v1.6)       | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| lmms-lab/llava-next-72b                       | LLaVA-NeXT (8B, 72B)      | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| lmms-lab/llava-onevision-qwen2-7b-ov          | LLaVA-OneVision           | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| Kimi/Kimi-VL-A3B-Instruct                     | Kimi-VL (A3B)             | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| ZhipuAI/GLM-4.5V                              | GLM-4.5V (106B)           | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| meta-llama/Llama-3.2-11B-Vision-Instruct      | Llama 3.2 Vision (11B)    | **<span style="color: red;">×</span>**   |  **<span style="color: red;">×</span>**  |
+
+## Embedding Models
+
+| Models                                    | Model Family             | A2 Supported                             |               A3 Supported               |
+|-------------------------------------------|--------------------------|------------------------------------------|:----------------------------------------:|
+| 	intfloat/e5-mistral-7b-instruct          | E5 (Llama/Mistral based) | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| 	iic/gte_Qwen2-1.5B-instruct              | GTE-Qwen2                | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| 	Qwen/Qwen3-Embedding-8B                  | Qwen3-Embedding          | **<span style="color: red;">×</span>**   |  **<span style="color: red;">×</span>**  |
+| 	Alibaba-NLP/gme-Qwen2-VL-2B-Instruct     | GME (Multimodal)         | **<span style="color: red;">×</span>**   | **<span style="color: green;">√</span>** |
+| 	AI-ModelScope/clip-vit-large-patch14-336 | CLIP                     | **<span style="color: green;">√</span>** | **<span style="color: green;">√</span>** |
+| 	BAAI/bge-large-en-v1.5                   | BGE                      | **<span style="color: red;">×</span>**   |  **<span style="color: red;">×</span>**  |
+
+## Reward Models
+
+| Models                                      | Model Family              | A2 Supported                           |               A3 Supported               |
+|---------------------------------------------|---------------------------|----------------------------------------|:----------------------------------------:|
+| 	Skywork/Skywork-Reward-Llama-3.1-8B-v0.2   | Llama3.1 Reward           | **<span style="color: red;">×</span>** | **<span style="color: green;">√</span>** |
+| 	Shanghai_AI_Laboratory/internlm2-7b-reward | InternLM 2 Reward         | **<span style="color: red;">×</span>** | **<span style="color: green;">√</span>** |
+| 	Qwen/Qwen2.5-Math-RM-72B                   | Qwen2.5 Reward - Math     | **<span style="color: red;">×</span>** | **<span style="color: green;">√</span>** |
+| 	jason9693/Qwen2.5-1.5B-apeach              | Qwen2.5 Reward - Sequence | **<span style="color: red;">×</span>** | **<span style="color: green;">√</span>** |
+| 	Skywork/Skywork-Reward-Gemma-2-27B-v0.2    | Gemma 2-27B Reward        | **<span style="color: red;">×</span>** |  **<span style="color: red;">×</span>**  |
+
+## Rerank Models
+
+| Models                  | Model Family |              A2 Supported              |              A3 Supported              |
+|-------------------------|--------------|:--------------------------------------:|:--------------------------------------:|
+| BAAI/bge-reranker-v2-m3 | BGE-Reranker | **<span style="color: red;">×</span>** | **<span style="color: red;">×</span>** |
diff --git a/python/sglang/srt/configs/model_config.py b/python/sglang/srt/configs/model_config.py
index 74cf0b8b1..370d14536 100644
--- a/python/sglang/srt/configs/model_config.py
+++ b/python/sglang/srt/configs/model_config.py
@@ -409,6 +409,8 @@ class ModelConfig:
             self.attention_arch = AttentionArch.MLA
             self.kv_lora_rank = self.hf_config.kv_lora_rank
             self.qk_rope_head_dim = self.hf_config.qk_rope_head_dim
+            self.qk_nope_head_dim = self.hf_config.qk_nope_head_dim
+            self.not_use_fused_infer_attention_score = True
         elif "DeepseekVL2ForCausalLM" in self.hf_config.architectures and getattr(
             self.hf_text_config, "use_mla", True
         ):
@@ -416,6 +418,7 @@ class ModelConfig:
             self.attention_arch = AttentionArch.MLA
             self.kv_lora_rank = self.hf_text_config.kv_lora_rank
             self.qk_rope_head_dim = self.hf_text_config.qk_rope_head_dim
+            self.qk_nope_head_dim = self.hf_text_config.qk_nope_head_dim
         elif "KimiVLForConditionalGeneration" in self.hf_config.architectures:
             self.head_dim = 256
             self.attention_arch = AttentionArch.MLA
@@ -423,6 +426,7 @@ class ModelConfig:
             self.qk_rope_head_dim = self.hf_text_config.qk_rope_head_dim
             self.v_head_dim = self.hf_text_config.v_head_dim
             self.qk_nope_head_dim = self.hf_text_config.qk_nope_head_dim
+            self.index_head_dim = None
         elif "KimiLinearForCausalLM" in self.hf_config.architectures:
             self.head_dim = 72
             self.attention_arch = AttentionArch.MLA
@@ -447,6 +451,9 @@ class ModelConfig:
                     ):
                         setattr(self.hf_text_config, "head_dim", self.head_dim)
 
+            elif "BaichuanForCausalLM" in self.hf_config.architectures:
+                self.use_alibi = True if self.hf_config.hidden_size !=4096 else False
+
             self.attention_arch = AttentionArch.MHA
 
         self.num_attention_heads = self.hf_text_config.num_attention_heads
diff --git a/python/sglang/srt/entrypoints/EngineBase.py b/python/sglang/srt/entrypoints/EngineBase.py
index 5d3162afd..a842d62cd 100644
--- a/python/sglang/srt/entrypoints/EngineBase.py
+++ b/python/sglang/srt/entrypoints/EngineBase.py
@@ -29,6 +29,7 @@ class EngineBase(ABC):
         bootstrap_port: Optional[Union[List[int], int]] = None,
         bootstrap_room: Optional[Union[List[int], int]] = None,
         data_parallel_rank: Optional[int] = None,
+        data_parallel_rank_decode: Optional[int] = None,
         rid: Optional[Union[List[str], str]] = None,
     ) -> Union[Dict, Iterator[Dict]]:
         """Generate outputs based on given inputs."""
diff --git a/python/sglang/srt/entrypoints/engine.py b/python/sglang/srt/entrypoints/engine.py
index 5050a65e0..f52ae9d76 100644
--- a/python/sglang/srt/entrypoints/engine.py
+++ b/python/sglang/srt/entrypoints/engine.py
@@ -292,6 +292,7 @@ class Engine(EngineBase):
         bootstrap_port: Optional[Union[List[int], int]] = None,
         bootstrap_room: Optional[Union[List[int], int]] = None,
         data_parallel_rank: Optional[int] = None,
+        data_parallel_rank_decode: Optional[int] = None,
         rid: Optional[Union[List[str], str]] = None,
     ) -> Union[Dict, Iterator[Dict]]:
         """
@@ -328,6 +329,7 @@ class Engine(EngineBase):
             bootstrap_port=bootstrap_port,
             bootstrap_room=bootstrap_room,
             data_parallel_rank=data_parallel_rank,
+            data_parallel_rank_decode=data_parallel_rank_decode,
             rid=rid,
         )
         generator = self.tokenizer_manager.generate_request(obj, None)
@@ -377,6 +379,7 @@ class Engine(EngineBase):
         bootstrap_port: Optional[Union[List[int], int]] = None,
         bootstrap_room: Optional[Union[List[int], int]] = None,
         data_parallel_rank: Optional[int] = None,
+        data_parallel_rank_decode: Optional[int] = None,
         rid: Optional[Union[List[str], str]] = None,
     ) -> Union[Dict, AsyncIterator[Dict]]:
         """
@@ -414,6 +417,7 @@ class Engine(EngineBase):
             bootstrap_port=bootstrap_port,
             bootstrap_room=bootstrap_room,
             data_parallel_rank=data_parallel_rank,
+            data_parallel_rank_decode=data_parallel_rank_decode,
             rid=rid,
         )
         generator = self.tokenizer_manager.generate_request(obj, None)
diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py
index 99e07a9a0..236495c5e 100644
--- a/python/sglang/srt/entrypoints/http_server.py
+++ b/python/sglang/srt/entrypoints/http_server.py
@@ -1515,7 +1515,11 @@ def _execute_server_warmup(
         # TODO Workaround the bug that embedding errors for list of size 1
         if server_args.dp_size == 1:
             json_data["input_ids"] = json_data["input_ids"][0]
-    elif is_vlm and server_args.disaggregation_mode == "null":
+    elif (
+        is_vlm
+        and server_args.disaggregation_mode == "null"
+        and model_info["is_generation"]
+    ):
         # TODO: ChatCompletionRequest does not have bootstrap info required by disaggregation mode, disable image-warmup for now
         json_data = {
             "model": _global_state.tokenizer_manager.served_model_name,
diff --git a/python/sglang/srt/entrypoints/openai/protocol.py b/python/sglang/srt/entrypoints/openai/protocol.py
index 34aa364cf..07cbe2155 100644
--- a/python/sglang/srt/entrypoints/openai/protocol.py
+++ b/python/sglang/srt/entrypoints/openai/protocol.py
@@ -536,6 +536,7 @@ class ChatCompletionRequest(BaseModel):
 
     # For data parallel rank routing
     data_parallel_rank: Optional[int] = None
+    data_parallel_rank_decode: Optional[int] = None
 
     # OpenAI/SGLang default sampling parameters
     _DEFAULT_SAMPLING_PARAMS = {
diff --git a/python/sglang/srt/entrypoints/openai/serving_chat.py b/python/sglang/srt/entrypoints/openai/serving_chat.py
index 354aa6c1d..443452409 100644
--- a/python/sglang/srt/entrypoints/openai/serving_chat.py
+++ b/python/sglang/srt/entrypoints/openai/serving_chat.py
@@ -211,7 +211,9 @@ class OpenAIServingChat(OpenAIServingBase):
             bootstrap_host=request.bootstrap_host,
             bootstrap_port=request.bootstrap_port,
             bootstrap_room=request.bootstrap_room,
+            # For data parallel rank routing
             data_parallel_rank=request.data_parallel_rank,
+            data_parallel_rank_decode=request.data_parallel_rank_decode,
             return_hidden_states=request.return_hidden_states,
             rid=request.rid,
             extra_key=self._compute_extra_key(request),
diff --git a/python/sglang/srt/environ.py b/python/sglang/srt/environ.py
index 9fceae6fe..aacd4841b 100644
--- a/python/sglang/srt/environ.py
+++ b/python/sglang/srt/environ.py
@@ -375,6 +375,9 @@ class Envs:
     # Metrics
     SGLANG_ENABLE_METRICS_DEVICE_TIMER = EnvBool(False)
 
+    # Attention
+    SGLANG_USE_PAGED_ATTENTION = EnvBool(False)
+
     # fmt: on
 
 
diff --git a/python/sglang/srt/eplb/async_eplb_manager.py b/python/sglang/srt/eplb/async_eplb_manager.py
new file mode 100644
index 000000000..3de405ecd
--- /dev/null
+++ b/python/sglang/srt/eplb/async_eplb_manager.py
@@ -0,0 +1,157 @@
+import logging
+from multiprocessing import Manager, Process, Queue
+from typing import TYPE_CHECKING
+
+import torch
+
+from sglang.srt.eplb.eplb_manager import EPLBManager
+from sglang.srt.eplb.expert_distribution import get_global_expert_distribution_recorder
+from sglang.srt.eplb.expert_location import ExpertLocationMetadata
+
+if TYPE_CHECKING:
+    from sglang.srt.model_executor.model_runner import ModelRunner
+
+
+logger = logging.getLogger(__name__)
+
+
+class AsyncEPLBManager(EPLBManager):
+    def __init__(self, model_runner: "ModelRunner"):
+        super().__init__(model_runner)
+
+        self.device = self._server_args.device
+
+        if torch.distributed.is_initialized():
+            self.rank = torch.distributed.get_rank()
+        else:
+            self.rank = 0
+
+        self.planner_q = Queue()
+        self.block_q = Queue(maxsize=1)
+
+        self.num_wait_worker_iterations = 40
+
+        self.manager = Manager()
+        self.shared_dict = self.manager.dict(
+            {
+                "moe_load": None,
+            }
+        )
+
+        self.eplb = EplbProcess(
+            shared_dict=self.shared_dict,
+            planner_q=self.planner_q,
+            block_q=self.block_q,
+            server_args=self._server_args,
+            model_config=self._model_runner.model_config,
+            rank=self.rank,
+        )
+
+        self.eplb_process = self.eplb.launch_process()
+
+        logger.info(
+            f"[ModelRunner] Launched EPLB process (pid={self.eplb_process.pid})"
+        )
+
+    def _entrypoint(self):
+        while True:
+            for _ in range(self._rebalance_num_iterations):
+                yield
+            self.forward_eplb_process()
+            for _ in range(self.num_wait_worker_iterations):
+                yield
+            self.take_update_info_from_eplb_process()
+            yield from self.forward_update_weight()
+
+    def forward_eplb_process(self):
+        logger.info("[EPLBManager] rebalance start")
+        logical_count = get_global_expert_distribution_recorder().dump_record(
+            output_mode="object"
+        )["logical_count"]
+        self.shared_dict["moe_load"] = logical_count.cpu()
+        self.wakeup_eplb_worker()
+
+    def forward_update_weight(self):
+        expert_location_metadata = self.to_device(self.update_info_all)
+
+        update_layer_ids_chunks = self._compute_update_layer_ids_chunks()
+        for chunk_index, update_layer_ids in enumerate(update_layer_ids_chunks):
+            if len(update_layer_ids_chunks) > 1:
+                yield
+
+            yield from self._model_runner.update_expert_location(
+                expert_location_metadata,
+                update_layer_ids=update_layer_ids,
+            )
+
+    def to_device(self, metadata):
+        fields = (
+            "physical_to_logical_map",
+            "logical_to_all_physical_map",
+            "logical_to_all_physical_map_num_valid",
+            "logical_to_rank_dispatch_physical_map",
+        )
+        for name in fields:
+            t = getattr(metadata, name, None)
+            if t is None:
+                continue
+            if hasattr(t, "device") and t.device != self.device:
+                setattr(metadata, name, t.to(self.device, non_blocking=True))
+        return metadata
+
+    def take_update_info_from_eplb_process(self):
+        self.update_info_all = self.block_q.get()
+
+    def wakeup_eplb_worker(self):
+        self.planner_q.put(1)
+
+
+class EplbProcess:
+    def __init__(
+        self,
+        shared_dict,
+        planner_q,
+        block_q,
+        server_args,
+        model_config,
+        rank,
+    ):
+        self.shared_dict = shared_dict
+        self.planner_q = planner_q
+        self.block_q = block_q
+
+        self._server_args = server_args
+        self._model_config = model_config
+        self.rank = rank
+        self._server_args.device = "cpu"
+
+    def do_algorithm(self):
+        logical_count = self.shared_dict["moe_load"]
+        return ExpertLocationMetadata.init_by_eplb(
+            self._server_args, self._model_config, logical_count, self.rank
+        )
+
+    def worker_process(self, planner_q, block_q):
+        while True:
+            try:
+                planner_q.get()
+                update_info = self.do_algorithm()
+
+                while True:
+                    if not block_q.empty():
+                        continue
+                    block_q.put(update_info)
+                    break
+
+            except Exception as e:
+                logger.warning(
+                    f"[EPLB process] Exiting due to error:{e}", exc_info=True
+                )
+                break
+
+    def launch_process(self):
+        proc = Process(
+            target=self.worker_process, args=(self.planner_q, self.block_q), daemon=True
+        )
+        proc.start()
+        return proc
diff --git a/python/sglang/srt/hardware_backend/npu/allocator_npu.py b/python/sglang/srt/hardware_backend/npu/allocator_npu.py
index dd7221287..e863de62b 100644
--- a/python/sglang/srt/hardware_backend/npu/allocator_npu.py
+++ b/python/sglang/srt/hardware_backend/npu/allocator_npu.py
@@ -182,3 +182,21 @@ class NPUPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):
 
         self.free_pages = self.free_pages[num_new_pages:]
         return out_indices.int()
+
+    def free(self, free_index: torch.Tensor):
+        if free_index.numel() == 0:
+            return
+
+        if self.is_not_in_free_group:
+            device = free_index.device
+            free_page_indices = torch.unique(free_index.cpu() // self.page_size)
+            free_page_indices = free_page_indices.to(device)
+            if self.need_sort:
+                self.release_pages = torch.cat((free_page_indices, self.release_pages))
+            else:
+                self.free_pages = torch.cat((free_page_indices, self.free_pages))
+        else:
+            self.free_group.append(free_index)
+
+        if self.debug_mode:
+            assert len(torch.unique(self.free_pages)) == len(self.free_pages)
diff --git a/python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py b/python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py
index 6a7fa7127..3ef86347d 100644
--- a/python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py
+++ b/python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py
@@ -18,6 +18,7 @@ from sglang.srt.layers.dp_attention import get_attention_tp_size
 from sglang.srt.layers.radix_attention import AttentionType
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode
 from sglang.srt.speculative.spec_info import SpecInput
+from sglang.srt.environ import envs
 from sglang.srt.utils import get_bool_env_var
 
 if TYPE_CHECKING:
@@ -239,9 +240,10 @@ class AscendAttnBackend(AttentionBackend):
             self.kv_lora_rank = model_runner.model_config.kv_lora_rank
             self.qk_rope_head_dim = model_runner.model_config.qk_rope_head_dim
             self.qk_nope_head_dim = model_runner.model_config.qk_nope_head_dim
-            self.q_head_dim = (
-                self.qk_rope_head_dim + model_runner.model_config.qk_nope_head_dim
-            )
+            if hasattr(model_runner.model_config, "not_use_fused_infer_attention_score"):
+                self.not_use_fused_infer_attention_score = True
+        else:
+            self.use_alibi = hasattr(model_runner.model_config, "use_alibi") and model_runner.model_config.use_alibi
         self.native_attn = TorchNativeAttnBackend(model_runner)
         self.graph_metadata = {}
         self.max_context_len = model_runner.model_config.context_len
@@ -422,6 +424,116 @@ class AscendAttnBackend(AttentionBackend):
     def get_cuda_graph_seq_len_fill_value(self):
         return 0
 
+    def _generate_alibi_bias(
+        self,
+        seq_len: int,
+        slopes: torch.Tensor,
+        num_heads: int,
+        device: torch.device,
+        dtype: torch.dtype = torch.bfloat16
+    ) -> torch.Tensor:
+        position_point = torch.arange(seq_len) - seq_len + 1
+        position_point = position_point.unsqueeze(0).unsqueeze(0).expand(num_heads, -1, -1)
+        diag = torch.diag(position_point[0])
+        position_point = position_point - diag.unsqueeze(0).unsqueeze(0).transpose(-1, -2)
+        position_point = position_point.to(device)
+        alibi = slopes.unsqueeze(1).unsqueeze(1) * position_point
+        alibi_bias = alibi.view(num_heads, 1, seq_len)
+        alibi_bias = alibi_bias.to(device).to(dtype)
+        return alibi_bias
+    
+    def generate_alibi_bias(
+        self,
+        q_seq_len: int,
+        kv_seq_len: int,
+        slopes: torch.Tensor,
+        num_heads: int,
+        device: torch.device,
+        is_extend: bool = True,
+        dtype: torch.dtype = torch.bfloat16
+    ) -> torch.Tensor:
+        if not hasattr(self, "alibi_bias") or self.alibi_bias is None:
+            MAX_LEN_ALB=5000
+            max_seq_len=max(kv_seq_len,q_seq_len)
+            max_seq_len=max(max_seq_len,MAX_LEN_ALB)
+            self.alibi_bias = self._generate_alibi_bias(max_seq_len,slopes,num_heads,device,dtype)
+            
+        if not hasattr(self, "super_mask") or self.super_mask is None:
+            MAX_LEN_ALB=5000
+            max_seq_len=max(kv_seq_len,q_seq_len)
+            max_seq_len=max(max_seq_len,MAX_LEN_ALB)
+            super_mask = torch.ones(size=(1, max_seq_len, max_seq_len), dtype=dtype)
+            super_mask = super_mask.float().fill_(float("-inf")).type_as(super_mask)
+            super_mask = torch.triu(super_mask, 1).to(device)
+            self.super_mask = super_mask
+        if is_extend:
+            return self.alibi_bias[:, :q_seq_len, :kv_seq_len] + self.super_mask[:, :q_seq_len, :kv_seq_len]
+        else:
+            return self.alibi_bias[:, :q_seq_len, :kv_seq_len]
+
+        
+    
+    def attn_alibi(self,q,k_cache,v_cache,block_tables,seq_lens,query_lens,scale_value,num_heads,slopes,is_extend):
+        curr=0
+        num_prompts=query_lens.shape[0]
+        head_size = k_cache.shape[3]
+        head_size_v = v_cache.shape[3]
+        block_size = k_cache.shape[1]
+        attn_output = []
+        for i in range(num_prompts):
+            seq_len = seq_lens[i].item()
+            block_table = block_tables[i]
+
+            j= torch.arange(seq_len,device=block_table.device)
+
+            block_number = block_table[j // block_size]
+            block_offset = j % block_size
+
+            k = k_cache[block_number,block_offset]
+            v = v_cache[block_number,block_offset]
+            k = k.view(seq_len, num_heads, head_size)
+            v = v.view(seq_len, num_heads, head_size_v)
+            
+            if is_extend:
+                q_len = query_lens[i].item()
+                querys = q[curr :curr + q_len]
+                assert q_len==seq_len, "Warning: Q sequence length is not consistant with KV sequence length during Prefill phase."
+            else:
+                q_len = 1
+                querys = q[curr :curr + 1]
+            
+            querys = querys.to(torch.float32)
+            querys = querys * scale_value
+            querys = querys.permute(1, 0, 2)
+            k = k.permute(1, 2, 0)
+
+            score = torch.bmm(querys, k)
+            score = score.to(torch.float32)
+            if slopes is not None:
+                alibi_bias = self.generate_alibi_bias(
+                    q_seq_len=q_len,
+                    kv_seq_len=seq_len,
+                    slopes=slopes,
+                    num_heads=num_heads,
+                    device=q.device,
+                    is_extend=is_extend,
+                    dtype=querys.dtype
+                )
+                score = score + alibi_bias
+            score = torch.max(
+                score, torch.tensor(torch.finfo(score.dtype).min)
+            )
+            p = torch.nn.functional.softmax(score, dim=-1)
+            v = v.permute(1,0,2)
+            out = torch.bmm(p, v)
+            out = out.permute(1, 0, 2)
+            out = out.reshape(-1, num_heads * head_size_v)
+            attn_output.append(out)
+            curr += q_len
+        attn_output = torch.cat(attn_output, dim=0).to(q.dtype).to(q.device)
+        attn_output = attn_output.view(-1, num_heads * head_size)
+        return attn_output
+    
     def do_cp_balance_attn(
         self,
         q_nope,
@@ -610,6 +722,7 @@ class AscendAttnBackend(AttentionBackend):
         q_rope: Optional[torch.Tensor] = None,
         k_rope: Optional[torch.Tensor] = None,
         topk_indices: Optional[torch.Tensor] = None,
+        slopes:  Optional[torch.Tensor] = None,
     ):
         if topk_indices is not None:
             return self.forward_sparse(
@@ -681,27 +794,51 @@ class AscendAttnBackend(AttentionBackend):
                 )
 
             else:
-                if layer.qk_head_dim <= 128:
-                    query = q.reshape(-1, layer.tp_q_head_num * layer.qk_head_dim)
-                    attn_output = torch.empty(
-                        (query.shape[0], layer.tp_q_head_num * layer.v_head_dim),
-                        dtype=query.dtype,
-                        device=query.device,
-                    )
+                causal = True
+                if (
+                    layer.is_cross_attention
+                    or layer.attn_type == AttentionType.ENCODER_ONLY
+                ):
+                    causal = False
+
+                if layer.qk_head_dim <= 128 and layer.logit_cap == 0 and causal:
+                    """When logit_cap > 0, use torch native attention backend instead
+                    cause Ascend attn ops do not support soft-capping attention currently.
+                    """
+                    if not self.use_alibi:
+                        query = q.reshape(-1, layer.tp_q_head_num * layer.qk_head_dim)
+                        attn_output = torch.empty(
+                            (query.shape[0], layer.tp_q_head_num * layer.v_head_dim),
+                            dtype=query.dtype,
+                            device=query.device,
+                        )
 
-                    torch_npu._npu_flash_attention_qlens(
-                        query=query,
-                        key_cache=k_cache,
-                        value_cache=v_cache,
-                        mask=self.mask,
-                        block_table=self.forward_metadata.block_tables,
-                        seq_len=self.forward_metadata.extend_seq_lens_cpu_int,
-                        context_lens=self.forward_metadata.seq_lens_cpu_int,
-                        scale_value=layer.scaling,
-                        num_heads=layer.tp_q_head_num,
-                        num_kv_heads=layer.tp_k_head_num,
-                        out=attn_output,
-                    )
+                        torch_npu._npu_flash_attention_qlens(
+                            query=query,
+                            key_cache=k_cache,
+                            value_cache=v_cache,
+                            mask=self.mask,
+                            block_table=self.forward_metadata.block_tables,
+                            seq_len=self.forward_metadata.extend_seq_lens_cpu_int,
+                            context_lens=self.forward_metadata.seq_lens_cpu_int,
+                            scale_value=layer.scaling,
+                            num_heads=layer.tp_q_head_num,
+                            num_kv_heads=layer.tp_k_head_num,
+                            out=attn_output,
+                        )
+                    else:
+                        attn_output = self.attn_alibi(
+                            q=q.reshape(-1, layer.tp_q_head_num, layer.qk_head_dim),
+                            k_cache=k_cache,
+                            v_cache=v_cache,
+                            block_tables=self.forward_metadata.block_tables,
+                            seq_lens=self.forward_metadata.seq_lens_cpu_int,
+                            query_lens=self.forward_metadata.extend_seq_lens_cpu_int,
+                            scale_value=layer.scaling,
+                            num_heads=layer.tp_q_head_num,
+                            slopes=slopes,
+                            is_extend=True
+                        )
                 else:
                     if layer.qk_head_dim != layer.v_head_dim:
                         attn_output = q.new_empty(
@@ -715,13 +852,6 @@ class AscendAttnBackend(AttentionBackend):
                     q_ = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)
                     o_ = attn_output.view(-1, layer.tp_q_head_num, layer.v_head_dim)
 
-                    causal = True
-                    if (
-                        layer.is_cross_attention
-                        or layer.attn_type == AttentionType.ENCODER_ONLY
-                    ):
-                        causal = False
-
                     self.native_attn._run_sdpa_forward_extend(
                         q_,
                         o_,
@@ -735,6 +865,8 @@ class AscendAttnBackend(AttentionBackend):
                         scaling=layer.scaling,
                         enable_gqa=use_gqa,
                         causal=causal,
+                        logit_cap=layer.logit_cap,
+                        logit_capping_method=layer.logit_capping_method,
                     )
         elif sum(forward_batch.extend_prefix_lens_cpu) > 0:
             num_token_padding = q.shape[0]
@@ -840,43 +972,88 @@ class AscendAttnBackend(AttentionBackend):
             assert (
                 layer.qk_head_dim != layer.v_head_dim
             ), "FIA only supports qk_head_dim != v_head_dim"
+            if not hasattr(self, "not_use_fused_infer_attention_score"):
+                num_token_padding = q.shape[0]
+                q, k, v = [
+                    data[: forward_batch.num_token_non_padded_cpu] for data in [q, k, v]
+                ]
 
-            num_token_padding = q.shape[0]
-            q, k, v = [
-                data[: forward_batch.num_token_non_padded_cpu] for data in [q, k, v]
-            ]
+                q_nope, q_rope = q.split([layer.v_head_dim, self.qk_rope_head_dim], dim=-1)
+                k_nope, k_rope = k.split([layer.v_head_dim, self.qk_rope_head_dim], dim=-1)
 
-            q_nope, q_rope = q.split([layer.v_head_dim, self.qk_rope_head_dim], dim=-1)
-            k_nope, k_rope = k.split([layer.v_head_dim, self.qk_rope_head_dim], dim=-1)
+                attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(
+                    q_nope,
+                    k_nope,
+                    v,
+                    query_rope=q_rope,
+                    key_rope=k_rope,
+                    num_heads=layer.tp_q_head_num,
+                    input_layout="TND",
+                    atten_mask=self.fia_mask,
+                    sparse_mode=3,
+                    actual_seq_lengths=self.forward_metadata.seq_lens_list_cumsum,
+                    actual_seq_lengths_kv=self.forward_metadata.seq_lens_list_cumsum,
+                    scale=layer.scaling,
+                    next_tokens=0,
+                )
 
-            attn_output, _ = torch.ops.npu.npu_fused_infer_attention_score(
-                q_nope,
-                k_nope,
-                v,
-                query_rope=q_rope,
-                key_rope=k_rope,
-                num_heads=layer.tp_q_head_num,
-                input_layout="TND",
-                atten_mask=self.fia_mask,
-                sparse_mode=3,
-                actual_seq_lengths=self.forward_metadata.seq_lens_list_cumsum,
-                actual_seq_lengths_kv=self.forward_metadata.seq_lens_list_cumsum,
-                scale=layer.scaling,
-                next_tokens=0,
-            )
+                attn_output = attn_output.reshape(-1, layer.tp_q_head_num, layer.v_head_dim)
+                if num_token_padding != forward_batch.num_token_non_padded_cpu:
+                    attn_output = torch.cat(
+                        [
+                            attn_output,
+                            attn_output.new_zeros(
+                                num_token_padding - attn_output.shape[0],
+                                *attn_output.shape[1:],
+                            ),
+                        ],
+                        dim=0,
+                    )
+            else:
+                kv_lora_rank = k.shape[-1]-self.qk_rope_head_dim
+                kv_c, k_rope = k.split([kv_lora_rank, self.qk_rope_head_dim], dim=-1)
+                if save_kv_cache:
+                    forward_batch.token_to_kv_pool.set_kv_buffer(layer, forward_batch.out_cache_loc, kv_c, k_rope)
+                attn_output = q.new_empty((q.shape[0], layer.tp_q_head_num, kv_lora_rank))
+                use_gqa = (layer.tp_q_head_num != layer.tp_k_head_num)
+
+                q = q.movedim(0, q.dim() - 2)
+                k = k.movedim(0, q.dim() - 2)
+                v = v.movedim(0, q.dim() - 2)
+                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)
+                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(layer.layer_id)
+
+                curr=0
+                head_size = k_cache.shape[3]
+                head_size_v = v_cache.shape[3]
+                block_size = k_cache.shape[1]
+                for i in range(forward_batch.seq_lens_cpu.shape[0]):
+                    q_len = forward_batch.extend_seq_lens[i]
+                    q_ = q[:, curr:curr + q_len, :]
+
+                    block_table = self.forward_metadata.block_tables[i]
+                    j = torch.arange(q_len, device=block_table.device)
+                    block_number = block_table[j // block_size]
+                    block_offset = j % block_size
+
+                    k_ = k_cache[block_number,block_offset]
+                    v_ = v_cache[block_number,block_offset]
+                    k_ = k_.view(q_len, layer.tp_k_head_num, head_size)
+                    v_ = v_.view(q_len, layer.tp_k_head_num, head_size_v)
+
+                    k_kpe = torch.cat([k_, v_], dim=-1).transpose(0, 1)
+                    o_ = torch.nn.functional.scaled_dot_product_attention(
+                        q_.unsqueeze(0),
+                        k_kpe.unsqueeze(0),
+                        v.unsqueeze(0),
+                        enable_gqa=use_gqa,
+                        scale=layer.scaling,
+                        is_causal=True,
+                    ).squeeze(0).movedim(0, q.dim() - 2)
+
+                    attn_output[curr: curr + q_len, :, :] = o_
+                    curr += q_len
 
-            attn_output = attn_output.reshape(-1, layer.tp_q_head_num, layer.v_head_dim)
-            if num_token_padding != forward_batch.num_token_non_padded_cpu:
-                attn_output = torch.cat(
-                    [
-                        attn_output,
-                        attn_output.new_zeros(
-                            num_token_padding - attn_output.shape[0],
-                            *attn_output.shape[1:],
-                        ),
-                    ],
-                    dim=0,
-                )
         return attn_output
 
     def forward_mtp(
@@ -1084,9 +1261,42 @@ class AscendAttnBackend(AttentionBackend):
                 )
 
         if not self.use_mla:
-            num_tokens = q.shape[0]
-            """PA will support bs<tp in the later version of CANN"""
-            if num_tokens < get_attention_tp_size():
+            if envs.SGLANG_USE_PAGED_ATTENTION.get():
+                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)
+                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(
+                    layer.layer_id
+                )
+                query = q.reshape(-1, layer.tp_q_head_num, layer.qk_head_dim)
+                num_tokens = query.shape[0]
+                attn_output = torch.empty(
+                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),
+                    dtype=query.dtype,
+                    device=query.device,
+                )
+                if self.forward_metadata.seq_lens_cpu_int is None:
+                    actual_seq_len_kv = torch.from_numpy(
+                        np.array(self.forward_metadata.seq_lens_cpu_list).astype(
+                            np.int32
+                        )
+                    )
+                else:
+                    actual_seq_len_kv = self.forward_metadata.seq_lens_cpu_int
+
+                torch_npu._npu_paged_attention(
+                    query=query,
+                    key_cache=k_cache,
+                    value_cache=v_cache,
+                    num_heads=layer.tp_q_head_num,
+                    num_kv_heads=layer.tp_k_head_num,
+                    scale_value=layer.scaling,
+                    block_table=self.forward_metadata.block_tables,
+                    context_lens=actual_seq_len_kv,
+                    out=attn_output,
+                )
+                return attn_output.view(
+                    num_tokens, layer.tp_q_head_num * layer.v_head_dim
+                )
+            else:
                 k_cache = forward_batch.token_to_kv_pool.get_key_buffer(
                     layer.layer_id
                 ).view(-1, self.page_size, layer.tp_k_head_num * layer.qk_head_dim)
@@ -1136,41 +1346,6 @@ class AscendAttnBackend(AttentionBackend):
                     out=[output, softmax_lse],
                 )
                 return output.view(num_tokens, layer.tp_q_head_num * layer.v_head_dim)
-            else:
-                k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)
-                v_cache = forward_batch.token_to_kv_pool.get_value_buffer(
-                    layer.layer_id
-                )
-                query = q.reshape(-1, layer.tp_q_head_num, layer.qk_head_dim)
-                num_tokens = query.shape[0]
-                attn_output = torch.empty(
-                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),
-                    dtype=query.dtype,
-                    device=query.device,
-                )
-                if self.forward_metadata.seq_lens_cpu_int is None:
-                    actual_seq_len_kv = torch.from_numpy(
-                        np.array(self.forward_metadata.seq_lens_cpu_list).astype(
-                            np.int32
-                        )
-                    )
-                else:
-                    actual_seq_len_kv = self.forward_metadata.seq_lens_cpu_int
-
-                torch_npu._npu_paged_attention(
-                    query=query,
-                    key_cache=k_cache,
-                    value_cache=v_cache,
-                    num_heads=layer.tp_q_head_num,
-                    num_kv_heads=layer.tp_k_head_num,
-                    scale_value=layer.scaling,
-                    block_table=self.forward_metadata.block_tables,
-                    context_lens=actual_seq_len_kv,
-                    out=attn_output,
-                )
-                return attn_output.view(
-                    num_tokens, layer.tp_q_head_num * layer.v_head_dim
-                )
         else:
             c_kv, k_rope = forward_batch.token_to_kv_pool.get_kv_buffer(layer.layer_id)
             if is_fia_nz():
@@ -1251,6 +1426,7 @@ class AscendAttnBackend(AttentionBackend):
         q_rope: Optional[torch.Tensor] = None,
         k_rope: Optional[torch.Tensor] = None,
         topk_indices: Optional[torch.Tensor] = None,
+        slopes:  Optional[torch.Tensor] = None,
     ):
         if is_mla_preprocess_enabled():
             # MLAPO does saving kv_cache
@@ -1317,25 +1493,66 @@ class AscendAttnBackend(AttentionBackend):
                     actual_seq_lengths_kv=actual_seq_len_kv,
                     scale=layer.scaling,
                 )
-            else:
+            elif layer.logit_cap == 0:
                 query = q.reshape(-1, layer.tp_q_head_num, layer.qk_head_dim)
                 num_tokens = query.shape[0]
-                attn_output = torch.empty(
-                    (num_tokens, layer.tp_q_head_num, layer.v_head_dim),
-                    dtype=query.dtype,
-                    device=query.device,
-                )
+                if not self.use_alibi:
+                    attn_output = torch.empty(
+                        (num_tokens, layer.tp_q_head_num, layer.v_head_dim),
+                        dtype=query.dtype,
+                        device=query.device,
+                    )
 
-                torch_npu._npu_paged_attention(
-                    query=query,
-                    key_cache=k_cache,
-                    value_cache=v_cache,
-                    num_heads=layer.tp_q_head_num,
-                    num_kv_heads=layer.tp_k_head_num,
-                    scale_value=layer.scaling,
-                    block_table=self.forward_metadata.block_tables,
-                    context_lens=self.forward_metadata.seq_lens_cpu_int,
-                    out=attn_output,
+                    torch_npu._npu_paged_attention(
+                        query=query,
+                        key_cache=k_cache,
+                        value_cache=v_cache,
+                        num_heads=layer.tp_q_head_num,
+                        num_kv_heads=layer.tp_k_head_num,
+                        scale_value=layer.scaling,
+                        block_table=self.forward_metadata.block_tables,
+                        context_lens=self.forward_metadata.seq_lens_cpu_int,
+                        out=attn_output,
+                    )
+                else:
+                    attn_output = self.attn_alibi(
+                        q=query,
+                        k_cache=k_cache,
+                        v_cache=v_cache,
+                        block_tables=self.forward_metadata.block_tables,
+                        seq_lens=self.forward_metadata.seq_lens_cpu_int,
+                        query_lens=torch.ones(num_tokens,dtype=torch.int32),
+                        scale_value=layer.scaling,
+                        num_heads=layer.tp_q_head_num,
+                        slopes=slopes,
+                        is_extend=False
+                    )
+            else:
+                if layer.qk_head_dim != layer.v_head_dim:
+                    attn_output = q.new_empty(
+                        (q.shape[0], layer.tp_q_head_num * layer.v_head_dim)
+                    )
+                else:
+                    attn_output = torch.empty_like(q)
+
+                use_gqa = layer.tp_q_head_num != layer.tp_k_head_num
+
+                q_ = q.view(-1, layer.tp_q_head_num, layer.qk_head_dim)
+                o_ = attn_output.view(-1, layer.tp_q_head_num, layer.v_head_dim)
+
+                self.native_attn._run_sdpa_forward_decode(
+                    q_,
+                    o_,
+                    k_cache.view(-1, layer.tp_k_head_num, layer.qk_head_dim),
+                    v_cache.view(-1, layer.tp_v_head_num, layer.v_head_dim),
+                    forward_batch.req_to_token_pool.req_to_token,
+                    forward_batch.req_pool_indices,
+                    forward_batch.seq_lens,
+                    scaling=layer.scaling,
+                    enable_gqa=use_gqa,
+                    causal=False,
+                    logit_cap=layer.logit_cap,
+                    logit_capping_method=layer.logit_capping_method,
                 )
             return attn_output.view(num_tokens, layer.tp_q_head_num * layer.v_head_dim)
         else:
@@ -1394,7 +1611,8 @@ class AscendAttnBackend(AttentionBackend):
                 assert (
                     self.graph_mode == False
                 )  # _npu_paged_attention_mla not support graph mode
-                q = torch.cat([q, q_rope], dim=-1)
+                if q_rope is not None:
+                    q = torch.cat([q, q_rope], dim=-1)
                 query = q.view(-1, layer.tp_q_head_num, layer.head_dim)
                 kv_c_and_k_pe_cache = torch.cat([kv_c, k_pe], dim=-1)
                 kv_c_and_k_pe_cache = kv_c_and_k_pe_cache.view(
diff --git a/python/sglang/srt/hardware_backend/npu/graph_runner/eagle_draft_npu_graph_runner.py b/python/sglang/srt/hardware_backend/npu/graph_runner/eagle_draft_npu_graph_runner.py
index adca2c6c2..169e2cde3 100644
--- a/python/sglang/srt/hardware_backend/npu/graph_runner/eagle_draft_npu_graph_runner.py
+++ b/python/sglang/srt/hardware_backend/npu/graph_runner/eagle_draft_npu_graph_runner.py
@@ -24,6 +24,7 @@ import torch
 
 from sglang.srt.configs.model_config import AttentionArch, is_deepseek_nsa
 from sglang.srt.layers.dp_attention import get_attention_tp_size
+from sglang.srt.environ import envs
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.speculative.eagle_draft_cuda_graph_runner import (
     EAGLEDraftCudaGraphRunner,
@@ -78,15 +79,15 @@ class EAGLEDraftNpuGraphRunner(EAGLEDraftCudaGraphRunner):
             out = run_once_fn()
         return out
 
-    def _get_update_attr_name(self, model_runner):
-        if self.bs < get_attention_tp_size():
-            return self.attr_name[AttentionArch.MLA]
-        return self.attr_name[model_runner.model_config.attention_arch]
+    def _get_update_attr_name(self):
+        if envs.SGLANG_USE_PAGED_ATTENTION.get():
+            return self.attr_name[AttentionArch.MHA]
+        return self.attr_name[AttentionArch.MLA]
 
-    def _get_update_attr_type(self, model_runner):
-        if self.bs < get_attention_tp_size():
-            return self.attr_type[AttentionArch.MLA]
-        return self.attr_type[model_runner.model_config.attention_arch]
+    def _get_update_attr_type(self):
+        if envs.SGLANG_USE_PAGED_ATTENTION.get():
+            return self.attr_type[AttentionArch.MHA]
+        return self.attr_type[AttentionArch.MLA]
 
     def _replay_update(self, seq_lens):
         if isinstance(self.update_attr_type, torch.Tensor):
@@ -97,8 +98,8 @@ class EAGLEDraftNpuGraphRunner(EAGLEDraftCudaGraphRunner):
         )
 
     def _replay(self, forward_batch: ForwardBatch):
-        self.update_attr_name = self._get_update_attr_name(self.model_runner)
-        self.update_attr_type = self._get_update_attr_type(self.model_runner)
+        self.update_attr_name = self._get_update_attr_name()
+        self.update_attr_type = self._get_update_attr_type()
         if not is_deepseek_nsa(self.model_runner.model_config.hf_config):
             seq_lens = forward_batch.seq_lens_cpu.tolist() + [0] * (
                 self.bs - self.raw_bs
diff --git a/python/sglang/srt/hardware_backend/npu/graph_runner/npu_graph_runner.py b/python/sglang/srt/hardware_backend/npu/graph_runner/npu_graph_runner.py
index d089fcc63..39d6f72b0 100644
--- a/python/sglang/srt/hardware_backend/npu/graph_runner/npu_graph_runner.py
+++ b/python/sglang/srt/hardware_backend/npu/graph_runner/npu_graph_runner.py
@@ -29,6 +29,7 @@ import sglang
 from sglang.srt.configs.model_config import AttentionArch, is_deepseek_nsa
 from sglang.srt.distributed.parallel_state import GroupCoordinator
 from sglang.srt.layers.dp_attention import get_attention_tp_size
+from sglang.srt.environ import envs
 from sglang.srt.model_executor.cuda_graph_runner import CudaGraphRunner
 from sglang.srt.utils import (
     empty_context,
@@ -111,23 +112,15 @@ class NPUGraphRunner(CudaGraphRunner):
             out = run_once_fn()
         return out
 
-    def _get_update_attr_name(self, model_runner, forward_batch):
-        if (
-            self.bs < get_attention_tp_size()
-            or forward_batch.forward_mode.is_target_verify()
-            or self.use_fia
-        ):
-            return self.attr_name[AttentionArch.MLA]
-        return self.attr_name[model_runner.model_config.attention_arch]
-
-    def _get_update_attr_type(self, model_runner, forward_batch):
-        if (
-            self.bs < get_attention_tp_size()
-            or forward_batch.forward_mode.is_target_verify()
-            or self.use_fia
-        ):
-            return self.attr_type[AttentionArch.MLA]
-        return self.attr_type[model_runner.model_config.attention_arch]
+    def _get_update_attr_name(self):
+        if envs.SGLANG_USE_PAGED_ATTENTION.get():
+            return self.attr_name[AttentionArch.MHA]
+        return self.attr_name[AttentionArch.MLA]
+
+    def _get_update_attr_type(self):
+        if envs.SGLANG_USE_PAGED_ATTENTION.get():
+            return self.attr_type[AttentionArch.MHA]
+        return self.attr_type[AttentionArch.MLA]
 
     def _update_inputs(self, seq_lens):
         if isinstance(self.update_attr_type, torch.Tensor):
@@ -144,6 +137,10 @@ class NPUGraphRunner(CudaGraphRunner):
         output_dir = os.path.join(
             os.getenv("SGLANG_TORCH_PROFILER_DIR", "/tmp"), "graph_capture_profile"
         )
+        if ".." in output_dir:
+            raise ValueError(
+                "Output directory path contains '..', which is not allowed."
+            )
         if not Path(output_dir).exists():
             Path(output_dir).mkdir(parents=True, exist_ok=True)
         logger.info(
@@ -181,12 +178,8 @@ class NPUGraphRunner(CudaGraphRunner):
             self.buffers.input_ids[: self.raw_num_token].copy_(forward_batch.input_ids)
             self.buffers.positions[: self.raw_num_token].copy_(forward_batch.positions)
 
-        self.update_attr_name = self._get_update_attr_name(
-            self.model_runner, forward_batch
-        )
-        self.update_attr_type = self._get_update_attr_type(
-            self.model_runner, forward_batch
-        )
+        self.update_attr_name = self._get_update_attr_name()
+        self.update_attr_type = self._get_update_attr_type()
         # Replay
         if not is_deepseek_nsa(self.model_runner.model_config.hf_config):
             if forward_batch.forward_mode.is_target_verify():
diff --git a/python/sglang/srt/hardware_backend/npu/modules/deepseek_v2_attention_mla_npu.py b/python/sglang/srt/hardware_backend/npu/modules/deepseek_v2_attention_mla_npu.py
index 6626c468f..3bf7571a9 100644
--- a/python/sglang/srt/hardware_backend/npu/modules/deepseek_v2_attention_mla_npu.py
+++ b/python/sglang/srt/hardware_backend/npu/modules/deepseek_v2_attention_mla_npu.py
@@ -64,36 +64,45 @@ def forward_mha_prepare_npu(
     kv_a, _ = latent_cache.split([m.kv_lora_rank, m.qk_rope_head_dim], dim=-1)
     latent_cache = latent_cache.unsqueeze(1)
 
-    B, S = q.shape[0], 1
-    cos, sin = m.rotary_emb.get_cos_sin_cache(
-        positions, hidden_states.dtype, offsets=None
-    )
-    q_pe = torch_npu.npu_interleave_rope(
-        q_pe.reshape(B, -1, S, m.qk_rope_head_dim),
-        cos,
-        sin,
-    )
-    q_pe = q_pe.reshape(B, -1, m.qk_rope_head_dim)
-
-    ckv_cache, k_rope_cache = forward_batch.token_to_kv_pool.get_kv_buffer(m.layer_id)
-    _, _, k_pe, kv_a = torch_npu.npu_kv_rmsnorm_rope_cache(
-        latent_cache.view(-1, 1, 1, m.kv_lora_rank + m.qk_rope_head_dim),  # bnsd
-        m.kv_a_layernorm.weight,
-        cos,
-        sin,
-        forward_batch.out_cache_loc.to(torch.int64),
-        k_rope_cache,
-        ckv_cache,
-        k_rope_scale=None,
-        c_kv_scale=None,
-        k_rope_offset=None,
-        c_kv_offset=None,
-        epsilon=m.kv_a_layernorm.variance_epsilon,
-        cache_mode="PA_NZ" if is_fia_nz() else "PA_BNSD",
-        is_output_kv=True,
-    )  # adapter NZ
-
-    k_pe = k_pe.reshape(B, -1, m.qk_rope_head_dim)
+    if m.use_deepseek_yarn_rope:
+        B, S = q.shape[0], 1
+        cos, sin = m.rotary_emb.get_cos_sin_cache(
+            positions, hidden_states.dtype, offsets=None
+        )
+        q_pe = torch_npu.npu_interleave_rope(
+            q_pe.reshape(B, -1, S, m.qk_rope_head_dim),
+            cos,
+            sin,
+        )
+        q_pe = q_pe.reshape(B, -1, m.qk_rope_head_dim)
+
+        ckv_cache, k_rope_cache = forward_batch.token_to_kv_pool.get_kv_buffer(m.layer_id)
+        _, _, k_pe, kv_a = torch_npu.npu_kv_rmsnorm_rope_cache(
+            latent_cache.view(-1, 1, 1, m.kv_lora_rank + m.qk_rope_head_dim),  # bnsd
+            m.kv_a_layernorm.weight,
+            cos,
+            sin,
+            forward_batch.out_cache_loc.to(torch.int64),
+            k_rope_cache,
+            ckv_cache,
+            k_rope_scale=None,
+            c_kv_scale=None,
+            k_rope_offset=None,
+            c_kv_offset=None,
+            epsilon=m.kv_a_layernorm.variance_epsilon,
+            cache_mode="PA_NZ" if is_fia_nz() else "PA_BNSD",
+            is_output_kv=True,
+        )  # adapter NZ
+
+        k_pe = k_pe.reshape(B, -1, m.qk_rope_head_dim)
+    else:
+        kv_a = m.kv_a_layernorm(kv_a)
+        k_pe = latent_cache[:, :, m.kv_lora_rank:]
+        if m.rotary_emb is not None:
+            q_pe, k_pe = m.rotary_emb(positions, q_pe, k_pe)
+        forward_batch.token_to_kv_pool.set_kv_buffer(
+            m, forward_batch.out_cache_loc, kv_a.unsqueeze(1), k_pe
+        )
 
     q[..., m.qk_nope_head_dim :] = q_pe
 
@@ -329,7 +338,7 @@ def forward_dsa_prepare_npu(
         k_nope, k_pe = latent_cache.unsqueeze(1).split(
             [m.kv_lora_rank, m.qk_rope_head_dim], dim=-1
         )
-        k_nope = m.kv_a_layernorm(k_nope).unsqueeze(1)
+        k_nope = m.kv_a_layernorm(k_nope)
         torch.npu.current_stream().wait_event(q_event)
 
         q_nope, q_pe = q.split([m.qk_nope_head_dim, m.qk_rope_head_dim], dim=-1)
diff --git a/python/sglang/srt/hardware_backend/npu/moe/topk.py b/python/sglang/srt/hardware_backend/npu/moe/topk.py
index 813c12f6a..51ffe82ca 100644
--- a/python/sglang/srt/hardware_backend/npu/moe/topk.py
+++ b/python/sglang/srt/hardware_backend/npu/moe/topk.py
@@ -38,7 +38,7 @@ def fused_topk_npu(
             )
         topk_weights = topk_weights.to(torch.float32)
 
-    elif use_grouped_topk and correction_bias is not None:
+    elif use_grouped_topk and correction_bias is not None and router_logits.shape[-1] <= 2048:
         # Force set routed_scaling_factor = 1 to optimize renormalize
         topk_weights, topk_ids, _ = torch.ops.npu.npu_moe_gating_top_k(
             router_logits.to(torch.float32),
diff --git a/python/sglang/srt/layers/activation.py b/python/sglang/srt/layers/activation.py
index 6db49012d..1f9888404 100644
--- a/python/sglang/srt/layers/activation.py
+++ b/python/sglang/srt/layers/activation.py
@@ -149,6 +149,10 @@ class NewGELU(CustomOp):
         # TODO: Implement the CUDA kernel for NewGELU in sgl-kernel
         return self.forward_native(x)
 
+    def forward_npu(self, x: torch.Tensor) -> torch.Tensor:
+        # TODO: Implement the NPU kernel for NewGELU in sgl-kernel
+        return self.forward_native(x)
+
 
 class ReLU2(nn.Module):
     """
diff --git a/python/sglang/srt/layers/attention/torch_native_backend.py b/python/sglang/srt/layers/attention/torch_native_backend.py
index 6a67ea947..254049ec6 100644
--- a/python/sglang/srt/layers/attention/torch_native_backend.py
+++ b/python/sglang/srt/layers/attention/torch_native_backend.py
@@ -1,5 +1,6 @@
 from __future__ import annotations
 
+import math
 from typing import TYPE_CHECKING
 
 import torch
@@ -24,6 +25,51 @@ class TorchNativeAttnBackend(AttentionBackend):
         """Init the metadata for a forward pass."""
         pass
 
+    def scaled_dot_product_attention_with_softcapping(
+        self,
+        query,
+        key,
+        value,
+        attn_mask=None,
+        dropout_p=0.0,
+        is_causal=False,
+        scale=None,
+        enable_gqa=False,
+        logit_cap=0.0,
+        logit_capping_method="tanh",
+    ) -> torch.Tensor:
+        L, S = query.size(-2), key.size(-2)
+        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale
+        attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)
+        if is_causal:
+            assert attn_mask is None
+            temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(
+                diagonal=0
+            )
+            attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
+            attn_bias.to(query.dtype)
+
+        if attn_mask is not None:
+            if attn_mask.dtype == torch.bool:
+                attn_bias.masked_fill_(attn_mask.logical_not(), float("-inf"))
+            else:
+                attn_bias = attn_mask + attn_bias
+
+        if enable_gqa:
+            key = key.repeat_interleave(query.size(-3) // key.size(-3), -3)
+            value = value.repeat_interleave(query.size(-3) // value.size(-3), -3)
+
+        attn_weight = query @ key.transpose(-2, -1) * scale_factor
+
+        if logit_cap > 0:
+            if logit_capping_method == "tanh":
+                attn_weight = logit_cap * torch.tanh(attn_weight / logit_cap)
+
+        attn_weight += attn_bias
+        attn_weight = torch.softmax(attn_weight, dim=-1)
+        attn_weight = torch.dropout(attn_weight, dropout_p, train=False)
+        return attn_weight @ value
+
     def _run_sdpa_forward_extend(
         self,
         query: torch.Tensor,
@@ -38,6 +84,8 @@ class TorchNativeAttnBackend(AttentionBackend):
         scaling=None,
         enable_gqa=False,
         causal=False,
+        logit_cap: float = 0.0,
+        logit_capping_method: str = "tanh",
     ):
         """Run the extend forward by using torch native sdpa op.
 
@@ -93,18 +141,34 @@ class TorchNativeAttnBackend(AttentionBackend):
             per_req_key = k_cache[per_req_tokens].movedim(0, query.dim() - 2)
             per_req_value = v_cache[per_req_tokens].movedim(0, query.dim() - 2)
 
-            per_req_out_redudant = (
-                scaled_dot_product_attention(
-                    per_req_query_redudant.unsqueeze(0),
-                    per_req_key.unsqueeze(0),
-                    per_req_value.unsqueeze(0),
-                    enable_gqa=enable_gqa,
-                    scale=scaling,
-                    is_causal=causal,
+            if logit_cap > 0:
+                per_req_out_redudant = (
+                    self.scaled_dot_product_attention_with_softcapping(
+                        per_req_query_redudant.unsqueeze(0),
+                        per_req_key.unsqueeze(0),
+                        per_req_value.unsqueeze(0),
+                        enable_gqa=enable_gqa,
+                        scale=scaling,
+                        is_causal=causal,
+                        logit_cap=logit_cap,
+                        logit_capping_method=logit_capping_method,
+                    )
+                    .squeeze(0)
+                    .movedim(query.dim() - 2, 0)
+                )
+            else:
+                per_req_out_redudant = (
+                    scaled_dot_product_attention(
+                        per_req_query_redudant.unsqueeze(0),
+                        per_req_key.unsqueeze(0),
+                        per_req_value.unsqueeze(0),
+                        enable_gqa=enable_gqa,
+                        scale=scaling,
+                        is_causal=causal,
+                    )
+                    .squeeze(0)
+                    .movedim(query.dim() - 2, 0)
                 )
-                .squeeze(0)
-                .movedim(query.dim() - 2, 0)
-            )
             output[start_q:end_q, :, :] = per_req_out_redudant[prefill_seq_len_q:, :, :]
             start_q, start_kv = end_q, end_kv
         return output
@@ -121,6 +185,8 @@ class TorchNativeAttnBackend(AttentionBackend):
         scaling=None,
         enable_gqa=False,
         causal=False,
+        logit_cap: float = 0.0,
+        logit_capping_method: str = "tanh",
     ):
         """Run the decode forward by using torch native sdpa op.
 
@@ -162,18 +228,34 @@ class TorchNativeAttnBackend(AttentionBackend):
             per_req_key = k_cache[per_req_tokens].movedim(0, query.dim() - 2)
             per_req_value = v_cache[per_req_tokens].movedim(0, query.dim() - 2)
 
-            per_req_out = (
-                scaled_dot_product_attention(
-                    per_req_query.unsqueeze(0),
-                    per_req_key.unsqueeze(0),
-                    per_req_value.unsqueeze(0),
-                    enable_gqa=enable_gqa,
-                    scale=scaling,
-                    is_causal=causal,
+            if logit_cap > 0:
+                per_req_out = (
+                    self.scaled_dot_product_attention_with_softcapping(
+                        per_req_query.unsqueeze(0),
+                        per_req_key.unsqueeze(0),
+                        per_req_value.unsqueeze(0),
+                        enable_gqa=enable_gqa,
+                        scale=scaling,
+                        is_causal=causal,
+                        logit_cap=logit_cap,
+                        logit_capping_method=logit_capping_method,
+                    )
+                    .squeeze(0)
+                    .movedim(query.dim() - 2, 0)
+                )
+            else:
+                per_req_out = (
+                    scaled_dot_product_attention(
+                        per_req_query.unsqueeze(0),
+                        per_req_key.unsqueeze(0),
+                        per_req_value.unsqueeze(0),
+                        enable_gqa=enable_gqa,
+                        scale=scaling,
+                        is_causal=causal,
+                    )
+                    .squeeze(0)
+                    .movedim(query.dim() - 2, 0)
                 )
-                .squeeze(0)
-                .movedim(query.dim() - 2, 0)
-            )
             output[start_q:end_q, :, :] = per_req_out
             start_q, start_kv = end_q, end_kv
 
diff --git a/python/sglang/srt/layers/attention/vision.py b/python/sglang/srt/layers/attention/vision.py
index f66c24844..de13cdda7 100644
--- a/python/sglang/srt/layers/attention/vision.py
+++ b/python/sglang/srt/layers/attention/vision.py
@@ -463,7 +463,7 @@ class VisionAscendAttention(nn.Module):
         Returns:
              [b * s, h, head_size]
         """
-        cu_seqlens = resolve_seqlens(cu_seqlens, bsz, seq_len, device=q.device)
+        cu_seqlens = resolve_seqlens(cu_seqlens, bsz, seq_len, device="cpu")
 
         seq_lens = cu_seqlens[1:] - cu_seqlens[:-1]
         if seq_lens.is_npu:
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
index 7d9900bbe..3411f6c0a 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
@@ -265,6 +265,7 @@ class FusedMoE(torch.nn.Module):
                 else self.weight_loader_fused
             ),
             with_bias=with_bias,
+            intermediate_size_full=intermediate_size,
         )
 
         self.quant_method.create_moe_runner(self, self.moe_runner_config)
diff --git a/python/sglang/srt/layers/rotary_embedding.py b/python/sglang/srt/layers/rotary_embedding.py
index e9f375138..00a689c43 100644
--- a/python/sglang/srt/layers/rotary_embedding.py
+++ b/python/sglang/srt/layers/rotary_embedding.py
@@ -115,9 +115,10 @@ class RotaryEmbedding(CustomOp):
             cache = cache.to(dtype)
 
         if (
-            (not (_is_cuda or _is_npu) or self.head_size not in [64, 128, 256, 512])
+            (not _is_cuda or self.head_size not in [64, 128, 256, 512])
             and not (_is_cpu and _is_cpu_amx_available)
             and not (_is_xpu)
+            and not (_is_npu)
         ):
             if _is_cuda or _is_hip:
                 from sgl_kernel import rotary_embedding
@@ -208,16 +209,37 @@ class RotaryEmbedding(CustomOp):
         )
 
     def get_cos_sin_with_position(self, positions):
-        cos_sin = self.cos_sin_cache.index_select(0, positions.flatten())
-        last_dim = cos_sin.size()[-1]
-        cos, sin = (
-            cos_sin.reshape(-1, 2, last_dim // 2).repeat(1, 1, 2).chunk(2, dim=-2)
-        )
-        # BSNH
-        self.position_cos, self.position_sin = (
-            cos.view(-1, 1, 1, last_dim).contiguous(),
-            sin.view(-1, 1, 1, last_dim).contiguous(),
-        )
+        assert positions.ndim == 1 or positions.ndim == 2
+        if positions.ndim == 1:
+            cos_sin = self.cos_sin_cache.index_select(0, positions.flatten())
+            last_dim = cos_sin.size()[-1]
+            cos, sin = (
+                cos_sin.reshape(-1, 2, last_dim // 2).repeat(1, 1, 2).chunk(2, dim=-2)
+            )
+            # BSNH
+            self.position_cos, self.position_sin = (
+                cos.view(-1, 1, 1, last_dim).contiguous(),
+                sin.view(-1, 1, 1, last_dim).contiguous(),
+            )
+        else:
+            assert self.mrope_section
+            cos_sin = self.cos_sin_cache[positions]
+            last_dim = cos_sin.size()[-1]
+            cos, sin = cos_sin.chunk(2, dim=-1)
+            if self.mrope_interleaved:
+                cos = apply_interleaved_rope(cos, self.mrope_section)
+                sin = apply_interleaved_rope(sin, self.mrope_section)
+            else:
+                cos = torch.cat(
+                    [m[i] for i, m in enumerate(cos.split(self.mrope_section, dim=-1))],
+                    dim=-1,
+                )
+                sin = torch.cat(
+                    [m[i] for i, m in enumerate(sin.split(self.mrope_section, dim=-1))],
+                    dim=-1,
+                )
+            self.position_cos = cos.repeat(1, 2).view(-1, 1, 1, last_dim).contiguous()
+            self.position_sin = sin.repeat(1, 2).view(-1, 1, 1, last_dim).contiguous()
 
     def get_cos_sin(self, seqlen: int) -> tuple[torch.Tensor, torch.Tensor]:
         cos_sin = self.cos_sin_cache[:seqlen]
@@ -274,6 +296,9 @@ class RotaryEmbedding(CustomOp):
             fused_set_kv_buffer_arg is None
         ), "fused_set_kv_buffer_arg is not supported for npu implementation"
 
+        if (query.dtype == torch.bfloat16 and self.cos_sin_cache.dtype == torch.float) or get_bool_env_var("SGLANG_ENABLE_TORCH_COMPILE"):
+            return self.forward_native(positions, query, key, offsets)
+
         rotary_mode = "half"
         if self.is_neox_style:
             rotary_mode = "half"
@@ -734,8 +759,8 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
         key: torch.Tensor,
         offsets: Optional[torch.Tensor] = None,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        query = query.view(*query.shape[:-1], -1, self.head_size)
-        key = key.view(*key.shape[:-1], -1, self.head_size)
+        query = query.unflatten(1, (-1,  self.head_size))
+        key = key.unflatten(1, (-1,  self.head_size))
 
         k = self.original_max_position_embeddings
         long_prompt_offset = (
diff --git a/python/sglang/srt/layers/vocab_parallel_embedding.py b/python/sglang/srt/layers/vocab_parallel_embedding.py
index f171a15f2..192970904 100644
--- a/python/sglang/srt/layers/vocab_parallel_embedding.py
+++ b/python/sglang/srt/layers/vocab_parallel_embedding.py
@@ -31,6 +31,7 @@ from sglang.srt.utils import (
     cpu_has_amx_support,
     get_compiler_backend,
     is_cpu,
+    is_npu,
     set_weight_attrs,
 )
 
@@ -38,6 +39,7 @@ DEFAULT_VOCAB_PADDING_SIZE = 64
 
 _is_cpu_amx_available = cpu_has_amx_support()
 _is_cpu = is_cpu()
+_is_npu = is_npu()
 
 logger = logging.getLogger(__name__)
 
@@ -123,7 +125,7 @@ class VocabParallelEmbeddingShardIndices:
         assert self.num_added_elements <= self.num_added_elements_padded
 
 
-@torch.compile(dynamic=True, backend=get_compiler_backend())
+@torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)
 def get_masked_input_and_mask(
     input_: torch.Tensor,
     org_vocab_start_index: int,
diff --git a/python/sglang/srt/lora/backend/chunked_backend.py b/python/sglang/srt/lora/backend/chunked_backend.py
index f17f473cb..54b28a771 100644
--- a/python/sglang/srt/lora/backend/chunked_backend.py
+++ b/python/sglang/srt/lora/backend/chunked_backend.py
@@ -5,7 +5,7 @@ from sglang.srt.lora.triton_ops import (
     chunked_sgmv_lora_expand_forward,
     chunked_sgmv_lora_shrink_forward,
 )
-from sglang.srt.lora.utils import LoRABatchInfo
+from sglang.srt.lora.utils import LoRABatchInfo, generate_sequence_lengths
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.server_args import ServerArgs
 
@@ -283,24 +283,7 @@ class ChunkedSgmvLoRABackend(BaseLoRABackend):
         """
         with torch.device("cpu"):
             seq_weight_indices = torch.tensor(seq_weight_indices, dtype=torch.int32)
-
-            if forward_batch.forward_mode.is_decode():
-                seg_lens_cpu = torch.ones(forward_batch.batch_size, dtype=torch.int32)
-            elif forward_batch.forward_mode.is_target_verify():
-                seg_lens_cpu = torch.full(
-                    size=(forward_batch.batch_size,),
-                    fill_value=forward_batch.spec_info.draft_token_num,
-                    dtype=torch.int32,
-                )
-            elif forward_batch.forward_mode.is_extend():
-                seg_lens_cpu = torch.tensor(
-                    forward_batch.extend_seq_lens_cpu,
-                    dtype=torch.int32,
-                )
-            else:
-                raise ValueError(
-                    f"Unsupported forward mode: {forward_batch.forward_mode}"
-                )
+            seg_lens_cpu = generate_sequence_lengths(forward_batch)
 
             row_weight_indices = torch.repeat_interleave(
                 seq_weight_indices, seg_lens_cpu
diff --git a/python/sglang/srt/lora/backend/torch_backend.py b/python/sglang/srt/lora/backend/torch_backend.py
index af467bc81..d31471226 100644
--- a/python/sglang/srt/lora/backend/torch_backend.py
+++ b/python/sglang/srt/lora/backend/torch_backend.py
@@ -1,11 +1,29 @@
+from dataclasses import dataclass
+from typing import Optional
+
 import torch
 
 from sglang.srt.lora.backend.base_backend import BaseLoRABackend
-from sglang.srt.lora.torch_ops import sgmv_expand, sgmv_expand_slice, sgmv_shrink
-from sglang.srt.lora.utils import LoRABatchInfo
+from sglang.srt.lora.torch_ops import sgemm_lora_a_fwd, sgemm_lora_b_fwd
+from sglang.srt.lora.utils import LoRABatchInfo, generate_sequence_lengths
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 
 
+@dataclass
+class TorchNativeLoRABatchInfo(LoRABatchInfo):
+    # ranks of each lora adapter, in shape (lora_num,) placed on cpu device
+    lora_ranks_cpu: Optional[torch.Tensor] = None
+
+    # Indice pointers of each segment in shape (num_segments + 1, ) placed on cpu device
+    seg_indptr_cpu: Optional[torch.Tensor] = None
+
+    # Lengths of each segments in shape (num_segments,) placed on cpu device
+    seg_lens_cpu: Optional[torch.Tensor] = None
+
+    # The index of lora adapter used by each segment, in shape (num_segments,) placed on cpu device
+    weight_indices_cpu: Optional[torch.Tensor] = None
+
+
 class TorchNativeLoRABackend(BaseLoRABackend):
     name = "torch_native"
 
@@ -20,27 +38,15 @@ class TorchNativeLoRABackend(BaseLoRABackend):
     def run_lora_a_sgemm(
         self, x: torch.Tensor, weights: torch.Tensor, *args, **kwargs
     ) -> torch.Tensor:
-
-        total_seq_len, _ = x.shape
-        _, weight_out_dim, _ = weights.shape
-
-        output_tensor = torch.zeros(
-            (total_seq_len, weight_out_dim), dtype=x.dtype, device=x.device
-        )
-        sgmv_shrink(
-            x,
-            weights,
-            output_tensor,
-            self.batch_info.seg_lens,
-            self.batch_info.weight_indices,
-            1.0,
+        output_tensor = sgemm_lora_a_fwd(
+            inputs=x,
+            weights=weights,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            scaling_tensor=self.batch_info.scalings,
+            num_slices=1,
         )
-        scaling = torch.repeat_interleave(
-            self.batch_info.scalings[self.batch_info.weight_indices],
-            self.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        output_tensor = output_tensor * scaling
 
         return output_tensor
 
@@ -52,23 +58,18 @@ class TorchNativeLoRABackend(BaseLoRABackend):
         *args,
         **kwargs,
     ) -> torch.Tensor:
-        total_seq_len, _ = x.shape
         _, weight_out_dim, _ = weights.shape
-
-        if base_output is None:
-            output_tensor = torch.zeros(
-                (total_seq_len, weight_out_dim), device=x.device, dtype=x.dtype
-            )
-        else:
-            output_tensor = base_output
-
-        sgmv_expand(
-            x,
-            weights,
-            output_tensor,
-            self.batch_info.seg_lens,
-            self.batch_info.weight_indices,
-            True,
+        output_offset = torch.tensor(
+            [0, weight_out_dim], dtype=torch.int32, device="cpu"
+        )
+        output_tensor = sgemm_lora_b_fwd(
+            inputs=x,
+            weights=weights,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            slice_offsets=output_offset,
+            base_output=base_output,
         )
 
         return output_tensor
@@ -85,53 +86,26 @@ class TorchNativeLoRABackend(BaseLoRABackend):
         *args,
         **kwargs,
     ) -> torch.Tensor:
-        num_slices = 3
-        assert isinstance(qkv_lora_b, torch.Tensor)
-
-        total_seq_len, _ = x.shape
-        _, weight_intermediate_dim, _ = qkv_lora_a.shape
-        _, weight_out_dim, _ = qkv_lora_b.shape
-        max_rank = weight_intermediate_dim // num_slices
-
-        if base_output is None:
-            output_tensor = torch.zeros(
-                (total_seq_len, weight_out_dim), device=x.device, dtype=x.dtype
-            )
-        else:
-            output_tensor = base_output
-
-        lora_a_output = torch.zeros(
-            total_seq_len, weight_intermediate_dim, dtype=x.dtype, device=x.device
+        num_slices = len(output_offset_cpu) - 1
+        lora_a_output = sgemm_lora_a_fwd(
+            inputs=x,
+            weights=qkv_lora_a,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            scaling_tensor=self.batch_info.scalings,
+            num_slices=num_slices,
         )
-        sgmv_shrink(
-            x,
-            qkv_lora_a,
-            lora_a_output,
-            self.batch_info.seg_lens,
-            self.batch_info.weight_indices,
-            1.0,
+
+        output_tensor = sgemm_lora_b_fwd(
+            inputs=lora_a_output,
+            weights=qkv_lora_b,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            slice_offsets=output_offset_cpu,
+            base_output=base_output,
         )
-        scaling = torch.repeat_interleave(
-            self.batch_info.scalings[self.batch_info.weight_indices],
-            self.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        lora_a_output = lora_a_output * scaling
-
-        for slice_id in range(num_slices):
-            slice_offset = output_offset_cpu[slice_id]
-            slice_offset_next = output_offset_cpu[slice_id + 1]
-            slice_size = slice_offset_next - slice_offset
-            sgmv_expand_slice(
-                lora_a_output[:, (max_rank * slice_id) : (max_rank * (slice_id + 1))],
-                qkv_lora_b[:, slice_offset:slice_offset_next],
-                output_tensor,
-                self.batch_info.seg_lens,
-                self.batch_info.weight_indices,
-                slice_offset,
-                slice_size,
-                True,
-            )
 
         return output_tensor
 
@@ -144,54 +118,32 @@ class TorchNativeLoRABackend(BaseLoRABackend):
         *args,
         **kwargs,
     ) -> torch.Tensor:
-
         num_slices = 2
-        assert isinstance(gate_up_lora_b, torch.Tensor)
-
-        total_seq_len, _ = x.shape
-        _, weight_intermediate_dim, _ = gate_up_lora_a.shape
         _, weight_out_dim, _ = gate_up_lora_b.shape
         slice_size = weight_out_dim // num_slices
-        max_rank = weight_intermediate_dim // num_slices
-
-        if base_output is None:
-            output_tensor = torch.zeros(
-                (total_seq_len, weight_out_dim), device=x.device, dtype=x.dtype
-            )
-        else:
-            output_tensor = base_output
+        output_offset = torch.tensor(
+            [0, slice_size, weight_out_dim], dtype=torch.int32, device="cpu"
+        )
 
-        lora_a_output = torch.zeros(
-            total_seq_len, weight_intermediate_dim, dtype=x.dtype, device=x.device
+        lora_a_output = sgemm_lora_a_fwd(
+            inputs=x,
+            weights=gate_up_lora_a,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            scaling_tensor=self.batch_info.scalings,
+            num_slices=num_slices,
         )
-        sgmv_shrink(
-            x,
-            gate_up_lora_a,
-            lora_a_output,
-            self.batch_info.seg_lens,
-            self.batch_info.weight_indices,
-            1.0,
+
+        output_tensor = sgemm_lora_b_fwd(
+            inputs=lora_a_output,
+            weights=gate_up_lora_b,
+            weight_indices=self.batch_info.weight_indices_cpu,
+            seg_len_tensor=self.batch_info.seg_lens_cpu,
+            lora_ranks=self.batch_info.lora_ranks_cpu,
+            slice_offsets=output_offset,
+            base_output=base_output,
         )
-        scaling = torch.repeat_interleave(
-            self.batch_info.scalings[self.batch_info.weight_indices],
-            self.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        lora_a_output = lora_a_output * scaling
-
-        slice_offset = 0
-        for slice_id in range(num_slices):
-            sgmv_expand_slice(
-                lora_a_output[:, (max_rank * slice_id) : (max_rank * (slice_id + 1))],
-                gate_up_lora_b[:, slice_offset : slice_offset + slice_size],
-                output_tensor,
-                self.batch_info.seg_lens,
-                self.batch_info.weight_indices,
-                slice_offset,
-                slice_size,
-                True,
-            )
-            slice_offset += slice_size
 
         return output_tensor
 
@@ -201,19 +153,19 @@ class TorchNativeLoRABackend(BaseLoRABackend):
         num_tokens_per_bs: int,
     ):
         with torch.device("cuda"):
-            self.cuda_graph_batch_info = LoRABatchInfo(
-                bs=max_bs_in_cuda_graph,
+            self.cuda_graph_batch_info = TorchNativeLoRABatchInfo(
                 use_cuda_graph=True,
-                num_segments=None,
+                bs=max_bs_in_cuda_graph,
+                num_segments=self.max_loras_per_batch,
                 seg_lens=torch.full(
                     (max_bs_in_cuda_graph,), num_tokens_per_bs, dtype=torch.int32
                 ),
-                seg_indptr=torch.empty(max_bs_in_cuda_graph + 1, dtype=torch.int32),
-                max_len=num_tokens_per_bs,
+                seg_indptr=torch.zeros(max_bs_in_cuda_graph + 1, dtype=torch.int32),
                 weight_indices=torch.zeros(max_bs_in_cuda_graph, dtype=torch.int32),
                 lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),
                 scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),
                 permutation=None,
+                max_len=num_tokens_per_bs,
             )
 
             # Initialize seg_indptr for CUDA graph as they remain constant
@@ -232,10 +184,36 @@ class TorchNativeLoRABackend(BaseLoRABackend):
         scalings: list[float],
         use_cuda_graph: bool,
     ):
-        # Use pinned memory to avoid synchronizations during host-to-device transfer
-        weight_indices_tensor = torch.tensor(
-            weight_indices, dtype=torch.int32, pin_memory=True, device="cpu"
+        original_seq_lens_cpu = generate_sequence_lengths(forward_batch, device="cpu")
+        original_weight_indices_tensor = torch.tensor(
+            weight_indices, dtype=torch.int32, device="cpu"
+        )
+
+        unique_weight_indices_tensor, inverse_weight_indices_tensor = (
+            torch.unique_consecutive(
+                original_weight_indices_tensor, return_inverse=True
+            )
         )
+
+        seg_lens_cpu = (
+            torch.zeros_like(
+                unique_weight_indices_tensor, dtype=torch.int32, device="cpu"
+            )
+            .scatter_add_(
+                0,
+                inverse_weight_indices_tensor,
+                original_seq_lens_cpu,
+            )
+            .pin_memory()
+        )
+
+        seg_indptr_cpu = torch.zeros(
+            (len(seg_lens_cpu) + 1,), dtype=torch.int32, pin_memory=True
+        )
+        seg_indptr_cpu[1:] = torch.cumsum(seg_lens_cpu, dim=0)
+
+        # Use pinned memory to avoid synchronizations during host-to-device transfer
+        weight_indices_tensor = unique_weight_indices_tensor.pin_memory()
         lora_ranks_tensor = torch.tensor(
             lora_ranks, dtype=torch.int32, pin_memory=True, device="cpu"
         )
@@ -253,27 +231,17 @@ class TorchNativeLoRABackend(BaseLoRABackend):
             batch_info.bs = forward_batch.batch_size
             batch_info.num_segments = forward_batch.batch_size
         else:
-            max_len = (
-                # Calculate max_len from the CPU copy to avoid D2H transfer.
-                max(forward_batch.extend_seq_lens_cpu)
-                if forward_batch.forward_mode.is_extend()
-                else 1
-            )
-            seg_lens = (
-                forward_batch.extend_seq_lens
-                if forward_batch.forward_mode.is_extend()
-                else torch.ones(bs, dtype=torch.int32, device=self.device)
-            )
-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)
-            seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)
+            max_len = max(seg_lens_cpu)
 
-            batch_info = LoRABatchInfo(
+            batch_info = TorchNativeLoRABatchInfo(
                 bs=forward_batch.batch_size,
                 num_segments=forward_batch.batch_size,
                 max_len=max_len,
                 use_cuda_graph=False,
-                seg_lens=seg_lens,
-                seg_indptr=seg_indptr,
+                seg_lens=torch.empty((bs,), dtype=torch.int32, device=self.device),
+                seg_indptr=torch.empty(
+                    (bs + 1,), dtype=torch.int32, device=self.device
+                ),
                 weight_indices=torch.empty(
                     (bs,), dtype=torch.int32, device=self.device
                 ),
@@ -294,4 +262,14 @@ class TorchNativeLoRABackend(BaseLoRABackend):
             scalings_tensor, non_blocking=True
         )
         batch_info.weight_indices[:bs].copy_(weight_indices_tensor, non_blocking=True)
+        batch_info.seg_indptr[: len(seg_indptr_cpu)].copy_(
+            seg_indptr_cpu, non_blocking=True
+        )
+        batch_info.seg_lens[: len(seg_lens_cpu)].copy_(seg_lens_cpu, non_blocking=True)
+
+        batch_info.lora_ranks_cpu = lora_ranks_tensor
+        batch_info.seg_indptr_cpu = seg_indptr_cpu
+        batch_info.seg_lens_cpu = seg_lens_cpu
+        batch_info.weight_indices_cpu = weight_indices_tensor
+
         self.batch_info = batch_info
diff --git a/python/sglang/srt/lora/torch_ops/__init__.py b/python/sglang/srt/lora/torch_ops/__init__.py
index 8cc3826f6..bc3a5391d 100644
--- a/python/sglang/srt/lora/torch_ops/__init__.py
+++ b/python/sglang/srt/lora/torch_ops/__init__.py
@@ -1,7 +1,6 @@
-from .lora_ops import sgmv_expand, sgmv_expand_slice, sgmv_shrink
+from .lora_ops import sgemm_lora_a_fwd, sgemm_lora_b_fwd
 
 __all__ = [
-    "sgmv_expand",
-    "sgmv_expand_slice",
-    "sgmv_shrink",
+    "sgemm_lora_a_fwd",
+    "sgemm_lora_b_fwd",
 ]
diff --git a/python/sglang/srt/lora/torch_ops/lora_ops.py b/python/sglang/srt/lora/torch_ops/lora_ops.py
index 98c5848a8..7e72b9d4f 100644
--- a/python/sglang/srt/lora/torch_ops/lora_ops.py
+++ b/python/sglang/srt/lora/torch_ops/lora_ops.py
@@ -1,125 +1,109 @@
-# SPDX-License-Identifier: Apache-2.0
-# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+from typing import Optional
 
 import torch
 
 
-def sgmv_expand(
+def sgemm_lora_a_fwd(
     inputs: torch.Tensor,
-    lora_b_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    seq_len_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    add_inputs: bool = False,
+    weights: torch.Tensor,
+    weight_indices: torch.Tensor,
+    seg_len_tensor: torch.Tensor,
+    lora_ranks: torch.Tensor,
+    scaling_tensor: torch.Tensor,
+    num_slices: int = 1,
 ):
-    total_seq_len, _ = inputs.shape
-    exploded_indices = torch.repeat_interleave(
-        lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-    )
-
-    bgmv_expand(inputs, lora_b_weights, output_tensor, exploded_indices, add_inputs)
-
-
-def bgmv_expand(
-    inputs: torch.Tensor,
-    lora_b_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    add_inputs: bool = True,
-):
-    selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=output_tensor.dtype)
-    if len(selected_loras.shape) == 4:
-        selected_loras = selected_loras.squeeze(dim=1)
-    inputs = inputs.to(dtype=output_tensor.dtype)
-    outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-
-    limit = output_tensor.shape[0]
-    if outputs.shape[0] == 1 and output_tensor.shape[0] != 1:
-        limit = 1
+    total_seq_len, input_dim = inputs.shape
+    if weights.numel() == 0:
+        return torch.zeros(total_seq_len, 0, dtype=inputs.dtype, device=inputs.device)
 
-    # LoRA adapter and model may add different amounts of padding to output
-    common_len = min(outputs.shape[1], output_tensor.shape[1])
+    num_loras, weight_out_dim, _ = weights.shape
+    max_rank = weight_out_dim // num_slices
 
-    if add_inputs:
-        output_tensor[:, :common_len] += outputs[:limit, :common_len]
-    else:
-        output_tensor[:, :common_len] = outputs[:limit, :common_len]
+    output = torch.zeros(
+        total_seq_len, num_slices * max_rank, dtype=inputs.dtype, device=inputs.device
+    )
 
+    token_offset = 0
+    for lora_idx, seq_len, rank in zip(
+        weight_indices, seg_len_tensor, lora_ranks[weight_indices]
+    ):
+        if seq_len == 0:
+            continue
 
-def sgmv_shrink(
-    inputs: torch.Tensor,
-    lora_a_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    seq_len_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    scaling: float,
-):
-    total_seq_len, _ = inputs.shape
-    exploded_indices = torch.repeat_interleave(
-        lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-    )
+        if rank > 0:
 
-    bgmv_shrink(inputs, lora_a_weights, output_tensor, exploded_indices, scaling)
+            x_seq = inputs[token_offset : token_offset + seq_len, :]
+            w_seq = weights[lora_idx, : num_slices * rank, :]
 
+            result = torch.einsum("si, oi -> so", x_seq, w_seq)
+            output[token_offset : token_offset + seq_len, : num_slices * rank] = (
+                scaling_tensor[lora_idx] * result
+            )
 
-def bgmv_shrink(
-    inputs: torch.Tensor,
-    lora_a_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    scaling: float = 1.0,
-):
-    selected_loras = lora_a_weights[lora_indices_tensor].to(dtype=output_tensor.dtype)
-    if len(selected_loras.shape) == 4:
-        selected_loras = selected_loras.squeeze(dim=1)
-    inputs = inputs.to(dtype=output_tensor.dtype)
-    outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
+        token_offset += seq_len
 
-    output_tensor[:, : outputs.shape[1]] = scaling * outputs[:]
+    return output
 
 
-def sgmv_expand_slice(
+def sgemm_lora_b_fwd(
     inputs: torch.Tensor,
-    lora_b_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    seq_len_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    slice_offset: int,
-    slice_size: int,
-    add_inputs: bool = False,
+    weights: torch.Tensor,
+    weight_indices: torch.Tensor,
+    seg_len_tensor: torch.Tensor,
+    lora_ranks: torch.Tensor,
+    slice_offsets: torch.Tensor,
+    base_output: Optional[torch.Tensor] = None,
 ):
     total_seq_len, _ = inputs.shape
-    exploded_indices = torch.repeat_interleave(
-        lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-    )
+    num_loras, weight_out_dim, _ = weights.shape
+    total_output_dim = slice_offsets[-1].item() if len(slice_offsets) > 0 else 0
 
-    bgmv_expand_slice(
-        inputs,
-        lora_b_weights,
-        output_tensor,
-        exploded_indices,
-        slice_offset,
-        slice_size,
-        add_inputs,
-    )
+    if weights.numel() == 0:
+        return torch.zeros(
+            total_seq_len, total_output_dim, dtype=inputs.dtype, device=inputs.device
+        )
 
+    num_slices = len(slice_offsets) - 1
 
-def bgmv_expand_slice(
-    inputs: torch.Tensor,
-    lora_b_weights: torch.Tensor,
-    output_tensor: torch.Tensor,
-    lora_indices_tensor: torch.Tensor,
-    slice_offset: int,
-    slice_size: int,
-    add_inputs: bool = True,
-):
-    selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=output_tensor.dtype)
-    inputs = inputs.to(dtype=output_tensor.dtype)
-    if len(selected_loras.shape) == 4:
-        selected_loras = selected_loras.squeeze(dim=1)
-    outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-
-    if add_inputs:
-        output_tensor[:, slice_offset : slice_offset + slice_size] += outputs[:]
+    if base_output is not None:
+        output = base_output
     else:
-        output_tensor[:, slice_offset : slice_offset + slice_size] = outputs[:]
+        output = torch.zeros(
+            total_seq_len, total_output_dim, dtype=inputs.dtype, device=inputs.device
+        )
+
+    token_offset = 0
+    for lora_idx, seq_len, rank in zip(
+        weight_indices, seg_len_tensor, lora_ranks[weight_indices]
+    ):
+        if seq_len == 0:
+            continue
+
+        if rank == 0:
+            token_offset += seq_len
+            continue
+
+        for slice_idx in range(num_slices):
+            slice_start_input = slice_idx * rank
+            slice_end_input = (slice_idx + 1) * rank
+
+            slice_start_output = slice_offsets[slice_idx]
+            slice_end_output = slice_offsets[slice_idx + 1]
+
+            x_slice = inputs[
+                token_offset : token_offset + seq_len :,
+                slice_start_input:slice_end_input,
+            ]  # (seq_len, rank)
+            w_slice = weights[
+                lora_idx, slice_start_output:slice_end_output, :rank
+            ]  # (slice_dim, rank)
+
+            result = torch.einsum("si, oi -> so", x_slice, w_slice)
+            output[
+                token_offset : token_offset + seq_len,
+                slice_start_output:slice_end_output,
+            ] += result
+
+        token_offset += seq_len
+
+    return output
diff --git a/python/sglang/srt/lora/utils.py b/python/sglang/srt/lora/utils.py
index b59c17aa5..1bdd832ac 100644
--- a/python/sglang/srt/lora/utils.py
+++ b/python/sglang/srt/lora/utils.py
@@ -4,6 +4,7 @@ from typing import Iterable, Optional, Set, Tuple
 
 import torch
 
+from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.utils.hf_transformers_utils import AutoConfig
 
 
@@ -151,3 +152,31 @@ def get_target_module_name(full_module_name: str, target_modules: Set[str]) -> s
 
 EMBEDDING_NAMES = ["embed_tokens", "lm_head"]
 ROW_PARALLELISM_LINEAR_LORA_NAMES = ["o_proj", "down_proj"]
+
+
+def generate_sequence_lengths(
+    forward_batch: ForwardBatch, device: Optional[torch.device] = None
+) -> torch.Tensor:
+
+    device = torch.get_default_device() if device is None else device
+    with torch.device(device):
+        if forward_batch.forward_mode.is_decode():
+            seg_lens = torch.ones(forward_batch.batch_size, dtype=torch.int32)
+        elif forward_batch.forward_mode.is_target_verify():
+            seg_lens = torch.full(
+                size=(forward_batch.batch_size,),
+                fill_value=forward_batch.spec_info.draft_token_num,
+                dtype=torch.int32,
+            )
+        elif forward_batch.forward_mode.is_extend():
+            seg_lens = (
+                forward_batch.extend_seq_lens
+                if forward_batch.extend_seq_lens.device == device
+                else torch.tensor(
+                    forward_batch.extend_seq_lens_cpu,
+                    dtype=torch.int32,
+                )
+            )
+        else:
+            raise ValueError(f"Unsupported forward mode: {forward_batch.forward_mode}")
+    return seg_lens
diff --git a/python/sglang/srt/managers/data_parallel_controller.py b/python/sglang/srt/managers/data_parallel_controller.py
index ca75f5b43..ecc2113af 100644
--- a/python/sglang/srt/managers/data_parallel_controller.py
+++ b/python/sglang/srt/managers/data_parallel_controller.py
@@ -464,11 +464,28 @@ class DataParallelController:
         self.max_req_input_len = scheduler_info[0]["max_req_input_len"]
 
     def maybe_external_dp_rank_routing(self, req: Req):
-        if req.data_parallel_rank is not None:
-            logger.debug(f"Direct routing to DP rank {req.data_parallel_rank}")
-            self.workers[req.data_parallel_rank].send_pyobj(req)
-            return True
-        return False
+        if self.server_args.disaggregation_mode == "prefill":
+            if req.data_parallel_rank is not None:
+                logger.debug(
+                    f"Prefill direct routing to DP rank {req.data_parallel_rank}"
+                )
+                self.workers[req.data_parallel_rank].send_pyobj(req)
+                return True
+            return False
+        else:
+            if req.data_parallel_rank_decode is not None:
+                logger.debug(
+                    f"Decode direct routing to DP rank {req.data_parallel_rank_decode}"
+                )
+                self.workers[req.data_parallel_rank_decode].send_pyobj(req)
+                return True
+            if req.data_parallel_rank is not None:
+                logger.debug(
+                    f"Decode direct routing to DP rank {req.data_parallel_rank}, by data parallel rank"
+                )
+                self.workers[req.data_parallel_rank].send_pyobj(req)
+                return True
+            return False
 
     def round_robin_scheduler(self, req: Req):
         if self.maybe_external_dp_rank_routing(req):
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index ef9f4886e..7c6f89be5 100644
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -229,6 +229,11 @@ class GenerateReqInput(BaseReq, APIServingTimingMixin):
     # For data parallel rank routing
     data_parallel_rank: Optional[int] = None
 
+    # In PD-Disaggregation mode, requests are scheduled to different DP group based on the load conditions of prefill and decode.
+    # `data_parallel_rank_decode` indicates which DP group the current request need to be dispatched to for decode
+    # `data_parallel_rank` indicates which DP group the request will be scheduled to on the prefill.
+    data_parallel_rank_decode: Optional[int] = None
+
     # For background responses (OpenAI responses API)
     background: bool = False
 
@@ -653,6 +658,11 @@ class GenerateReqInput(BaseReq, APIServingTimingMixin):
             data_parallel_rank=(
                 self.data_parallel_rank if self.data_parallel_rank is not None else None
             ),
+            data_parallel_rank_decode=(
+                self.data_parallel_rank_decode
+                if self.data_parallel_rank_decode is not None
+                else None
+            ),
             conversation_id=self.conversation_id,
             priority=self.priority,
             extra_key=self.extra_key,
@@ -722,6 +732,8 @@ class TokenizedGenerateReqInput(BaseReq):
     # For data parallel rank routing
     data_parallel_rank: Optional[int] = None
 
+    data_parallel_rank_decode: Optional[int] = None
+
     # Priority for the request
     priority: Optional[int] = None
 
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index 07a6455a4..ca9a39e86 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -496,6 +496,7 @@ class Req:
         bootstrap_room: Optional[int] = None,
         disagg_mode: Optional[DisaggregationMode] = None,
         data_parallel_rank: Optional[int] = None,
+        data_parallel_rank_decode: Optional[int] = None,
         vocab_size: Optional[int] = None,
         priority: Optional[int] = None,
         metrics_collector: Optional[SchedulerMetricsCollector] = None,
@@ -723,6 +724,7 @@ class Req:
 
         # For data parallel rank routing
         self.data_parallel_rank: Optional[int] = data_parallel_rank
+        self.data_parallel_rank_decode: Optional[int] = data_parallel_rank_decode
 
         # the start index of the sent kv cache
         # We want to send it chunk by chunk for chunked prefill.
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 5e48d95fd..e72db43c1 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -292,6 +292,8 @@ class Scheduler(
             )
         )
 
+        self.max_prefill_bs = 0
+
         # Init model configs
         self.init_model_config()
 
@@ -829,7 +831,11 @@ class Scheduler(
             self.server_args.disaggregation_transfer_backend
         )
 
-        if self.draft_worker is None or self.spec_algorithm.is_ngram():
+        if (
+            self.draft_worker is None
+            or self.spec_algorithm.is_ngram()
+            or self.spec_algorithm.is_suffix()
+        ):
             draft_token_to_kv_pool = None
         elif self.spec_algorithm.is_eagle() and self.enable_overlap:
             if self.enable_mtp:
@@ -1083,6 +1089,44 @@ class Scheduler(
             tmp_batch, tmp_result = self.result_queue.popleft()
             self.process_batch_result(tmp_batch, tmp_result)
 
+        import os
+
+        enable_profiling: bool = (
+            os.getenv("ENABLE_PROFILING", "0") == "1" and self.tp_rank == 0
+        )
+        prof_bs: int = int(os.getenv("PROFILING_BS", 8))
+        profiling_stage: str = os.getenv("PROFILING_STAGE", "decode")
+        prof_step: int = int(os.getenv("PROFILING_step", 10))
+        if enable_profiling:
+            prof_cnt = 0
+            import torch_npu
+
+            experimental_config = torch_npu.profiler._ExperimentalConfig(
+                aic_metrics=torch_npu.profiler.AiCMetrics.PipeUtilization,
+                profiler_level=torch_npu.profiler.ProfilerLevel.Level2,
+                l2_cache=False,
+                data_simplification=False,
+            )
+            profiling_path = "profiling/"
+            prof = torch_npu.profiler.profile(
+                activities=[
+                    torch_npu.profiler.ProfilerActivity.CPU,
+                    torch_npu.profiler.ProfilerActivity.NPU,
+                ],
+                on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(
+                    profiling_path
+                ),
+                schedule=torch_npu.profiler.schedule(
+                    wait=1, warmup=1, active=10, repeat=1, skip_first=1
+                ),
+                record_shapes=True,
+                profile_memory=True,
+                with_stack=False,
+                with_flops=False,
+                with_modules=False,
+                experimental_config=experimental_config,
+            )
+
         while True:
             # Receive requests
             recv_reqs = self.recv_requests()
@@ -1102,8 +1146,34 @@ class Scheduler(
 
             # Launch the current batch
             if batch:
+                if enable_profiling:
+                    is_prof_stage = False
+                    if (
+                        profiling_stage == "decode" and batch.forward_mode.is_decode()
+                    ) or (
+                        profiling_stage == "prefill" and batch.forward_mode.is_extend()
+                    ):
+                        is_prof_stage = True
+
+                    if len(batch.reqs) >= prof_bs and prof_cnt == 0 and is_prof_stage:
+                        prof.start()
+                        prof_cnt += 1
+                    if prof_cnt > 0 and is_prof_stage:
+                        prof_cnt += 1
+                    if prof_cnt == prof_step and is_prof_stage:
+                        torch.npu.synchronize()
+                        prof.stop()
+
                 batch_result = self.run_batch(batch)
                 self.result_queue.append((batch.copy(), batch_result))
+
+                if (
+                    enable_profiling
+                    and prof_cnt > 0
+                    and prof_cnt < prof_step
+                    and is_prof_stage
+                ):
+                    prof.step()
             else:
                 batch_result = None
 
@@ -1420,6 +1490,7 @@ class Scheduler(
                 bootstrap_room=recv_req.bootstrap_room,
                 disagg_mode=self.disaggregation_mode,
                 data_parallel_rank=recv_req.data_parallel_rank,
+                data_parallel_rank_decode=recv_req.data_parallel_rank_decode,
                 vocab_size=self.model_config.vocab_size,
                 priority=recv_req.priority,
                 metrics_collector=(
@@ -1749,6 +1820,15 @@ class Scheduler(
             if self.chunked_req is not None and self.chunked_req.finished():
                 self.chunked_req = None
 
+        skip_prefill_scheduler = False
+        if self.schedule_enhancer and not self.schedule_enhancer.get_schedule_decision(
+            self.running_batch, self.max_prefill_bs
+        ):
+            # Decrease prefill idle as much as possible during high dp load.
+            skip_prefill_scheduler = True
+            chunked_back = self.chunked_req
+            self.chunked_req = None
+
         # Merge the prefill batch into the running batch
         chunked_req_to_exclude = set()
         if self.chunked_req:
@@ -1788,7 +1868,10 @@ class Scheduler(
                     # Merge running_batch with prefill batch
                     self.running_batch.merge_batch(self.last_batch)
 
-        new_batch = self.get_new_batch_prefill()
+        if skip_prefill_scheduler:
+            new_batch = None
+        else:
+            new_batch = self.get_new_batch_prefill()
 
         need_mlp_sync = self.require_mlp_sync
         if need_mlp_sync and not self.spec_algorithm.is_none():
@@ -1816,7 +1899,8 @@ class Scheduler(
 
         if ret:
             trace_event_batch("schedule", ret.reqs)
-
+        if skip_prefill_scheduler:
+            self.chunked_req = chunked_back
         return ret
 
     def get_num_allocatable_reqs(self, running_bs):
@@ -1826,12 +1910,6 @@ class Scheduler(
         return res
 
     def get_new_batch_prefill(self) -> Optional[ScheduleBatch]:
-        if self.schedule_enhancer and not self.schedule_enhancer.get_schedule_decision(
-            self.running_batch
-        ):
-            # Decrease prefill idle as much as possible during high dp load.
-            return None
-
         # Check if the grammar is ready in the grammar queue
         if self.grammar_queue:
             self.move_ready_grammar_requests()
@@ -1959,6 +2037,7 @@ class Scheduler(
                         self.running_batch.batch_is_full = True
                 break
 
+        self.max_prefill_bs = max(self.max_prefill_bs, len(adder.can_run_list))
         # Update waiting queue
         can_run_list: List[Req] = adder.can_run_list
         if len(can_run_list) == 0:
diff --git a/python/sglang/srt/managers/scheduler_enhancer.py b/python/sglang/srt/managers/scheduler_enhancer.py
index d7799aa8c..fdccb2344 100644
--- a/python/sglang/srt/managers/scheduler_enhancer.py
+++ b/python/sglang/srt/managers/scheduler_enhancer.py
@@ -15,14 +15,12 @@ class SchedulerEnhancer:
         self.cpu_group = tp_worker.get_tp_group().cpu_group
         self.max_running_requests = max_running_requests
         self.stable_count = 0
-        # If scheduling is performed 30 times and some dp units are still at full load, the prefill-prioritized scheduling strategy will still be used.
-        self.max_stable_count = 30
+        # If scheduling is performed 200 times and some dp units are still at full load, the prefill-prioritized scheduling strategy will still be used.
+        self.max_stable_count = 200
+        self.server_args = server_args
         assert (
             server_args.schedule_policy == "fcfs"
         ), f"To use SCHEDULER_DECREASE_PREFILL_IDLE, schedule_policy must be 'fcfs'. '{self.schedule_policy}' is not supported."
-        assert (
-            server_args.enable_dp_attention == True
-        ), f"To use SCHEDULER_DECREASE_PREFILL_IDLE, enable_dp_attention must be enable."
         assert (
             server_args.disaggregation_mode == "null"
         ), f"To use SCHEDULER_DECREASE_PREFILL_IDLE, disaggregation_mode must be null."
@@ -46,14 +44,22 @@ class SchedulerEnhancer:
         tp0_info = self.global_batch_size[:, 0, :]
         return tp0_info
 
-    def get_schedule_decision(self, running_batch):
-        tp0_info = self.get_schedule_info(running_batch)
-        if (
-            int(tp0_info[:, 0].min().item()) < self.max_running_requests
-            and int(tp0_info[:, 0].max().item()) == self.max_running_requests
+    def get_schedule_decision(self, running_batch, max_prefill_bs):
+        if self.server_args.enable_dp_attention:
+            tp0_info = self.get_schedule_info(running_batch)
+            if (
+                int(tp0_info[:, 0].min().item()) < self.max_running_requests
+                and int(tp0_info[:, 0].max().item()) == self.max_running_requests
+            ):
+                self.stable_count += 1
+                if self.stable_count < self.max_stable_count:
+                    return False
+        elif (
+            self.max_running_requests - running_batch.batch_size() < max_prefill_bs
         ):
             self.stable_count += 1
             if self.stable_count < self.max_stable_count:
                 return False
+
         self.stable_count = 0
         return True
diff --git a/python/sglang/srt/managers/scheduler_metrics_mixin.py b/python/sglang/srt/managers/scheduler_metrics_mixin.py
index 57cb3f0c5..3172ac98c 100644
--- a/python/sglang/srt/managers/scheduler_metrics_mixin.py
+++ b/python/sglang/srt/managers/scheduler_metrics_mixin.py
@@ -336,8 +336,15 @@ class SchedulerMetricsMixin:
             msg += f"#transfer-req: {len(self.disagg_decode_transfer_queue.queue)}, "
             msg += f"#retracted-req: {len(self.disagg_decode_prealloc_queue.retracted_queue)}, "
 
+        graph_name = defaultdict(
+            lambda: "cuda graph",
+            {
+                "cpu": "cpu graph",
+                "npu": "npu graph",
+            },
+        )
         msg += (
-            f"{'cuda graph' if self.device == 'cuda' else 'cpu graph'}: {can_run_cuda_graph}, "
+            f"{graph_name[self.device]}: {can_run_cuda_graph}, "
             f"gen throughput (token/s): {self.last_gen_throughput:.2f}, "
             f"#queue-req: {len(self.waiting_queue)}, "
         )
diff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py
index 073584044..f47c500c9 100644
--- a/python/sglang/srt/managers/tokenizer_manager.py
+++ b/python/sglang/srt/managers/tokenizer_manager.py
@@ -848,6 +848,7 @@ class TokenizerManager(TokenizerCommunicatorMixin, TokenizerManagerMultiItemMixi
                 return_hidden_states=obj.return_hidden_states,
                 return_routed_experts=obj.return_routed_experts,
                 data_parallel_rank=obj.data_parallel_rank,
+                data_parallel_rank_decode=obj.data_parallel_rank_decode,
                 priority=obj.priority,
                 extra_key=obj.extra_key,
                 need_wait_for_image=obj.need_wait_for_image,
diff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py
index 2d96431a3..f515761ea 100644
--- a/python/sglang/srt/model_executor/cuda_graph_runner.py
+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py
@@ -279,6 +279,7 @@ class CudaGraphRunner:
             model_runner.spec_algorithm.is_eagle()
             or model_runner.spec_algorithm.is_standalone()
             or model_runner.spec_algorithm.is_ngram()
+            or model_runner.spec_algorithm.is_suffix()
         ):
             if self.model_runner.is_draft_worker:
                 raise RuntimeError("This should not happen")
@@ -425,6 +426,7 @@ class CudaGraphRunner:
                 == forward_batch.input_ids.numel()
             )
             if self.model_runner.spec_algorithm.is_ngram()
+            or self.model_runner.spec_algorithm.is_suffix()
             else True
         )
 
@@ -914,6 +916,20 @@ class CudaGraphRunner:
             )
             spec_info.capture_hidden_mode = CaptureHiddenMode.NULL
 
+        elif self.model_runner.spec_algorithm.is_suffix():
+            from sglang.srt.speculative.suffix_info import SuffixVerifyInput
+
+            spec_info = SuffixVerifyInput(
+                draft_token=None,
+                tree_mask=self.buffers.custom_mask,
+                positions=None,
+                retrive_index=None,
+                retrive_next_token=None,
+                retrive_next_sibling=None,
+                draft_token_num=self.num_tokens_per_bs,
+            )
+            spec_info.capture_hidden_mode = CaptureHiddenMode.NULL
+
         return spec_info
 
 
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 0660c9687..d8e10e8d7 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -447,11 +447,15 @@ class ModelRunner:
             )
 
         # Expert parallelism
-        self.eplb_manager = (
-            EPLBManager(self)
-            if self.server_args.enable_eplb and (not self.is_draft_worker)
-            else None
-        )
+        if self.server_args.enable_eplb and (not self.is_draft_worker):
+            if self.server_args.enable_async_eplb:
+                from sglang.srt.eplb.async_eplb_manager import AsyncEPLBManager
+
+                self.eplb_manager = AsyncEPLBManager(self)
+            else:
+                self.eplb_manager = EPLBManager(self)
+        else:
+            self.eplb_manager = None
         self.expert_location_updater = ExpertLocationUpdater()
 
         (
@@ -2005,7 +2009,9 @@ class ModelRunner:
                     dtype=self.kv_cache_dtype,
                     kv_lora_rank=self.model_config.kv_lora_rank,
                     qk_rope_head_dim=self.model_config.qk_rope_head_dim,
-                    index_head_dim=self.model_config.index_head_dim,
+                    index_head_dim=(
+                        self.model_config.index_head_dim if is_nsa_model else None
+                    ),
                     layer_num=self.num_effective_layers,
                     device=self.device,
                     enable_memory_saver=self.server_args.enable_memory_saver,
@@ -2655,10 +2661,17 @@ class ModelRunner:
         if self.device == "cpu" and not self.server_args.enable_torch_compile:
             return
 
+        graph_name = defaultdict(
+            lambda: "cuda graph",
+            {
+                "cpu": "cpu graph",
+                "npu": "npu graph",
+            },
+        )
         tic = time.perf_counter()
         before_mem = get_available_gpu_memory(self.device, self.gpu_id)
         logger.info(
-            f"Capture {'cpu graph' if self.device == 'cpu' else 'cuda graph'} begin. This can take up to several minutes. avail mem={before_mem:.2f} GB"
+            f"Capture {graph_name[self.device]} begin. This can take up to several minutes. avail mem={before_mem:.2f} GB"
         )
         graph_runners = defaultdict(
             lambda: CudaGraphRunner,
@@ -2672,7 +2685,7 @@ class ModelRunner:
         after_mem = get_available_gpu_memory(self.device, self.gpu_id)
         self.graph_mem_usage = before_mem - after_mem
         logger.info(
-            f"Capture {'cpu graph' if self.device == 'cpu' else 'cuda graph'} end. Time elapsed: {time.perf_counter() - tic:.2f} s. "
+            f"Capture {graph_name[self.device]} end. Time elapsed: {time.perf_counter() - tic:.2f} s. "
             f"mem usage={self.graph_mem_usage:.2f} GB. avail mem={after_mem:.2f} GB."
         )
 
diff --git a/python/sglang/srt/models/baichuan.py b/python/sglang/srt/models/baichuan.py
index 6e8d3b4a3..b173c4d35 100644
--- a/python/sglang/srt/models/baichuan.py
+++ b/python/sglang/srt/models/baichuan.py
@@ -32,7 +32,7 @@ from sglang.srt.distributed import (
 from sglang.srt.layers.activation import SiluAndMul
 from sglang.srt.layers.layernorm import RMSNorm
 from sglang.srt.layers.linear import (
-    MergedColumnParallelLinear,
+    ColumnParallelLinear,
     QKVParallelLinear,
     RowParallelLinear,
 )
@@ -46,7 +46,8 @@ from sglang.srt.layers.vocab_parallel_embedding import (
 )
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_loader.weight_utils import default_weight_loader
-from sglang.srt.utils import add_prefix
+from sglang.srt.utils import add_prefix, is_npu
+_is_npu = is_npu()
 
 
 def _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:
@@ -84,12 +85,19 @@ class BaiChuanMLP(nn.Module):
         prefix: str = "",
     ):
         super().__init__()
-        self.gate_up_proj = MergedColumnParallelLinear(
+        self.gate_proj = ColumnParallelLinear(
             hidden_size,
-            [intermediate_size] * 2,
+            intermediate_size,
             bias=False,
             quant_config=quant_config,
-            prefix=add_prefix("gate_up_proj", prefix),
+            prefix=add_prefix("gate_proj", prefix),
+        )
+        self.up_proj = ColumnParallelLinear(
+            hidden_size,
+            intermediate_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=add_prefix("up_proj", prefix),
         )
         self.down_proj = RowParallelLinear(
             intermediate_size,
@@ -103,13 +111,15 @@ class BaiChuanMLP(nn.Module):
                 f"Unsupported activation: {hidden_act}. "
                 "Only silu is supported for now."
             )
-        self.act_fn = SiluAndMul()
+        self.act_fn = torch.nn.functional.silu
 
     def forward(self, x):
-        gate_up, _ = self.gate_up_proj(x)
-        x = self.act_fn(gate_up)
-        x, _ = self.down_proj(x)
-        return x
+        dtype = x.dtype
+        x0, _ = self.gate_proj(x)
+        x1, _ = self.up_proj(x)
+        y = self.act_fn(x0.to(torch.float32)) * x1.permute(0, 1)
+        z, _ = self.down_proj(y)
+        return z.to(dtype)
 
 
 class BaiChuanAttention(nn.Module):
@@ -124,29 +134,21 @@ class BaiChuanAttention(nn.Module):
         max_position_embeddings: int = 8192,
         quant_config: Optional[QuantizationConfig] = None,
         layer_id: int = 0,
+        dtype: Optional[torch.dtype] = torch.bfloat16,
         prefix: str = "",
     ):
         super().__init__()
         self.hidden_size = hidden_size
-        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size()
         tp_size = get_tensor_model_parallel_world_size()
         self.total_num_heads = num_heads
-        assert self.total_num_heads % tensor_model_parallel_world_size == 0
-        self.num_heads = self.total_num_heads // tensor_model_parallel_world_size
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
         self.head_dim = hidden_size // self.total_num_heads
-        self.postion_embedding = position_embedding
+        self.position_embedding = position_embedding
         self.rope_theta = rope_theta
         self.max_position_embeddings = max_position_embeddings
-        self.total_num_kv_heads = self.num_heads
-        if self.total_num_kv_heads >= tp_size:
-            # Number of KV heads is greater than TP size, so we partition
-            # the KV heads across multiple tensor parallel GPUs.
-            assert self.total_num_kv_heads % tp_size == 0
-        else:
-            # Number of KV heads is less than TP size, so we replicate
-            # the KV heads across multiple tensor parallel GPUs.
-            assert tp_size % self.total_num_kv_heads == 0
-        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.total_num_kv_heads = self.total_num_heads
+        self.num_kv_heads = self.num_heads
 
         # pylint: disable=invalid-name
         self.W_pack = QKVParallelLinear(
@@ -156,26 +158,29 @@ class BaiChuanAttention(nn.Module):
             self.total_num_heads,
             bias=False,
             quant_config=quant_config,
+            prefix=add_prefix("W_pack", prefix),
         )
         self.o_proj = RowParallelLinear(
             self.total_num_heads * self.head_dim,
             hidden_size,
             bias=False,
             quant_config=quant_config,
+            prefix=add_prefix("o_proj", prefix),
         )
+        self.scaling = self.head_dim**-0.5
         # Create the alibi slopes and slice them.
-        if self.postion_embedding == "ALIBI":
+        if self.position_embedding == "ALIBI":
             tp_rank = get_tensor_model_parallel_rank()
             head_start = tp_rank * self.num_heads
             head_end = (tp_rank + 1) * self.num_heads
             alibi_slopes = _get_alibi_slopes(self.total_num_heads)
-            alibi_slopes = alibi_slopes[head_start:head_end].tolist()
+            alibi_slopes = alibi_slopes[head_start: head_end]
+            self.alibi_slopes = torch.tensor(alibi_slopes, dtype=dtype, device="npu" if _is_npu else "cuda")
 
-            scaling = self.head_dim**-0.5
             self.attn = RadixAttention(
                 self.num_heads,
                 self.head_dim,
-                scaling,
+                self.scaling,
                 num_kv_heads=self.num_kv_heads,
                 layer_id=layer_id,
                 quant_config=quant_config,
@@ -188,7 +193,7 @@ class BaiChuanAttention(nn.Module):
                 max_position=self.max_position_embeddings,
                 base=self.rope_theta,
             )
-            self.scaling = self.head_dim**-0.5
+
             self.attn = RadixAttention(
                 self.num_heads,
                 self.head_dim,
@@ -207,9 +212,12 @@ class BaiChuanAttention(nn.Module):
     ) -> torch.Tensor:
         qkv, _ = self.W_pack(hidden_states)
         q, k, v = qkv.chunk(chunks=3, dim=-1)
-        if self.postion_embedding != "ALIBI":
+        if self.position_embedding != "ALIBI":
             q, k = self.rotary_emb(positions, q, k)
-        attn_output = self.attn(q, k, v, forward_batch)
+            attn_output = self.attn(q, k, v, forward_batch)
+        else:
+            attn_output = self.attn(q, k, v, forward_batch, slopes=self.alibi_slopes)
+            
         output, _ = self.o_proj(attn_output)
         return output
 
@@ -292,6 +300,8 @@ class BaiChuanModel(nn.Module):
         self.embed_tokens = VocabParallelEmbedding(
             config.vocab_size,
             config.hidden_size,
+            org_num_embeddings=config.vocab_size,
+            prefix=add_prefix("embed_tokens",prefix),
         )
         self.layers = nn.ModuleList(
             [
@@ -330,19 +340,25 @@ class BaiChuanModel(nn.Module):
 class BaiChuanBaseForCausalLM(nn.Module):
     packed_modules_mapping = {
         "W_pack": ["W_pack"],
-        "gate_up_proj": [
-            "gate_proj",
-            "up_proj",
+        "gate_proj": [
+            "gate_proj"
+        ],
+        "up_proj": [
+            "up_proj"
+        ],
+        "down_proj": [
+            "down_proj"
         ],
     }
     # LoRA specific attributes
     supported_lora_modules = [
         "W_pack",
         "o_proj",
-        "gate_up_proj",
+        "gate_proj",
+        "up_proj",
         "down_proj",
     ]
-    embedding_modules = {}
+    embedding_modules = {"embed_tokens":["embed_tokens"],}
     embedding_padding_modules = []
 
     def __init__(
@@ -383,11 +399,6 @@ class BaiChuanBaseForCausalLM(nn.Module):
         )
 
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
-        stacked_params_mapping = [
-            # (param_name, shard_name, shard_id)
-            ("gate_up_proj", "gate_proj", 0),
-            ("gate_up_proj", "up_proj", 1),
-        ]
         params_dict = dict(self.named_parameters())
         for name, loaded_weight in weights:
             if "rotary_emb.inv_freq" in name:
@@ -403,24 +414,11 @@ class BaiChuanBaseForCausalLM(nn.Module):
                 if is_baichuan2:
                     loaded_weight = torch.nn.functional.normalize(loaded_weight)
 
-            for param_name, weight_name, shard_id in stacked_params_mapping:
-                if weight_name not in name:
-                    continue
-                name = name.replace(weight_name, param_name)
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                param = params_dict[name]
-                weight_loader = param.weight_loader
-                weight_loader(param, loaded_weight, shard_id)
-                break
-            else:
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                param = params_dict[name]
-                weight_loader = getattr(param, "weight_loader", default_weight_loader)
-                weight_loader(param, loaded_weight)
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+            param = params_dict[name]
+            weight_loader = getattr(param, "weight_loader", default_weight_loader)
+            weight_loader(param, loaded_weight)
 
 
 class BaichuanForCausalLM(BaiChuanBaseForCausalLM):
diff --git a/python/sglang/srt/models/dbrx.py b/python/sglang/srt/models/dbrx.py
index 74de384b3..ca64cd5af 100644
--- a/python/sglang/srt/models/dbrx.py
+++ b/python/sglang/srt/models/dbrx.py
@@ -32,10 +32,15 @@ from sglang.srt.layers.linear import (
     RowParallelLinear,
 )
 from sglang.srt.layers.logits_processor import LogitsProcessor
-from sglang.srt.layers.moe.fused_moe_triton.fused_moe import fused_moe
-from sglang.srt.layers.moe.moe_runner import MoeRunnerConfig
+from sglang.srt.layers.moe import MoeRunnerConfig, get_moe_runner_backend
+from sglang.srt.layers.moe.token_dispatcher.base import CombineInput
+from sglang.srt.layers.moe.token_dispatcher.standard import StandardDispatchOutput
 from sglang.srt.layers.moe.topk import TopK
-from sglang.srt.layers.quantization.base_config import QuantizationConfig
+from sglang.srt.layers.quantization.base_config import (
+    QuantizationConfig,
+    QuantizeMethodBase,
+)
+from sglang.srt.layers.quantization.unquant import UnquantizedFusedMoEMethod
 from sglang.srt.layers.radix_attention import RadixAttention
 from sglang.srt.layers.rotary_embedding import get_rope
 from sglang.srt.layers.vocab_parallel_embedding import (
@@ -48,7 +53,7 @@ from sglang.srt.model_loader.weight_utils import (
     default_weight_loader,
     maybe_remap_kv_scale_name,
 )
-from sglang.srt.utils import add_prefix, set_weight_attrs
+from sglang.srt.utils import add_prefix, is_cuda, is_npu, set_weight_attrs
 
 
 class DbrxRouter(nn.Module):
@@ -96,10 +101,22 @@ class DbrxExperts(nn.Module):
     ):
         super().__init__()
         self.tp_size = get_tensor_model_parallel_world_size()
-        self.num_total_experts = config.ffn_config.moe_num_experts
+        self.num_experts = self.num_total_experts = config.ffn_config.moe_num_experts
         self.top_k = config.ffn_config.moe_top_k
-        self.d_model = config.d_model
+        self.hidden_size = self.d_model = config.d_model
         self.intermediate_size = config.ffn_config.ffn_hidden_size // self.tp_size
+        self.use_triton_kernels = get_moe_runner_backend().is_triton_kernels()
+        self.moe_runner_config = MoeRunnerConfig(inplace=True)
+        if quant_config is None:
+            self.quant_method: Optional[QuantizeMethodBase] = UnquantizedFusedMoEMethod(
+                self.use_triton_kernels
+            )
+            self.quant_method.create_moe_runner(self, self.moe_runner_config)
+        else:
+            self.quant_method: Optional[QuantizeMethodBase] = (
+                quant_config.get_quant_method(self, prefix)
+            )
+        assert self.quant_method is not None
 
         if params_dtype is None:
             params_dtype = torch.get_default_dtype()
@@ -110,34 +127,39 @@ class DbrxExperts(nn.Module):
             self.top_k,
             renormalize=True,
         )
-        self.moe_runner_config = MoeRunnerConfig(inplace=True)
-        self.ws = nn.Parameter(
+        if is_npu():
+            devices = "npu"
+        elif is_cuda():
+            devices = "cuda"
+        else:
+            devices = "cpu"
+        self.w13_weight = nn.Parameter(
             torch.empty(
                 self.num_total_experts,
                 2 * self.intermediate_size,
                 self.d_model,
-                device="cuda",
+                device=devices,
                 dtype=self.params_dtype,
             )
         )
-        self.w2s = nn.Parameter(
+        self.w2_weight = nn.Parameter(
             torch.empty(
                 self.num_total_experts,
                 self.d_model,
                 self.intermediate_size,
-                device="cuda",
+                device=devices,
                 dtype=self.params_dtype,
             )
         )
 
         set_weight_attrs(
-            self.ws,
+            self.w13_weight,
             {
                 "weight_loader": self.weight_loader,
             },
         )
         set_weight_attrs(
-            self.w2s,
+            self.w2_weight,
             {
                 "weight_loader": self.weight_loader,
             },
@@ -177,13 +199,18 @@ class DbrxExperts(nn.Module):
         # router_logits: (num_tokens, n_experts)
         router_logits = self.router(hidden_states)
         topk_output = self.topk(hidden_states, router_logits)
-        final_hidden_states = fused_moe(
-            hidden_states,
-            self.ws,
-            self.w2s,
-            topk_output,
-            self.moe_runner_config,
+        dispatch_output = StandardDispatchOutput(
+            hidden_states=hidden_states,
+            hidden_states_scale=None,
+            topk_output=topk_output,
+        )
+        final_hidden_states = self.quant_method.apply(
+            layer=self, dispatch_output=dispatch_output
         )
+        if isinstance(
+            final_hidden_states, CombineInput
+        ):  # quant_method use `UnquantizedFusedMoEMethod`
+            final_hidden_states = final_hidden_states.hidden_states
 
         if self.tp_size > 1:
             final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
@@ -431,7 +458,7 @@ class DbrxForCausalLM(nn.Module):
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
         expert_params_mapping = [
             (
-                "ws" if weight_name in ["w1", "v1"] else "w2s",
+                "w13_weight" if weight_name in ["w1", "v1"] else "w2_weight",
                 f"experts.mlp.{weight_name}",
             )
             for weight_name in ["w1", "v1", "w2"]
diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index 4fd8376e4..b12e4f44a 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -1417,6 +1417,7 @@ class DeepseekV2AttentionMLA(nn.Module):
                 self.rotary_emb.forward = self.rotary_emb.forward_native
         else:
             self.rotary_emb = None
+        self.use_deepseek_yarn_rope = rope_scaling is not None
 
         self.attn_mqa = RadixAttention(
             self.num_local_heads,
diff --git a/python/sglang/srt/models/ernie4.py b/python/sglang/srt/models/ernie4.py
index dffd8f09a..c2e5499f3 100644
--- a/python/sglang/srt/models/ernie4.py
+++ b/python/sglang/srt/models/ernie4.py
@@ -172,6 +172,7 @@ class Ernie4DecoderLayer(nn.Module):
             quant_config=quant_config,
             prefix=add_prefix("self_attn", prefix),
             bias=config.use_bias,
+            force_native_attention_path=True,
         )
         moe_layer_start_index = getattr(
             config, "moe_layer_start_index", config.num_hidden_layers
diff --git a/python/sglang/srt/models/glm4v.py b/python/sglang/srt/models/glm4v.py
index cee2dd8d3..ace0924e8 100644
--- a/python/sglang/srt/models/glm4v.py
+++ b/python/sglang/srt/models/glm4v.py
@@ -412,6 +412,7 @@ class Glm4vVisionModel(nn.Module):
                     num_heads=self.num_heads,
                     quant_config=quant_config,
                     prefix=add_prefix(f"blocks.{layer_idx}", prefix),
+                    num_dummy_heads=vision_config.num_dummy_heads,
                     rms_norm_eps=vision_config.rms_norm_eps,
                     attn_qkv_bias=vision_config.attention_bias,
                     use_data_parallel=use_data_parallel,
@@ -547,6 +548,7 @@ class Glm4vForConditionalGeneration(nn.Module):
         self.pp_group = get_pp_group()
         self.config = config
         self.use_data_parallel = get_global_server_args().mm_enable_dp_encoder
+        vision_utils.update_vit_attn_dummy_heads_config(self.config)
         self.visual = Glm4vVisionModel(
             config.vision_config,
             quant_config=quant_config,
@@ -554,8 +556,6 @@ class Glm4vForConditionalGeneration(nn.Module):
             use_data_parallel=self.use_data_parallel,
         )
 
-        vision_utils.update_vit_attn_dummy_heads_config(self.config)
-
         self.model = Glm4Model(
             config,
             quant_config=quant_config,
diff --git a/python/sglang/srt/models/grok.py b/python/sglang/srt/models/grok.py
index a089475b7..9e4053325 100644
--- a/python/sglang/srt/models/grok.py
+++ b/python/sglang/srt/models/grok.py
@@ -18,6 +18,7 @@ import math
 from typing import Iterable, Optional, Tuple
 
 import torch
+import torch.nn.functional as F
 from torch import nn
 from transformers import PretrainedConfig
 
@@ -59,7 +60,9 @@ from sglang.srt.model_executor.cuda_graph_runner import get_is_capture_mode
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_loader.loader import DefaultModelLoader
 from sglang.srt.model_loader.weight_utils import default_weight_loader
-from sglang.srt.utils import add_prefix
+from sglang.srt.utils import add_prefix, is_npu
+
+_is_npu = is_npu()
 
 logger = logging.getLogger(__name__)
 
@@ -100,7 +103,10 @@ class Grok1MLP(nn.Module):
 
     def forward(self, x):
         gate_up, _ = self.gate_up_proj(x)
-        x, _ = gelu_and_mul_triton(gate_up)
+        if _is_npu:
+            x = self.act_fn(gate_up)
+        else:
+            x, _ = gelu_and_mul_triton(gate_up)
         x, _ = self.down_proj(x)
         return x
 
@@ -143,7 +149,7 @@ class Grok1MoE(nn.Module):
             top_k=top_k,
             renormalize=False,
             layer_id=layer_id,
-            custom_routing_function=custom_routing_function,
+            custom_routing_function=None if _is_npu else custom_routing_function,
         )
 
         self.experts = FusedMoE(
@@ -162,8 +168,21 @@ class Grok1MoE(nn.Module):
         )
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-        topk_output = self.topk(hidden_states, self.gate.weight)
-        return self.experts(hidden_states, topk_output)
+        if _is_npu:
+            orig_shape = hidden_states.shape
+            hidden_states = hidden_states.view(-1, self.hidden_size)
+
+            router_logits, _ = self.gate(hidden_states)
+            router_logits = self.router_logit_softcapping * F.tanh(
+                router_logits / self.router_logit_softcapping
+            )
+            topk_output = self.topk(hidden_states, router_logits)
+
+            final_hidden_states = self.experts(hidden_states, topk_output)
+            return final_hidden_states.view(orig_shape)
+        else:
+            topk_output = self.topk(hidden_states, self.gate.weight)
+            return self.experts(hidden_states, topk_output)
 
 
 def _yarn_linear_ramp_mask(
@@ -337,7 +356,7 @@ class Grok1Attention(nn.Module):
         self.rope_theta = rope_theta
         rope_scaling = get_rope_scaling(config)
         self.load_presharded_attn = load_presharded_attn
-        self.alt_stream = alt_stream or torch.cuda.Stream()
+        self.alt_stream = alt_stream or torch.get_device_module().Stream()
 
         self.qkv_proj = QKVParallelLinear(
             hidden_size,
@@ -451,7 +470,7 @@ class Grok1DecoderLayer(nn.Module):
         self.hidden_size = config.hidden_size
         self.residual_moe = getattr(config, "residual_moe", False)
         self.layer_id = layer_id
-        self.alt_stream = alt_stream or torch.cuda.Stream()
+        self.alt_stream = alt_stream or torch.get_device_module().Stream()
 
         rope_theta = getattr(config, "rope_theta", 10000)
         self.self_attn = Grok1Attention(
@@ -583,10 +602,10 @@ class Grok1DecoderLayer(nn.Module):
 
     def moe_with_rmoe(self, x):
         if self.alt_stream is not None and get_is_capture_mode():
-            current_stream = torch.cuda.current_stream()
+            current_stream = torch.get_device_module().current_stream()
             self.alt_stream.wait_stream(current_stream)
             mlp_result = self.mlp(x)
-            with torch.cuda.stream(self.alt_stream):
+            with torch.get_device_module().stream(self.alt_stream):
                 moe_result = self.block_sparse_moe(x)
             current_stream.wait_stream(self.alt_stream)
         else:
@@ -620,7 +639,7 @@ class Grok1Model(nn.Module):
             prefix=add_prefix("embed_tokens", prefix),
         )
 
-        self.alt_stream = torch.cuda.Stream()
+        self.alt_stream = torch.get_device_module().Stream()
         self.layers = nn.ModuleList(
             [
                 Grok1DecoderLayer(
diff --git a/python/sglang/srt/models/llama.py b/python/sglang/srt/models/llama.py
index 4176af28d..759f67c5d 100644
--- a/python/sglang/srt/models/llama.py
+++ b/python/sglang/srt/models/llama.py
@@ -29,6 +29,12 @@ from sglang.srt.distributed import (
     get_tensor_model_parallel_world_size,
 )
 from sglang.srt.layers.activation import SiluAndMul
+from sglang.srt.layers.communicator import LayerCommunicator, LayerScatterModes
+from sglang.srt.layers.dp_attention import (
+    get_attention_tp_rank,
+    get_attention_tp_size,
+    is_dp_attention_enabled,
+)
 from sglang.srt.layers.layernorm import RMSNorm
 from sglang.srt.layers.linear import (
     MergedColumnParallelLinear,
@@ -52,10 +58,14 @@ from sglang.srt.model_loader.weight_utils import (
     maybe_remap_kv_scale_name,
 )
 from sglang.srt.server_args import get_global_server_args
-from sglang.srt.utils import add_prefix, make_layers
+from sglang.srt.utils import add_prefix, make_layers, is_npu
 from sglang.utils import get_exception_traceback
 
 logger = logging.getLogger(__name__)
+_is_npu = is_npu()
+
+if _is_npu:
+    from sgl_kernel_npu.norm.split_qkv_rmsnorm_rope import split_qkv_rmsnorm_rope
 
 
 class LlamaMLP(nn.Module):
@@ -121,10 +131,14 @@ class LlamaAttention(nn.Module):
         quant_config: Optional[QuantizationConfig] = None,
         prefix: str = "",
         bias: bool = False,
+        force_native_attention_path: bool = False,
     ) -> None:
         super().__init__()
+        self.force_native_attention_path = force_native_attention_path
         self.hidden_size = hidden_size
-        tp_size = get_tensor_model_parallel_world_size()
+        tp_size = get_attention_tp_size()
+        self.attn_tp_size = tp_size
+        self.attn_tp_rank = get_attention_tp_rank()
         self.total_num_heads = num_heads
         assert self.total_num_heads % tp_size == 0
         self.num_heads = self.total_num_heads // tp_size
@@ -155,6 +169,8 @@ class LlamaAttention(nn.Module):
             self.head_dim,
             self.total_num_heads,
             self.total_num_kv_heads,
+            tp_rank=self.attn_tp_rank,
+            tp_size=self.attn_tp_size,
             bias=bias,
             quant_config=quant_config,
             prefix=add_prefix("qkv_proj", prefix),
@@ -164,6 +180,9 @@ class LlamaAttention(nn.Module):
             hidden_size,
             bias=bias,
             quant_config=quant_config,
+            tp_rank=self.attn_tp_rank,
+            tp_size=self.attn_tp_size,
+            reduce_results=False,
             prefix=add_prefix("o_proj", prefix),
         )
 
@@ -185,15 +204,44 @@ class LlamaAttention(nn.Module):
             prefix=add_prefix("attn", prefix),
         )
 
+    def forward_prepare_native(self, positions, hidden_states):
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k = self.rotary_emb(positions, q, k)
+        return q, k, v
+
+    def forward_prepare_npu(self, positions, hidden_states, forward_batch):
+        qkv, _ = self.qkv_proj(hidden_states)
+        if self.attn.layer_id == forward_batch.token_to_kv_pool.start_layer:
+            self.rotary_emb.get_cos_sin_with_position(positions)
+        q, k, v = split_qkv_rmsnorm_rope(
+            qkv,
+            self.rotary_emb.position_sin,
+            self.rotary_emb.position_cos,
+            self.q_size,
+            self.kv_size,
+            self.head_dim,
+        )
+        return q, k, v
+
     def forward(
         self,
         positions: torch.Tensor,
         hidden_states: torch.Tensor,
         forward_batch: ForwardBatch,
     ) -> torch.Tensor:
-        qkv, _ = self.qkv_proj(hidden_states)
-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
-        q, k = self.rotary_emb(positions, q, k)
+        if not _is_npu or not hasattr(self.rotary_emb, 'get_cos_sin_with_position') or self.force_native_attention_path:
+            q, k, v = self.forward_prepare_native(
+                positions=positions,
+                hidden_states=hidden_states,
+            )
+        else:
+            q, k, v = self.forward_prepare_npu(
+                positions=positions,
+                hidden_states=hidden_states,
+                forward_batch=forward_batch,
+            )
+
         attn_output = self.attn(q, k, v, forward_batch)
         output, _ = self.o_proj(attn_output)
         return output
@@ -249,6 +297,18 @@ class LlamaDecoderLayer(nn.Module):
         self.post_attention_layernorm = RMSNorm(
             config.hidden_size, eps=config.rms_norm_eps
         )
+        self.layer_scatter_modes = LayerScatterModes.init_new(
+            layer_id=layer_id,
+            num_layers=config.num_hidden_layers,
+            is_layer_sparse=False,
+            is_previous_layer_sparse=False,
+            is_next_layer_sparse=False,
+        )
+        self.layer_communicator = LayerCommunicator(
+            layer_scatter_modes=self.layer_scatter_modes,
+            input_layernorm=self.input_layernorm,
+            post_attention_layernorm=self.post_attention_layernorm,
+        )
 
     def forward(
         self,
@@ -258,20 +318,26 @@ class LlamaDecoderLayer(nn.Module):
         residual: Optional[torch.Tensor],
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         # Self Attention
-        if residual is None:
-            residual = hidden_states
-            hidden_states = self.input_layernorm(hidden_states)
-        else:
-            hidden_states, residual = self.input_layernorm(hidden_states, residual)
-        hidden_states = self.self_attn(
-            positions=positions,
-            hidden_states=hidden_states,
-            forward_batch=forward_batch,
+        hidden_states, residual = self.layer_communicator.prepare_attn(
+            hidden_states, residual, forward_batch
         )
+        if hidden_states.shape[0] != 0:
+            hidden_states = self.self_attn(
+                positions=positions,
+                hidden_states=hidden_states,
+                forward_batch=forward_batch,
+            )
 
         # Fully Connected
-        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
+        hidden_states, residual = self.layer_communicator.prepare_mlp(
+            hidden_states,
+            residual,
+            forward_batch,
+        )
         hidden_states = self.mlp(hidden_states)
+        hidden_states, residual = self.layer_communicator.postprocess_layer(
+            hidden_states, residual, forward_batch
+        )
         return hidden_states, residual
 
 
@@ -292,6 +358,7 @@ class LlamaModel(nn.Module):
                 config.vocab_size,
                 config.hidden_size,
                 quant_config=quant_config,
+                enable_tp=not is_dp_attention_enabled(),
                 prefix=add_prefix("embed_tokens", prefix),
             )
         else:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4a4e309bf..3cac61228 100644
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -56,11 +56,13 @@ from sglang.srt.utils import (
     fast_topk,
     get_compiler_backend,
     is_cuda,
+    is_npu,
     make_layers,
 )
 from sglang.srt.utils.common import get_current_device_stream_fast
 
 _is_cuda = is_cuda()
+_is_npu = is_npu()
 
 logger = logging.getLogger(__name__)
 
@@ -116,6 +118,7 @@ class Llama4MoE(nn.Module):
             hidden_size=config.hidden_size,
             intermediate_size=intermediate_size_moe,
             layer_id=layer_id,
+            top_k=self.top_k,
             reduce_results=False,
             quant_config=quant_config,
             apply_router_weight_on_input=True,
@@ -328,6 +331,8 @@ class Llama4Attention(nn.Module):
         if self.rotary_emb is not None:
             q_view, k_view = qk.split([self.q_size, self.kv_size], dim=-1)
             q_out_unused, k_out_unused = self.rotary_emb(positions, q_view, k_view)
+            if _is_npu:
+                qk = torch.cat([q_out_unused, k_out_unused], dim=-1)
             del q_view, k_view, q_out_unused, k_out_unused
 
         if self.qk_norm is not None:
diff --git a/python/sglang/srt/models/llama_eagle3.py b/python/sglang/srt/models/llama_eagle3.py
index 49f938a1c..149887754 100644
--- a/python/sglang/srt/models/llama_eagle3.py
+++ b/python/sglang/srt/models/llama_eagle3.py
@@ -27,6 +27,7 @@ from torch import nn
 from transformers import LlamaConfig
 
 from sglang.srt.distributed import get_pp_group
+from sglang.srt.layers.dp_attention import is_dp_attention_enabled
 from sglang.srt.layers.layernorm import RMSNorm
 from sglang.srt.layers.linear import QKVParallelLinear
 from sglang.srt.layers.logits_processor import LogitsProcessor
@@ -38,6 +39,7 @@ from sglang.srt.layers.vocab_parallel_embedding import (
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch, PPProxyTensors
 from sglang.srt.model_loader.weight_utils import default_weight_loader
 from sglang.srt.models.llama import LlamaDecoderLayer, LlamaForCausalLM, LlamaMLP
+from sglang.srt.server_args import get_global_server_args
 
 
 class LlamaDecoderLayer(LlamaDecoderLayer):
@@ -58,6 +60,8 @@ class LlamaDecoderLayer(LlamaDecoderLayer):
             self.self_attn.total_num_kv_heads,
             bias=False,
             quant_config=quant_config,
+            tp_rank=self.self_attn.attn_tp_rank,
+            tp_size=self.self_attn.attn_tp_size,
             prefix=add_prefix("qkv_proj", prefix),
         )
 
@@ -80,23 +84,29 @@ class LlamaDecoderLayer(LlamaDecoderLayer):
         forward_batch: ForwardBatch,
         residual: Optional[torch.Tensor],
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-
         residual = hidden_states
-        embeds = self.input_layernorm(embeds)
+        embeds, _ = self.layer_communicator.prepare_attn(embeds, None, forward_batch)
         hidden_states = self.hidden_norm(hidden_states)
-
-        hidden_states = torch.cat([embeds, hidden_states], dim=-1)
-        # Self Attention
-        hidden_states = self.self_attn(
-            positions=positions,
-            hidden_states=hidden_states,
-            forward_batch=forward_batch,
+        if hidden_states.shape[0] != 0:
+            hidden_states = torch.cat([embeds, hidden_states], dim=-1)
+            # Self Attention
+
+            hidden_states = self.self_attn(
+                positions=positions,
+                hidden_states=hidden_states,
+                forward_batch=forward_batch,
+            )
+        hidden_states, residual = self.layer_communicator.prepare_mlp(
+            hidden_states,
+            residual,
+            forward_batch,
         )
 
-        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
-
         # Fully Connected
         hidden_states = self.mlp(hidden_states)
+        hidden_states, residual = self.layer_communicator.postprocess_layer(
+            hidden_states, residual, forward_batch
+        )
 
         return hidden_states, residual
 
@@ -124,6 +134,7 @@ class LlamaModel(nn.Module):
         self.embed_tokens = VocabParallelEmbedding(
             config.vocab_size,
             config.hidden_size,
+            enable_tp=not is_dp_attention_enabled(),
             prefix=add_prefix("embed_tokens", prefix),
         )
 
@@ -162,10 +173,6 @@ class LlamaModel(nn.Module):
         if hidden_states.shape[-1] != embeds.shape[-1]:
             hidden_states = self.fc(hidden_states)
 
-        # idle batch
-        if hidden_states.shape[0] == 0:
-            return hidden_states, [hidden_states]
-
         residual = None
         hidden_states, residual = self.midlayer(
             positions,
@@ -174,6 +181,9 @@ class LlamaModel(nn.Module):
             forward_batch,
             residual,
         )
+        # idle batch
+        if forward_batch.forward_mode.is_idle():
+            return hidden_states, [hidden_states]
 
         hidden_states_to_logits, hidden_states_to_aux = self.norm(
             hidden_states, residual
@@ -214,6 +224,7 @@ class LlamaForCausalLMEagle3(LlamaForCausalLM):
                 config.draft_vocab_size,
                 config.hidden_size,
                 quant_config=quant_config,
+                use_attn_tp_group=get_global_server_args().enable_dp_lm_head,
                 prefix=add_prefix("lm_head", prefix),
             )
 
diff --git a/python/sglang/srt/models/minicpm3.py b/python/sglang/srt/models/minicpm3.py
index 821dfa98a..a7ba8dc09 100644
--- a/python/sglang/srt/models/minicpm3.py
+++ b/python/sglang/srt/models/minicpm3.py
@@ -239,7 +239,7 @@ class MiniCPM3AttentionMLA(nn.Module):
 
         original_shapes = [q_pe.shape, k_pe.shape]
         q_pe, k_pe = self.rotary_emb(
-            positions, q_pe.reshape(q_pe.shape[0], -1), k_pe.reshape(k_pe.shape[0], -1)
+            positions, q_pe.reshape(-1, q_pe.shape[1]*q_pe.shape[2]), k_pe.reshape(-1, k_pe.shape[1]*k_pe.shape[2])
         )
         q_pe, k_pe = q_pe.view(original_shapes[0]), k_pe.view(original_shapes[1])
         q_input[..., self.kv_lora_rank :] = q_pe
diff --git a/python/sglang/srt/models/minicpmo.py b/python/sglang/srt/models/minicpmo.py
index 0d9d728a2..d60b39ccb 100644
--- a/python/sglang/srt/models/minicpmo.py
+++ b/python/sglang/srt/models/minicpmo.py
@@ -53,7 +53,7 @@ from sglang.srt.model_loader.weight_utils import default_weight_loader
 from sglang.srt.models.idefics2 import Idefics2VisionTransformer
 from sglang.srt.models.minicpmv import MiniCPMBaseModel, Resampler2_5
 from sglang.srt.models.qwen2 import Qwen2ForCausalLM
-from sglang.srt.utils import logger
+from sglang.srt.utils import get_device, logger
 
 try:
     from transformers import LogitsWarper
@@ -1513,7 +1513,7 @@ class MiniCPMO(MiniCPMBaseModel):
                 prefix=prefix,
             )
 
-        return resampler.to(device="cuda", dtype=torch.get_default_dtype())
+        return resampler.to(device=get_device(), dtype=torch.get_default_dtype())
 
     def pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs):
         # Get all special token IDs
diff --git a/python/sglang/srt/models/minicpmv.py b/python/sglang/srt/models/minicpmv.py
index e621676fc..151b8a3c7 100644
--- a/python/sglang/srt/models/minicpmv.py
+++ b/python/sglang/srt/models/minicpmv.py
@@ -56,7 +56,7 @@ from sglang.srt.model_loader.weight_utils import default_weight_loader
 from sglang.srt.models.idefics2 import Idefics2VisionTransformer
 from sglang.srt.models.llama import LlamaConfig, LlamaForCausalLM
 from sglang.srt.models.qwen2 import Qwen2Config, Qwen2ForCausalLM
-from sglang.srt.utils import add_prefix, flatten_nested_list
+from sglang.srt.utils import add_prefix, flatten_nested_list, get_device
 
 RawImageType = Union[Image.Image, torch.Tensor]
 
@@ -708,7 +708,7 @@ class MiniCPMV2_6(MiniCPMBaseModel):
                 prefix=prefix,
             )
 
-        return resampler.to(device="cuda", dtype=torch.get_default_dtype())
+        return resampler.to(device=get_device(), dtype=torch.get_default_dtype())
 
     def get_vision_embedding(
         self,
@@ -866,7 +866,7 @@ class MiniCPMV4_0(MiniCPMBaseModel):
                 prefix=prefix,
             )
 
-        return resampler.to(device="cuda", dtype=torch.get_default_dtype())
+        return resampler.to(device=get_device(), dtype=torch.get_default_dtype())
 
     def get_vision_embedding(
         self,
diff --git a/python/sglang/srt/models/olmoe.py b/python/sglang/srt/models/olmoe.py
index a74a2968d..ab34c8849 100644
--- a/python/sglang/srt/models/olmoe.py
+++ b/python/sglang/srt/models/olmoe.py
@@ -90,6 +90,7 @@ class OlmoeMoE(nn.Module):
             reduce_results=True,
             quant_config=quant_config,
             layer_id=layer_id,
+            top_k=top_k,
             prefix=add_prefix("experts", prefix),
         )
 
diff --git a/python/sglang/srt/models/phi.py b/python/sglang/srt/models/phi.py
index 5679bc987..afafa0c43 100644
--- a/python/sglang/srt/models/phi.py
+++ b/python/sglang/srt/models/phi.py
@@ -22,7 +22,9 @@ from sglang.srt.layers.vocab_parallel_embedding import (
 )
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_loader.weight_utils import default_weight_loader
-from sglang.srt.utils import add_prefix, make_layers
+from sglang.srt.utils import add_prefix, is_npu, make_layers
+
+_is_npu = is_npu()
 
 
 class PhiAttention(nn.Module):
@@ -179,7 +181,7 @@ class PhiModel(nn.Module):
 
         pp_group = get_pp_group()
         pp_size = pp_group.world_size
-        pp_rank = pp_group.rank
+        pp_rank = pp_group.rank_in_group
 
         self.start_layer = pp_rank * config.num_hidden_layers // pp_size
         self.end_layer = (pp_rank + 1) * config.num_hidden_layers // pp_size
@@ -287,7 +289,14 @@ class PhiForCausalLM(nn.Module):
 
             # Handle packed weights
             is_packed = False
-            for packed_name, src_names in self.packed_modules_mapping.items():
+
+            packed_modules_mapping = self.packed_modules_mapping
+            # In npu mode, the packed mapping is updated in loader.py for w8a8
+            if _is_npu:
+                packed_modules_mapping = self.packed_modules_mapping.get(
+                    "model", self.packed_modules_mapping
+                )
+            for packed_name, src_names in packed_modules_mapping.items():
                 if packed_name not in name:
                     continue
 
diff --git a/python/sglang/srt/models/qwen3.py b/python/sglang/srt/models/qwen3.py
index 4d7f666d5..192cfb0ad 100644
--- a/python/sglang/srt/models/qwen3.py
+++ b/python/sglang/srt/models/qwen3.py
@@ -167,21 +167,21 @@ class Qwen3Attention(nn.Module):
         q, k = self.rotary_emb(positions, q, k)
         return q, k, v
 
-    def forward_prepare_npu(self, positions, hidden_states):
+    def forward_prepare_npu(self, positions, hidden_states, forward_batch):
         qkv, _ = self.qkv_proj(hidden_states)
 
-        if self.attn.layer_id == 0:
+        if self.attn.layer_id == forward_batch.token_to_kv_pool.start_layer:
             self.rotary_emb.get_cos_sin_with_position(positions)
         q, k, v = split_qkv_rmsnorm_rope(
             qkv,
             self.rotary_emb.position_sin,
             self.rotary_emb.position_cos,
-            self.q_norm.weight,
-            self.k_norm.weight,
             self.q_size,
             self.kv_size,
             self.head_dim,
-            self.q_norm.variance_epsilon,
+            eps=self.q_norm.variance_epsilon,
+            q_weight=self.q_norm.weight,
+            k_weight=self.k_norm.weight,
             q_bias=getattr(self.q_norm, "bias", None),
             k_bias=getattr(self.k_norm, "bias", None),
         )
@@ -196,7 +196,7 @@ class Qwen3Attention(nn.Module):
         if get_global_server_args().rl_on_policy_target is not None:
             hidden_states = hidden_states.bfloat16()
 
-        if not _is_npu:
+        if not _is_npu or get_global_server_args().is_embedding:
             q, k, v = self.forward_prepare_native(
                 positions=positions,
                 hidden_states=hidden_states,
@@ -205,6 +205,7 @@ class Qwen3Attention(nn.Module):
             q, k, v = self.forward_prepare_npu(
                 positions=positions,
                 hidden_states=hidden_states,
+                forward_batch=forward_batch,
             )
 
         if get_global_server_args().rl_on_policy_target is not None:
@@ -382,6 +383,7 @@ class Qwen3ForCausalLM(nn.Module):
                     config.vocab_size,
                     config.hidden_size,
                     quant_config=quant_config,
+                    use_attn_tp_group=get_global_server_args().enable_dp_lm_head,
                     prefix=add_prefix("lm_head", prefix),
                 )
         else:
diff --git a/python/sglang/srt/models/qwen3_moe.py b/python/sglang/srt/models/qwen3_moe.py
index 9a6d3070e..3d7ac90ee 100644
--- a/python/sglang/srt/models/qwen3_moe.py
+++ b/python/sglang/srt/models/qwen3_moe.py
@@ -542,18 +542,18 @@ class Qwen3MoeAttention(nn.Module):
         forward_batch: ForwardBatch,
     ):
         qkv, _ = self.qkv_proj(hidden_states)
-        if self.attn.layer_id == 0:
+        if self.attn.layer_id == forward_batch.token_to_kv_pool.start_layer:
             self.rotary_emb.get_cos_sin_with_position(positions)
         q, k, v = split_qkv_rmsnorm_rope(
             qkv,
             self.rotary_emb.position_sin,
             self.rotary_emb.position_cos,
-            self.q_norm.weight,
-            self.k_norm.weight,
             self.q_size,
             self.kv_size,
             self.head_dim,
-            self.q_norm.variance_epsilon,
+            eps=self.q_norm.variance_epsilon,
+            q_weight=self.q_norm.weight,
+            k_weight=self.k_norm.weight,
             q_bias=getattr(self.q_norm, "bias", None),
             k_bias=getattr(self.k_norm, "bias", None),
         )
diff --git a/python/sglang/srt/models/qwen3_next.py b/python/sglang/srt/models/qwen3_next.py
index b38a83d57..7986944cf 100644
--- a/python/sglang/srt/models/qwen3_next.py
+++ b/python/sglang/srt/models/qwen3_next.py
@@ -202,6 +202,7 @@ class Qwen3GatedDeltaNet(nn.Module):
         layer_id: int,
         quant_config: Optional[QuantizationConfig] = None,
         alt_stream: Optional[torch.cuda.Stream] = None,
+        prefix: str = "",
     ) -> None:
         super().__init__()
         self.config = config
@@ -229,6 +230,7 @@ class Qwen3GatedDeltaNet(nn.Module):
             quant_config=None,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("conv1d", prefix),
         )
         self.conv1d.weight.data = self.conv1d.weight.data.unsqueeze(1)
         projection_size_qkvz = self.key_dim * 2 + self.value_dim * 2
@@ -241,14 +243,16 @@ class Qwen3GatedDeltaNet(nn.Module):
             quant_config=quant_config,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("in_proj_qkvz", prefix),
         )
         self.in_proj_ba = ColumnParallelLinear(
             input_size=self.hidden_size,
             output_size=projection_size_ba,
             bias=False,
-            quant_config=None,
+            quant_config=quant_config,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("in_proj_ba", prefix),
         )
 
         query_key_settings = (self.key_dim, 0, False)
@@ -297,6 +301,7 @@ class Qwen3GatedDeltaNet(nn.Module):
             reduce_results=False,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("out_proj", prefix),
         )
 
     def fix_query_key_value_ordering(self, mixed_qkvz, mixed_ba):
@@ -452,7 +457,7 @@ class Qwen3GatedDeltaNet(nn.Module):
         z = z.reshape(-1, z.shape[-1])
 
         # Add padding for DP-Attn
-        if is_dp_attention_enabled():
+        if core_attn_out.shape != z.shape:
             core_attn_out_pad = torch.zeros_like(z)
             core_attn_out_pad[: core_attn_out.shape[0], :] = core_attn_out
             core_attn_out = core_attn_out_pad
@@ -478,7 +483,7 @@ class Qwen3HybridLinearDecoderLayer(nn.Module):
         super().__init__()
         self.config = config
         self.linear_attn = Qwen3GatedDeltaNet(
-            config, layer_id, quant_config, alt_stream
+            config, layer_id, quant_config, alt_stream, prefix
         )
 
         # Qwen3Next all layers are sparse and have no nextn now
@@ -501,14 +506,14 @@ class Qwen3HybridLinearDecoderLayer(nn.Module):
                 config=config,
                 quant_config=quant_config,
                 alt_stream=alt_stream,
-                prefix=add_prefix("mlp", prefix),
+                prefix=add_prefix("mlp", prefix.replace(".linear_attn", "")),
             )
         else:
             self.mlp = Qwen2MoeMLP(
                 hidden_size=config.hidden_size,
                 intermediate_size=config.intermediate_size,
                 hidden_act=config.hidden_act,
-                quant_config=quant_config,
+                prefix=add_prefix("mlp", prefix.replace(".linear_attn", "")),
             )
         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
         self.post_attention_layernorm = GemmaRMSNorm(
@@ -617,6 +622,7 @@ class Qwen3HybridAttentionDecoderLayer(nn.Module):
             quant_config=quant_config,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("qkv_proj", prefix),
         )
 
         self.o_proj = RowParallelLinear(
@@ -627,6 +633,7 @@ class Qwen3HybridAttentionDecoderLayer(nn.Module):
             reduce_results=False,
             tp_rank=self.attn_tp_rank,
             tp_size=self.attn_tp_size,
+            prefix=add_prefix("o_proj", prefix),
         )
 
         self.attn = RadixAttention(
@@ -657,7 +664,7 @@ class Qwen3HybridAttentionDecoderLayer(nn.Module):
                 config=config,
                 quant_config=quant_config,
                 alt_stream=alt_stream,
-                prefix=add_prefix("mlp", prefix),
+                prefix=add_prefix("mlp", prefix.replace(".self_attn", "")),
             )
         else:
             self.mlp = Qwen2MoeMLP(
@@ -665,6 +672,7 @@ class Qwen3HybridAttentionDecoderLayer(nn.Module):
                 intermediate_size=config.intermediate_size,
                 hidden_act=config.hidden_act,
                 quant_config=quant_config,
+                prefix=add_prefix("mlp", prefix.replace(".self_attn", "")),
             )
         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
         self.post_attention_layernorm = GemmaRMSNorm(
@@ -800,6 +808,10 @@ class Qwen3NextModel(nn.Module):
 
         def get_layer(idx: int, prefix: str):
             layer_class = ALL_DECODER_LAYER_TYPES[config.layers_block_type[idx]]
+            if config.layers_block_type[idx] == "attention":
+                prefix = add_prefix("self_attn", prefix)
+            else:
+                prefix = add_prefix("linear_attn", prefix)
             return layer_class(
                 config,
                 idx,
diff --git a/python/sglang/srt/models/qwen3_omni_moe.py b/python/sglang/srt/models/qwen3_omni_moe.py
index 045ba2eba..5c80458b6 100644
--- a/python/sglang/srt/models/qwen3_omni_moe.py
+++ b/python/sglang/srt/models/qwen3_omni_moe.py
@@ -42,7 +42,7 @@ from sglang.srt.models.qwen3_vl_moe import (
     Qwen3VLMoeForConditionalGeneration,
     load_fused_expert_weights,
 )
-from sglang.srt.utils import add_prefix, logger
+from sglang.srt.utils import add_prefix, is_npu, logger
 
 
 class Qwen3OmniMoeAudioEncoderLayer(nn.Module):
@@ -278,6 +278,9 @@ class Qwen3OmniMoeAudioEncoder(PreTrainedModel):
         cu_seqlens = torch.tensor(cu_chunk_lens, device=aftercnn_lens.device).cumsum(
             -1, dtype=torch.int32
         )
+        # cu_seqlens must be on cpu because of npu_flash_attention_unpad operator restriction
+        if is_npu():
+            cu_seqlens = cu_seqlens.to("cpu")
 
         for encoder_layer in self.layers:
             layer_outputs = encoder_layer(
diff --git a/python/sglang/srt/models/qwen3_vl.py b/python/sglang/srt/models/qwen3_vl.py
index b7adf8d8c..9338a31a5 100644
--- a/python/sglang/srt/models/qwen3_vl.py
+++ b/python/sglang/srt/models/qwen3_vl.py
@@ -59,7 +59,7 @@ from sglang.srt.models.utils import (
 from sglang.srt.multimodal.mm_utils import run_dp_sharded_mrope_vision_model
 from sglang.srt.multimodal.vit_cuda_graph_runner import ViTCudaGraphRunner
 from sglang.srt.server_args import get_global_server_args
-from sglang.srt.utils import add_prefix, get_bool_env_var, get_int_env_var
+from sglang.srt.utils import add_prefix, get_bool_env_var, get_int_env_var, is_npu
 from sglang.srt.utils.hf_transformers_utils import get_processor
 
 logger = logging.getLogger(__name__)
@@ -487,7 +487,11 @@ class Qwen3VLMoeVisionModel(nn.Module, RotaryPosMixin):
         cu_seqlens = compute_cu_seqlens_from_grid_numpy(grid_thw)
 
         x = x.unsqueeze(1)
-        cu_seqlens = cu_seqlens.to(self.device, non_blocking=True)
+        # cu_seqlens must be on cpu because of npu_flash_attention_unpad operator restriction
+        if is_npu():
+            cu_seqlens = cu_seqlens.to("cpu")
+        else:
+            cu_seqlens = cu_seqlens.to(self.device, non_blocking=True)
 
         deepstack_feature_lists = []
         num_deepstack_captured = 0
diff --git a/python/sglang/srt/models/xverse_moe.py b/python/sglang/srt/models/xverse_moe.py
index 6067acec6..db00b0b5a 100644
--- a/python/sglang/srt/models/xverse_moe.py
+++ b/python/sglang/srt/models/xverse_moe.py
@@ -33,10 +33,12 @@ from sglang.srt.layers.linear import (
     RowParallelLinear,
 )
 from sglang.srt.layers.logits_processor import LogitsProcessor
-from sglang.srt.layers.moe.fused_moe_triton.fused_moe import fused_moe
-from sglang.srt.layers.moe.moe_runner import MoeRunnerConfig
+from sglang.srt.layers.moe import MoeRunnerConfig, get_moe_runner_backend
+from sglang.srt.layers.moe.token_dispatcher.base import CombineInput
+from sglang.srt.layers.moe.token_dispatcher.standard import StandardDispatchOutput
 from sglang.srt.layers.moe.topk import TopK
 from sglang.srt.layers.quantization.base_config import QuantizationConfig
+from sglang.srt.layers.quantization.unquant import UnquantizedFusedMoEMethod
 from sglang.srt.layers.radix_attention import RadixAttention
 from sglang.srt.layers.rotary_embedding import get_rope
 from sglang.srt.layers.vocab_parallel_embedding import (
@@ -99,6 +101,8 @@ class XverseMoE(nn.Module):
     ):
         super().__init__()
         self.config = config
+        self.num_experts = config.num_experts
+        self.hidden_size = config.hidden_size
         self.rank = get_tensor_model_parallel_rank()
         self.tp_size = get_tensor_model_parallel_world_size()
         self.n_routed_experts = config.num_experts
@@ -125,6 +129,14 @@ class XverseMoE(nn.Module):
         self.pack_params()
         self.moe_runner_config = MoeRunnerConfig(inplace=True)
 
+        self.use_triton_kernels = get_moe_runner_backend().is_triton_kernels()
+        if quant_config is None:
+            self.quant_method = UnquantizedFusedMoEMethod(self.use_triton_kernels)
+            self.quant_method.create_moe_runner(self, self.moe_runner_config)
+        else:
+            self.quant_method = quant_config.get_quant_method(self, prefix)
+        assert self.quant_method is not None
+
         self.router = ReplicatedLinear(
             config.hidden_size,
             self.n_routed_experts,
@@ -154,18 +166,18 @@ class XverseMoE(nn.Module):
         for expert in self.experts:
             w1.append(expert.gate_up_proj.weight)
             w2.append(expert.down_proj.weight)
-        self.w1 = torch._utils._flatten_dense_tensors(w1)
-        w1s = torch._utils._unflatten_dense_tensors(self.w1, w1)
+        self.w13_weight = torch._utils._flatten_dense_tensors(w1)
+        w1s = torch._utils._unflatten_dense_tensors(self.w13_weight, w1)
         for data, param in zip(w1s, w1):
             param.data = data
-        self.w1 = self.w1.view(len(w1), *w1s[0].shape)
+        self.w13_weight = self.w13_weight.view(len(w1), *w1s[0].shape)
 
-        self.w2 = torch._utils._flatten_dense_tensors(w2)
-        w2s = torch._utils._unflatten_dense_tensors(self.w2, w2)
+        self.w2_weight = torch._utils._flatten_dense_tensors(w2)
+        w2s = torch._utils._unflatten_dense_tensors(self.w2_weight, w2)
         for data, param in zip(w2s, w2):
             param.data = data
 
-        self.w2 = self.w2.view(len(w2), *w2s[0].shape)
+        self.w2_weight = self.w2_weight.view(len(w2), *w2s[0].shape)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         num_tokens, hidden_dim = hidden_states.shape
@@ -175,14 +187,20 @@ class XverseMoE(nn.Module):
         # router_logits: (num_tokens, n_experts)
         router_logits, _ = self.router(hidden_states)
         topk_output = self.topk(hidden_states, router_logits)
-        final_hidden_states = fused_moe(
-            hidden_states,
-            self.w1,
-            self.w2,
-            topk_output,
-            self.moe_runner_config,
+        dispatch_output = StandardDispatchOutput(
+            hidden_states=hidden_states,
+            hidden_states_scale=None,
+            topk_output=topk_output,
+        )
+        final_hidden_states = self.quant_method.apply(
+            layer=self, dispatch_output=dispatch_output
         )
 
+        if isinstance(
+            final_hidden_states, CombineInput
+        ):  # quant_method use `UnquantizedFusedMoEMethod`
+            final_hidden_states = final_hidden_states.hidden_states
+
         if self.config.num_shared_experts is not None:
             final_hidden_states = final_hidden_states + shared_output
         final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
diff --git a/python/sglang/srt/multimodal/processors/base_processor.py b/python/sglang/srt/multimodal/processors/base_processor.py
index 685c04cbe..0d3a14e29 100644
--- a/python/sglang/srt/multimodal/processors/base_processor.py
+++ b/python/sglang/srt/multimodal/processors/base_processor.py
@@ -320,6 +320,8 @@ class BaseMultimodalProcessor(ABC):
             elif processor.__class__.__name__ not in {
                 "Qwen2_5_VLProcessor",
                 "Qwen3VLProcessor",
+                "Glm4vProcessor",
+                "Qwen2VLProcessor",
             }:
                 # Note: for qwen-vl, processor has some reshape issue because of dims restriction on Ascend.
                 kwargs["device"] = "npu"
diff --git a/python/sglang/srt/parser/code_completion_parser.py b/python/sglang/srt/parser/code_completion_parser.py
index 0067ac471..dfb091a8e 100644
--- a/python/sglang/srt/parser/code_completion_parser.py
+++ b/python/sglang/srt/parser/code_completion_parser.py
@@ -16,7 +16,7 @@
 
 import dataclasses
 import logging
-from enum import auto
+from enum import Enum, auto
 
 from sglang.srt.entrypoints.openai.protocol import CompletionRequest
 
@@ -24,7 +24,7 @@ logger = logging.getLogger(__name__)
 completion_template_name = None
 
 
-class FimPosition:
+class FimPosition(Enum):
     """Position of fim middle token."""
 
     MIDDLE = auto()
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index b0f7ba891..f0196e5f2 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -439,6 +439,11 @@ class ServerArgs:
     speculative_ngram_match_type: Literal["BFS", "PROB"] = "BFS"
     speculative_ngram_branch_length: int = 18
     speculative_ngram_capacity: int = 10 * 1000 * 1000
+    # For suffix decoding only
+    speculative_suffix_max_tree_depth: int = 24
+    speculative_suffix_max_cached_requests: int = 10000
+    speculative_suffix_max_spec_factor: float = 1.0
+    speculative_suffix_min_token_prob: float = 0.1
 
     # For Multi-Layer MTP
     # FIXME: rename -> enable_multi_layer_mtp
@@ -455,6 +460,7 @@ class ServerArgs:
     ep_dispatch_algorithm: Optional[Literal["static", "dynamic", "fake"]] = None
     init_expert_location: str = "trivial"
     enable_eplb: bool = False
+    enable_async_eplb: bool = False
     eplb_algorithm: str = "auto"
     eplb_rebalance_num_iterations: int = 1000
     eplb_rebalance_layers_per_chunk: Optional[int] = None
@@ -1086,6 +1092,10 @@ class ServerArgs:
                     )
 
                     print_nsa_bool_env_vars()
+                if self.enable_nsa_prefill_context_parallel:
+                    assert (
+                        self.disaggregation_mode != "decode"
+                    ), "CP is only supported for prefill when PD disaggregation, please remove --enable-nsa-prefill-context-parallel."
 
             else:
                 # DeepSeek V3/R1/V3.1
@@ -1215,9 +1225,10 @@ class ServerArgs:
                 "fa3",
                 "aiter",
                 "triton",
+                "ascend",
                 "trtllm_mha",
                 "intel_xpu",
-            }, f"fa3, aiter, triton, trtllm_mha or intel_xpu is required for Llama4 model but got {self.attention_backend}"
+            }, f"fa3, aiter, triton, ascend, trtllm_mha or intel_xpu is required for Llama4 model but got {self.attention_backend}"
             if is_sm100_supported() and self.moe_runner_backend == "auto":
                 if self.quantization in {"fp8", "modelopt_fp8"}:
                     self.moe_runner_backend = "flashinfer_trtllm"
@@ -2081,6 +2092,76 @@ class ServerArgs:
                     "Currently ngram speculative decoding does not support dp attention."
                 )
 
+        if self.speculative_algorithm == "SUFFIX":
+            # Validate Arctic Inference availability
+            from sglang.srt.utils.common import is_arctic_inference_available
+
+            if not is_arctic_inference_available():
+                raise ImportError(
+                    "Arctic Inference is required for suffix decoding. "
+                    "Install via: pip install arctic-inference==0.1.1"
+                )
+
+            if not self.device.startswith("cuda") and not is_npu():
+                raise ValueError(
+                    "Suffix decoding only supports CUDA device or NPU device."
+                )
+
+            if self.max_running_requests is None:
+                self.max_running_requests = 48
+                logger.warning(
+                    "Max running requests is reset to 48 for suffix decoding. You can override this by explicitly setting --max-running-requests."
+                )
+
+            self.disable_overlap_schedule = True
+            self.enable_mixed_chunk = False
+            if self.speculative_num_draft_tokens is None:
+                self.speculative_num_draft_tokens = (
+                    self.speculative_suffix_max_tree_depth
+                )
+                logger.warning(
+                    f"Defaulted speculative_num_draft_tokens to {self.speculative_suffix_max_tree_depth} for suffix decoding."
+                )
+            logger.warning(
+                "The overlap scheduler and mixed chunked prefill are disabled because of "
+                "using suffix decoding."
+            )
+
+            # Validate parameter ranges
+            if self.speculative_suffix_max_tree_depth < 1:
+                raise ValueError(
+                    f"speculative_suffix_max_tree_depth={self.speculative_suffix_max_tree_depth} must be >= 1"
+                )
+
+            if self.speculative_suffix_max_cached_requests < -1:
+                raise ValueError(
+                    f"speculative_suffix_max_cached_requests={self.speculative_suffix_max_cached_requests} must be >= -1 (use -1 for unlimited)"
+                )
+
+            if self.speculative_suffix_max_spec_factor < 0:
+                raise ValueError(
+                    f"speculative_suffix_max_spec_factor={self.speculative_suffix_max_spec_factor} must be >= 0"
+                )
+
+            if not 0 <= self.speculative_suffix_min_token_prob <= 1:
+                raise ValueError(
+                    f"speculative_suffix_min_token_prob={self.speculative_suffix_min_token_prob} must be in [0, 1]"
+                )
+
+            if self.enable_dp_attention:
+                # TODO: support dp attention for suffix decoding
+                raise ValueError(
+                    "Currently suffix decoding does not support dp attention."
+                )
+
+            logger.info(
+                f"Suffix decoding configured with: "
+                f"max_tree_depth={self.speculative_suffix_max_tree_depth}, "
+                f"max_cached_requests={self.speculative_suffix_max_cached_requests}, "
+                f"max_spec_factor={self.speculative_suffix_max_spec_factor}, "
+                f"min_token_prob={self.speculative_suffix_min_token_prob}"
+            )
+
     def _handle_load_format(self):
         if (
             self.load_format == "auto" or self.load_format == "gguf"
@@ -3328,7 +3409,7 @@ class ServerArgs:
         parser.add_argument(
             "--speculative-algorithm",
             type=str,
-            choices=["EAGLE", "EAGLE3", "NEXTN", "STANDALONE", "NGRAM"],
+            choices=["EAGLE", "EAGLE3", "NEXTN", "STANDALONE", "NGRAM", "SUFFIX"],
             help="Speculative algorithm.",
         )
         parser.add_argument(
@@ -3469,7 +3550,39 @@ class ServerArgs:
             default=ServerArgs.speculative_ngram_capacity,
             help="The cache capacity for ngram speculative decoding.",
         )
-
+        # Suffix decoding
+        parser.add_argument(
+            "--speculative-suffix-max-tree-depth",
+            type=int,
+            default=ServerArgs.speculative_suffix_max_tree_depth,
+            help="The maximum depth of the suffix decoding global and prompt trees. "
+            "The tree depth limits the sum of the prefix match and speculation lengths.",
+        )
+        parser.add_argument(
+            "--speculative-suffix-max-cached-requests",
+            type=int,
+            default=ServerArgs.speculative_suffix_max_cached_requests,
+            help="The maximum number of requests to cache in the global suffix tree. "
+            "If exceeded, will trigger eviction in FIFO order. Set to -1 for unlimited "
+            "cache size, or 0 to disable the global suffix tree (past responses are not "
+            "cached, but prompt trees are still used).",
+        )
+        parser.add_argument(
+            "--speculative-suffix-max-spec-factor",
+            type=float,
+            default=ServerArgs.speculative_suffix_max_spec_factor,
+            help="The maximum spec factor for suffix decoding. The spec factor controls "
+            "speculation lengths based on the prefix match length: max_spec_tokens = "
+            "max_spec_factor * prefix_match_length.",
+        )
+        parser.add_argument(
+            "--speculative-suffix-min-token-prob",
+            type=float,
+            default=ServerArgs.speculative_suffix_min_token_prob,
+            help="The minimum token probability for suffix decoding. Will only speculate "
+            "tokens with estimated probability (based on frequency counts) greater than "
+            "or equal to this value.",
+        )
         # Speculative decoding (MTP)
         parser.add_argument(
             "--enable-mtp",
@@ -3542,6 +3655,11 @@ class ServerArgs:
             action="store_true",
             help="Enable EPLB algorithm",
         )
+        parser.add_argument(
+            "--enable-async-eplb",
+            action="store_true",
+            help="Enable Async EPLB",
+        )
         parser.add_argument(
             "--eplb-algorithm",
             type=str,
diff --git a/python/sglang/srt/speculative/cpp_ngram/ngram.cpp b/python/sglang/srt/speculative/cpp_ngram/ngram.cpp
index e7f0297e2..812351b24 100644
--- a/python/sglang/srt/speculative/cpp_ngram/ngram.cpp
+++ b/python/sglang/srt/speculative/cpp_ngram/ngram.cpp
@@ -19,7 +19,7 @@ struct Node {
   std::unordered_map<int32_t, int32_t> next;
 };
 
-Ngram::Result fillResult(int last_token, int draft_token_num, std::vector<Node>& tree, int root) {
+Ngram::Result fillResult(int last_token, int draft_token_num, const std::vector<Node>& tree, int root) {
   Ngram::Result info;
   std::vector<int32_t> prevs;
   info.token.reserve(draft_token_num);
@@ -255,42 +255,55 @@ void Ngram::asyncInsert(std::vector<std::vector<int32_t>>&& tokens) {
 }
 
 Ngram::Result Ngram::matchBFS(const std::vector<int32_t>& tokens, size_t batch_size) const {
-  std::vector<std::pair<TrieNode*, int32_t>> nodes = match(tokens, batch_size);
+  auto nodes = match(tokens, batch_size);
+
+  if (nodes.empty()) {
+    return fillResult(tokens.back(), 1, {{}}, 0);
+  }
 
   double bfs_breadth_scale = double(param_.max_bfs_breadth - param_.min_bfs_breadth) /
                              (param_.max_match_window_size - param_.min_match_window_size + 1);
 
-  auto draft_token_num = param_.get_draft_token_num(batch_size);
-  std::vector<Node> tree(draft_token_num + 1);
+  std::vector<Node> tree;
+  tree.reserve(64);
+  tree.push_back(Node{});
+
   int root = 0;
   int cursor = 1;
 
+  const int max_actual_draft = param_.get_draft_token_num(batch_size) + 1;
+
   for (auto [node, depth] : nodes) {
-    std::queue<std::tuple<int32_t, double, const TrieNode*>> queue;  // parent, bfs_breadth, node
-    queue.push({root, (param_.max_match_window_size - depth) * bfs_breadth_scale + param_.min_bfs_breadth, node});
-    while (queue.size() && cursor <= draft_token_num) {
-      auto front = queue.front();
+    std::queue<std::tuple<int32_t, double, const TrieNode*>> queue;
+    double init_breadth = (param_.max_match_window_size - depth) * bfs_breadth_scale + param_.min_bfs_breadth;
+    queue.push({root, init_breadth, node});
+
+    while (!queue.empty() && cursor < max_actual_draft) {
+      auto [parent, cur_breadth, trie_node] = queue.front();
       queue.pop();
 
-      auto parent = std::get<0>(front);
-      auto cur_breadth = std::get<1>(front);
-      auto iter = std::get<2>(front)->lru.begin();
+      int breadth = std::max(1, static_cast<int>(cur_breadth));
+      auto iter = trie_node->lru.begin();
+
+      for (int i = 0; i < breadth && iter != trie_node->lru.end() && cursor < max_actual_draft; ++i, ++iter) {
+        int32_t token = (*iter)->token;
 
-      auto breadth = std::max(1, int32_t(cur_breadth));
-      for (int i = 0; i < breadth && iter != std::get<2>(front)->lru.end() && cursor <= draft_token_num; ++i, ++iter) {
-        auto token = (*iter)->token;
-        auto pos = -1;
-        if (auto tit = tree[parent].next.find(token); tit != tree[parent].next.end()) {
-          pos = tit->second;
+        auto& next_map = tree[parent].next;
+        int pos;
+        if (auto it = next_map.find(token); it != next_map.end()) {
+          pos = it->second;
         } else {
-          pos = tree[parent].next.insert(std::make_pair(token, cursor++)).first->second;
+          pos = cursor++;
+          next_map[token] = pos;
+          tree.push_back(Node{});
         }
+
         queue.emplace(pos, cur_breadth - bfs_breadth_scale, *iter);
       }
     }
   }
 
-  return fillResult(tokens.back(), draft_token_num + 1, tree, root);
+  return fillResult(tokens.back(), cursor, tree, root);
 }
 
 Ngram::Result Ngram::matchProb(const std::vector<int32_t>& tokens, size_t batch_size) const {
diff --git a/python/sglang/srt/speculative/ngram_info.py b/python/sglang/srt/speculative/ngram_info.py
index 637f51c97..2aa6218bd 100644
--- a/python/sglang/srt/speculative/ngram_info.py
+++ b/python/sglang/srt/speculative/ngram_info.py
@@ -33,7 +33,7 @@ from sglang.srt.speculative.spec_utils import (
     get_src_tgt_cache_loc,
     get_target_cache_loc,
 )
-from sglang.srt.utils import is_cuda, is_hip, next_power_of_2
+from sglang.srt.utils import is_cuda, is_hip, is_npu, next_power_of_2
 
 if is_cuda():
     from sgl_kernel import (
@@ -42,6 +42,8 @@ if is_cuda():
         tree_speculative_sampling_target_only,
         verify_tree_greedy,
     )
+elif is_npu():
+    from sgl_kernel_npu.sample.verify_tree_greedy import verify_tree_greedy
 elif is_hip():
     from sgl_kernel import verify_tree_greedy
 
@@ -252,9 +254,17 @@ class NgramVerifyInput(SpecInput):
             batch.token_to_kv_pool_allocator.free(to_free_slots)
 
             # Copy the kv cache
-            batch.token_to_kv_pool_allocator.get_kvcache().move_kv_cache(
-                tgt_cache_loc, src_cache_loc
-            )
+            if is_npu:
+                kvcache = batch.token_to_kv_pool_allocator.get_kvcache()
+
+                if getattr(kvcache, "enable_kv_cache_copy", False):
+                    kvcache.move_kv_cache(tgt_cache_loc, src_cache_loc)
+            else:
+                batch.token_to_kv_pool_allocator.get_kvcache().move_kv_cache(
+                    tgt_cache_loc, src_cache_loc
+                )
+
+
             batch.out_cache_loc = tgt_cache_loc
 
         accept_length_list = accept_length_cpu.tolist()
diff --git a/python/sglang/srt/speculative/ngram_worker.py b/python/sglang/srt/speculative/ngram_worker.py
index 4296c6924..a8109ce6b 100644
--- a/python/sglang/srt/speculative/ngram_worker.py
+++ b/python/sglang/srt/speculative/ngram_worker.py
@@ -3,7 +3,6 @@ from typing import List, Optional
 
 import numpy as np
 import torch
-from sgl_kernel.speculative import reconstruct_indices_from_tree_mask
 
 from sglang.srt.environ import envs
 from sglang.srt.layers.logits_processor import LogitsProcessorOutput
@@ -16,11 +15,17 @@ from sglang.srt.server_args import ServerArgs
 from sglang.srt.speculative.cpp_ngram.ngram_cache import NgramCache
 from sglang.srt.speculative.ngram_info import NgramVerifyInput
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
+from sglang.srt.utils import is_npu
 
-logger = logging.getLogger(__name__)
+if not is_npu:
+    from sgl_kernel.speculative import reconstruct_indices_from_tree_mask
+
+    USE_FULL_MASK = True
+else:
+    USE_FULL_MASK = False
 
 
-USE_FULL_MASK = True
+logger = logging.getLogger(__name__)
 
 
 class NGRAMWorker:
@@ -158,16 +163,26 @@ class NGRAMWorker:
         tree_mask.copy_(torch.from_numpy(mask), non_blocking=True)
         draft_tokens.copy_(torch.from_numpy(req_drafts), non_blocking=True)
 
-        reconstruct_indices_from_tree_mask(
-            tree_mask,
-            batch.seq_lens,
-            positions,  # mutable
-            retrive_index,  # mutable
-            retrive_next_token,  # mutable
-            retrive_next_sibling,  # mutable
-            bs,
-            self.draft_token_num,
-        )
+        if is_npu:
+            # npu just support linear speculative decoding now
+            self._prepare_linear_speculative_indices(
+                batch,
+                positions,
+                retrive_index,
+                retrive_next_token,
+                retrive_next_sibling,
+            )
+        else:
+            reconstruct_indices_from_tree_mask(
+                tree_mask,
+                batch.seq_lens,
+                positions,  # mutable
+                retrive_index,  # mutable
+                retrive_next_token,  # mutable
+                retrive_next_sibling,  # mutable
+                bs,
+                self.draft_token_num,
+            )
 
         # NOTE: QLEN_MASK is faster than FULL_MASK, but requires corresponding changes in flashinfer.
         # Testing shows about 8% performance improvement (the effect is roughly proportional to batch size).
@@ -198,6 +213,30 @@ class NGRAMWorker:
         )
         batch.spec_info.prepare_for_verify(batch, self.page_size)
 
+    def _prepare_linear_speculative_indices(
+        self, batch, positions, retrive_index, retrive_next_token, retrive_next_sibling
+    ):
+        B = batch.batch_size()
+        K = self.draft_token_num
+        device = positions.device
+
+        retrive_index.fill_(-1)
+        retrive_next_token.fill_(-1)
+        retrive_next_sibling.fill_(-1)
+
+        bases = batch.seq_lens_cpu.to(device, non_blocking=True).unsqueeze(1)  # [B, 1]
+        offsets = torch.arange(K, device=device, dtype=positions.dtype)  # [K]
+        positions[:] = (bases + offsets).view(-1)
+
+        global_idx = torch.arange(B * K, device=device, dtype=retrive_index.dtype).view(
+            B, K
+        )
+        retrive_index[:, :K] = global_idx
+
+        if K > 1:
+            next_vals = global_idx[:, 1:]  # [B, K-1]
+            retrive_next_token[:, : K - 1] = next_vals
+
     def add_logprob_values(
         self,
         batch: ScheduleBatch,
diff --git a/python/sglang/srt/speculative/spec_info.py b/python/sglang/srt/speculative/spec_info.py
index 2b0d48aaa..7a06b14cd 100644
--- a/python/sglang/srt/speculative/spec_info.py
+++ b/python/sglang/srt/speculative/spec_info.py
@@ -173,6 +173,9 @@ class SpeculativeAlgorithm(metaclass=_SpeculativeAlgorithmMeta):
     def is_ngram(self) -> bool:
         return self._has_flag("NGRAM")
 
+    def is_suffix(self) -> bool:
+        return self._has_flag("SUFFIX")
+
     def create_draft_worker(self, **factory_kwargs: Any) -> Any:
         if self._draft_worker_factory is None:
             return None
@@ -189,6 +192,7 @@ _FLAG_MARKERS: Dict[str, Callable[[Union[SpeculativeAlgorithm, str]], None]] = {
         "STANDALONE", algorithm
     ),
     "NGRAM": lambda algorithm: SpeculativeAlgorithm._add_flag("NGRAM", algorithm),
+    "SUFFIX": lambda algorithm: SpeculativeAlgorithm._add_flag("SUFFIX", algorithm),
 }
 
 
@@ -284,6 +288,10 @@ def _create_ngram_worker(**kwargs: Any) -> Any:
 
     return NGRAMWorker(**kwargs)
 
+def _create_suffix_worker(**kwargs: Any) -> Any:
+    from sglang.srt.speculative.suffix_worker import SuffixWorker
+
+    return SuffixWorker(**kwargs)
 
 # Register built-in algorithms.
 # Third-party integrations should import `SpeculativeAlgorithm` and either
@@ -316,6 +324,11 @@ register_speculative_algorithm(
     flags=("NGRAM",),
 )
 
+register_speculative_algorithm(
+    "SUFFIX",
+    worker_cls=_create_suffix_worker,
+    flags=("SUFFIX",),
+)
 
 class SpecInputType(IntEnum):
     # NOTE: introduce this to distinguish the SpecInput types of multiple algorithms when asserting in attention backends.
@@ -323,7 +336,7 @@ class SpecInputType(IntEnum):
     EAGLE_DRAFT = auto()
     EAGLE_VERIFY = auto()
     NGRAM_VERIFY = auto()
-
+    SUFFIX_VERIFY = auto()
 
 class SpecInput(ABC):
     def __init__(self, spec_input_type: SpecInputType):
@@ -338,6 +351,7 @@ class SpecInput(ABC):
         return self.spec_input_type in {
             SpecInputType.EAGLE_VERIFY,
             SpecInputType.NGRAM_VERIFY,
+            SpecInputType.SUFFIX_VERIFY,
         }
 
     @abstractmethod
diff --git a/python/sglang/srt/speculative/suffix_cache_adapter.py b/python/sglang/srt/speculative/suffix_cache_adapter.py
new file mode 100644
index 000000000..a2641eeaf
--- /dev/null
+++ b/python/sglang/srt/speculative/suffix_cache_adapter.py
@@ -0,0 +1,340 @@
+"""
+Cache adapter that wraps the suffix decoding backend cache to provide
+the same interface as NgramCache.
+
+This allows NGRAMWorker to use suffix decoding without modification.
+"""
+
+import logging
+import os
+from collections import deque
+from typing import List, Optional, Tuple
+
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+class SuffixCacheAdapter:
+    """
+    Adapter that wraps SuffixDecodingCache to match NgramCache interface.
+
+    NGRAMWorker expects:
+    - batch_get(batch_tokens: List[List[int]]) -> Tuple[np.ndarray, np.ndarray]
+      Returns (draft_tokens, tree_mask) as flat numpy arrays
+    - batch_put(batch_tokens: List[List[int]]) -> None
+      Updates cache with verified tokens
+    - synchronize() -> None
+      No-op for suffix cache
+    - reset() -> None
+      Clears all cached data
+    """
+
+    def __init__(
+        self,
+        draft_token_num: int,
+        max_batch_size: int,
+        max_tree_depth: int = 24,
+        max_cached_requests: int = 10000,
+        max_spec_factor: float = 1.0,
+        min_token_prob: float = 0.1,
+    ):
+        """
+        Args:
+            draft_token_num: Fixed number of draft tokens (for padding)
+            max_tree_depth: Maximum depth for suffix tree
+            max_cached_requests: Maximum number of cached requests
+            max_spec_factor: Maximum speculation factor
+            min_token_prob: Minimum token probability threshold
+        """
+        # Lazy import to avoid error when Suffix Decoding is not used
+        from arctic_inference.suffix_decoding import SuffixDecodingCache
+
+        self.suffix_cache = SuffixDecodingCache(
+            max_tree_depth=max_tree_depth,
+            max_cached_requests=max_cached_requests,
+        )
+        self.draft_token_num = draft_token_num
+        self.max_batch_size = max_batch_size
+        self.max_tree_depth = max_tree_depth
+        self.max_spec_factor = max_spec_factor
+        self.min_token_prob = min_token_prob
+
+        # Debug toggles (set env e.g. SUFFIX_DEBUG_TREE=1 to dump first batch)
+        self.debug_tree_dump_remaining = int(os.environ.get("SUFFIX_DEBUG_TREE", "0"))
+
+        # Track state by SGlang request ID (stable identifier)
+        # Map: sglang_req_id → (arctic_req_id, last_length)
+        self.req_state = {}
+
+        # Preallocate buffers to avoid per-step allocations
+        self.max_total_drafts = self.max_batch_size * self.draft_token_num
+        self.draft_buffer = np.empty((self.max_total_drafts,), dtype=np.int64)
+        self.mask_buffer = np.empty(
+            (self.max_batch_size, self.draft_token_num, self.draft_token_num),
+            dtype=bool,
+        )
+
+    def _cleanup_inactive_requests(self, active_req_ids: set[str]):
+        """Stop backend requests that are no longer active in SGlang."""
+        inactive_req_ids = [
+            rid for rid in self.req_state.keys() if rid not in active_req_ids
+        ]
+        for rid in inactive_req_ids:
+            cache_req_id, _ = self.req_state.pop(rid)
+            if cache_req_id in getattr(self.suffix_cache, "active_requests", set()):
+                self.suffix_cache.stop_request(cache_req_id)
+
+    def _get_or_create_cache_req_id(
+        self, sglang_req_id: str, prompt: List[int], tokens: List[int]
+    ) -> tuple:
+        """Get or create a backend request ID for the given SGlang request.
+
+        Args:
+            sglang_req_id: Stable request ID from SGlang
+            prompt: Prompt tokens only (no generated tokens)
+            tokens: Full token sequence (prompt + outputs)
+
+        Returns: (arctic_req_id, last_length)
+        """
+        if sglang_req_id not in self.req_state:
+            # Use SGlang request ID directly as backend request ID
+            cache_req_id = sglang_req_id
+
+            # Initialize the request in suffix cache with ONLY the prompt
+            self.suffix_cache.start_request(cache_req_id, prompt)
+
+            # Track: [arctic_req_id, last_length]
+            # IMPORTANT: Set last_length to prompt length since the backend already has the prompt
+            self.req_state[sglang_req_id] = [cache_req_id, len(prompt)]
+
+        cache_req_id, last_length = self.req_state[sglang_req_id]
+        return cache_req_id, last_length
+
+    def batch_get(
+        self,
+        batch_req_ids: List[str],
+        batch_prompts: List[List[int]],
+        batch_tokens: List[List[int]],
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """
+        Get draft tokens for a batch of token sequences.
+
+        This is called BEFORE verification with the current state.
+        We speculate based on the current tokens.
+
+        Args:
+            batch_req_ids: List of SGlang request IDs (stable)
+            batch_prompts: List of prompt tokens (no generated tokens)
+            batch_tokens: List of token sequences (prompt + output tokens)
+
+        Returns:
+            Tuple of:
+            - draft_tokens: np.ndarray of shape (batch_size * draft_token_num,)
+            - tree_mask: np.ndarray of shape (batch_size * draft_token_num * draft_token_num,)
+        """
+        batch_size = len(batch_req_ids)
+        if batch_size == 0:
+            return np.empty((0,), dtype=np.int64), np.empty((0,), dtype=bool)
+
+        if batch_size > self.max_batch_size:
+            raise ValueError(
+                f"Batch size {batch_size} exceeds configured max_batch_size={self.max_batch_size}"
+            )
+
+        total_draft_tokens = batch_size * self.draft_token_num
+        draft_view = self.draft_buffer[:total_draft_tokens]
+        mask_view = self.mask_buffer[:batch_size]
+        mask_view.fill(False)
+
+        active_req_ids = set(batch_req_ids)
+        self._cleanup_inactive_requests(active_req_ids)
+
+        for idx, (sglang_req_id, prompt, tokens) in enumerate(
+            zip(batch_req_ids, batch_prompts, batch_tokens)
+        ):
+            cache_req_id, last_length = self._get_or_create_cache_req_id(
+                sglang_req_id, prompt, tokens
+            )
+
+            # Ensure cache includes the latest verified tokens before speculation.
+            current_length = len(tokens)
+            if current_length > last_length:
+                new_tokens = tokens[last_length:current_length]
+                if cache_req_id in self.suffix_cache.active_requests:
+                    self.suffix_cache.add_active_response(cache_req_id, new_tokens)
+                    self.req_state[sglang_req_id][1] = current_length
+                    last_length = current_length
+                else:
+                    logger.warning(
+                        f"[BATCH_GET {idx}] Suffix cache req {cache_req_id} not active when updating!"
+                    )
+
+            # Extract pattern from end of tokens (up to max_tree_depth)
+            pattern_start = max(0, len(tokens) - self.max_tree_depth)
+            pattern = tokens[pattern_start:]
+
+            # Speculate using suffix cache
+            draft = self.suffix_cache.speculate(
+                cache_req_id,
+                pattern,
+                max_spec_tokens=self.draft_token_num,
+                max_spec_factor=self.max_spec_factor,
+                min_token_prob=self.min_token_prob,
+            )
+
+            # Convert to fixed-size arrays
+            draft_ids = list(draft.token_ids)
+            draft_parents = list(draft.parents)
+            draft_ids, draft_parents = self._reorder_tree_bfs(draft_ids, draft_parents)
+
+            context_token = tokens[-1] if tokens else 0
+            draft_ids, draft_parents = self._inject_root_node(
+                draft_ids, draft_parents, context_token
+            )
+
+            # Pad or truncate to match draft_token_num (includes root node at index 0)
+            original_draft_len = len(draft_ids)
+            if original_draft_len < self.draft_token_num:
+                pad_len = self.draft_token_num - original_draft_len
+                draft_ids.extend([0] * pad_len)
+                draft_parents.extend([0] * pad_len)
+            elif original_draft_len > self.draft_token_num:
+                draft_ids = draft_ids[: self.draft_token_num]
+                draft_parents = draft_parents[: self.draft_token_num]
+                original_draft_len = self.draft_token_num
+
+            start = idx * self.draft_token_num
+            end = start + self.draft_token_num
+            draft_view[start:end] = draft_ids
+
+            # Build tree mask from parent structure
+            # Token i can attend to token j if j is an ancestor of i
+            mask = mask_view[idx]
+            if original_draft_len > 0:
+                for i in range(original_draft_len):
+                    mask[i, i] = True  # Self-attention
+                    parent_idx = draft_parents[i]
+                    while parent_idx >= 0 and parent_idx < self.draft_token_num:
+                        mask[i, parent_idx] = True
+                        parent_idx = draft_parents[parent_idx]
+
+            if self.debug_tree_dump_remaining > 0 and original_draft_len > 0:
+                logger.warning(
+                    "[SUFFIX DEBUG] req=%s, original_draft_len=%d, masked_len=%d, draft_ids=%s",
+                    sglang_req_id,
+                    original_draft_len,
+                    len(draft_ids),
+                    draft_ids,
+                )
+                logger.warning(
+                    "[SUFFIX DEBUG] mask=\n%s",
+                    mask.astype(int),
+                )
+                self.debug_tree_dump_remaining -= 1
+
+        tree_mask = mask_view.reshape(-1)[: total_draft_tokens * self.draft_token_num]
+
+        return draft_view, tree_mask
+
+    def batch_put(self, batch_req_ids: List[str], batch_tokens: List[List[int]]):
+        """
+        No-op: cache updates now happen inside batch_get before speculation.
+        Kept for interface compatibility with NGRAMWorker.
+        """
+        _ = batch_tokens  # Intentional placeholder to satisfy interface.
+        for idx, sglang_req_id in enumerate(batch_req_ids):
+            if sglang_req_id not in self.req_state:
+                logger.error(
+                    f"[BATCH_PUT {idx}] Called for unknown request {sglang_req_id}! "
+                    f"This should not happen - batch_get must be called first."
+                )
+
+    def synchronize(self):
+        """No-op for suffix cache (no async operations)."""
+        pass
+
+    def reset(self):
+        """Clear all cached data."""
+        # Stop all active requests
+        for cache_req_id in list(self.suffix_cache.active_requests):
+            self.suffix_cache.stop_request(cache_req_id)
+        # Clear tracking
+        self.req_state.clear()
+        logger.info("[SUFFIX ADAPTER] Cache reset")
+
+    def _reorder_tree_bfs(
+        self, token_ids: List[int], parents: List[Optional[int]]
+    ) -> Tuple[List[int], List[int]]:
+        """
+        Reorder nodes so parents always precede their descendants.
+
+        reconstruct_indices_from_tree_mask assumes this layout; the backend emits
+        score-ordered nodes, so we re-topologize the list before building masks.
+        """
+        n = len(token_ids)
+        if n <= 1:
+            return token_ids, parents
+
+        children: List[List[int]] = [[] for _ in range(n)]
+        roots: List[int] = []
+        for idx, parent in enumerate(parents):
+            if parent is None or parent < 0 or parent >= n:
+                roots.append(idx)
+            else:
+                children[parent].append(idx)
+
+        if not roots:
+            roots = [0]
+
+        order: List[int] = []
+        visited = [False] * n
+        for root in roots:
+            if visited[root]:
+                continue
+            queue = deque([root])
+            while queue:
+                node = queue.popleft()
+                if visited[node]:
+                    continue
+                visited[node] = True
+                order.append(node)
+                for child in children[node]:
+                    if not visited[child]:
+                        queue.append(child)
+
+        # Append any detached nodes (should not happen, but keep deterministic order).
+        for idx in range(n):
+            if not visited[idx]:
+                order.append(idx)
+
+        if order == list(range(n)):
+            return token_ids, parents
+
+        remap = {old_idx: new_idx for new_idx, old_idx in enumerate(order)}
+        reordered_ids = [token_ids[old_idx] for old_idx in order]
+        reordered_parents: List[int] = []
+        for old_idx in order:
+            parent = parents[old_idx]
+            if parent is None or parent < 0:
+                reordered_parents.append(-1)
+            else:
+                reordered_parents.append(remap.get(parent, -1))
+
+        return reordered_ids, reordered_parents
+
+    def _inject_root_node(
+        self, token_ids: List[int], parents: List[int], context_token: int
+    ) -> Tuple[List[int], List[int]]:
+        """
+        Insert the latest verified token as index 0 so the layout matches NGRAM.
+        """
+        rooted_ids = [context_token]
+        rooted_parents = [-1]
+        for parent_idx in parents:
+            if parent_idx < 0:
+                rooted_parents.append(0)
+            else:
+                rooted_parents.append(parent_idx + 1)
+        rooted_ids.extend(token_ids)
+        return rooted_ids, rooted_parents
diff --git a/python/sglang/srt/speculative/suffix_info.py b/python/sglang/srt/speculative/suffix_info.py
new file mode 100644
index 000000000..0553e93e4
--- /dev/null
+++ b/python/sglang/srt/speculative/suffix_info.py
@@ -0,0 +1,109 @@
+"""
+Data structures for suffix decoding speculative method.
+
+Most of the logic is reused from ngram_info.py since both use tree-based verification.
+"""
+
+from __future__ import annotations
+
+import logging
+from dataclasses import dataclass
+
+import torch
+
+from sglang.srt.managers.schedule_batch import ScheduleBatch
+from sglang.srt.speculative.ngram_info import NgramVerifyInput
+from sglang.srt.speculative.spec_info import SpecInputType
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class SuffixVerifyInput(NgramVerifyInput):
+    """
+    Data structure for suffix decoding verification.
+
+    Inherits from NgramVerifyInput as both use identical tree-based verification
+    without a separate draft model. The only differences are the SpecInputType
+    and debug logging for accepted tokens.
+    """
+
+    def __init__(
+        self,
+        draft_token: torch.Tensor,
+        tree_mask: torch.Tensor,
+        positions: torch.Tensor,
+        retrive_index: torch.Tensor,
+        retrive_next_token: torch.Tensor,
+        retrive_next_sibling: torch.Tensor,
+        draft_token_num: int,
+    ):
+        # Call parent init to reuse all initialization logic
+        super().__init__(
+            draft_token,
+            tree_mask,
+            positions,
+            retrive_index,
+            retrive_next_token,
+            retrive_next_sibling,
+            draft_token_num,
+        )
+        # Override to set correct SpecInputType for suffix decoding
+        self.spec_input_type = SpecInputType.SUFFIX_VERIFY
+
+    def _fill_requests(
+        self,
+        batch: ScheduleBatch,
+        logits_output: torch.Tensor,
+    ):
+        """
+        Fill requests with accepted tokens.
+
+        Overrides parent to add debug logging of accepted tokens.
+        """
+        accept_index_cpu = self.accept_index.tolist()
+        predict_cpu = self.predict.tolist()
+        has_finished = False
+
+        # Iterate every accepted token and check if req has finished after append the token
+        # should be checked BEFORE free kv cache slots
+        for i, (req, accept_index_row) in enumerate(zip(batch.reqs, accept_index_cpu)):
+            accepted_tokens = []
+            for j, idx in enumerate(accept_index_row):
+                if idx == -1:
+                    break
+                id = predict_cpu[idx]
+                accepted_tokens.append(id)
+                req.output_ids.append(id)
+                req.check_finished()
+                if req.finished():
+                    has_finished = True
+                    # set all tokens after finished token to -1 and break
+                    self.accept_index[i, j + 1 :] = -1
+                    break
+                else:
+                    if req.grammar is not None:
+                        try:
+                            req.grammar.accept_token(id)
+                        except ValueError as e:
+                            logger.info(
+                                f"{i=}, {req=}\n"
+                                f"{self.accept_index=}\n"
+                                f"{self.predict=}\n"
+                            )
+                            raise e
+            if accepted_tokens:
+                logger.debug(
+                    f"[DEBUG SUFFIX VERIFY] req={req.rid}: Accepted {len(accepted_tokens)} tokens: {accepted_tokens[:10]}"
+                )
+            req.spec_verify_ct += 1
+        if has_finished:
+            self.accept_length = (self.accept_index != -1).sum(dim=1) - 1
+        self.accept_index = self.accept_index[self.accept_index != -1]
+
+        logits_output.next_token_logits = logits_output.next_token_logits[
+            self.accept_index
+        ]
+        if logits_output.hidden_states:
+            logits_output.hidden_states = logits_output.hidden_states[self.accept_index]
+        self.verified_id = self.predict[self.accept_index]
diff --git a/python/sglang/srt/speculative/suffix_worker.py b/python/sglang/srt/speculative/suffix_worker.py
new file mode 100644
index 000000000..ad015f04e
--- /dev/null
+++ b/python/sglang/srt/speculative/suffix_worker.py
@@ -0,0 +1,106 @@
+"""
+Suffix decoding worker that reuses NGRAMWorker with a cache adapter.
+
+This is a thin wrapper that replaces NgramCache with SuffixCacheAdapter,
+allowing all the tree-based verification logic to be reused.
+"""
+
+import logging
+from typing import Optional
+
+from sglang.srt.managers.tp_worker import TpModelWorker
+from sglang.srt.server_args import ServerArgs
+from sglang.srt.speculative.ngram_worker import NGRAMWorker
+from sglang.srt.speculative.suffix_cache_adapter import SuffixCacheAdapter
+
+logger = logging.getLogger(__name__)
+
+
+class SuffixWorker(NGRAMWorker):
+    """
+    Suffix decoding worker that inherits from NGRAMWorker.
+
+    The only difference is using SuffixCacheAdapter instead of NgramCache.
+    All tree-based verification logic is inherited from NGRAMWorker.
+    """
+
+    def __init__(
+        self,
+        server_args: ServerArgs,
+        gpu_id: int,
+        tp_rank: int,
+        dp_rank: Optional[int],
+        moe_ep_rank: int,
+        nccl_port: int,
+        target_worker: TpModelWorker,
+    ):
+        # Call parent __init__ which sets up all the infrastructure
+        super().__init__(
+            server_args,
+            gpu_id,
+            tp_rank,
+            dp_rank,
+            moe_ep_rank,
+            nccl_port,
+            target_worker,
+        )
+
+        self.ngram_cache = SuffixCacheAdapter(
+            draft_token_num=server_args.speculative_num_draft_tokens,
+            max_batch_size=self.max_batch_size,
+            max_tree_depth=server_args.speculative_suffix_max_tree_depth,
+            max_cached_requests=server_args.speculative_suffix_max_cached_requests,
+            max_spec_factor=server_args.speculative_suffix_max_spec_factor,
+            min_token_prob=server_args.speculative_suffix_min_token_prob,
+        )
+
+    def _prepare_draft_tokens(self, batch):
+        """
+        Override to pass FULL token sequences to the cache adapter.
+
+        NGRAMWorker passes only last N tokens, but the suffix cache needs:
+        1. Full prompt for start_request()
+        2. Full sequence for suffix tree building
+        3. Request identity tracking
+        """
+
+        bs = batch.batch_size()
+
+        self.ngram_cache.synchronize()
+        batch_req_ids = []
+        batch_prompts = []
+        batch_tokens = []
+        for req in batch.reqs:
+            # Pass request ID for stable tracking
+            batch_req_ids.append(req.rid)
+            # Pass prompt separately (for cache initialization)
+            batch_prompts.append(req.origin_input_ids)
+            # Pass FULL token sequence (prompt + outputs), not just last N
+            full_tokens = req.origin_input_ids + req.output_ids
+            batch_tokens.append(full_tokens)
+
+        req_drafts, mask = self.ngram_cache.batch_get(
+            batch_req_ids, batch_prompts, batch_tokens
+        )
+        # total_draft_token_num = len(req_drafts)
+
+        # assert (
+        #     total_draft_token_num == bs * self.draft_token_num
+        # ), f"{total_draft_token_num=}, {bs=}, {self.draft_token_num=}"
+
+        return req_drafts, mask
+
+    def _update_ngram_cache(self, batch):
+        """
+        Override to pass FULL token sequences for cache updates.
+        """
+        batch_req_ids = []
+        batch_tokens = []
+        for req in batch.reqs:
+            # Pass request ID for stable tracking
+            batch_req_ids.append(req.rid)
+            # Pass FULL token sequence for delta computation
+            full_tokens = req.origin_input_ids + req.output_ids
+            batch_tokens.append(full_tokens)
+
+        self.ngram_cache.batch_put(batch_req_ids, batch_tokens)
diff --git a/python/sglang/srt/utils/common.py b/python/sglang/srt/utils/common.py
index cc53385fd..d2023a556 100644
--- a/python/sglang/srt/utils/common.py
+++ b/python/sglang/srt/utils/common.py
@@ -1933,7 +1933,7 @@ def get_compiler_backend(mode=None) -> str:
                 "Please install torchair for torch.compile support on NPU."
             )
         compiler_config = CompilerConfig()
-        compiler_config.mode = "max-autotune"
+        compiler_config.mode = "max-autotune" if mode is None else mode
         if mode == "npugraph_ex":
             compiler_config.mode = "reduce-overhead"
             compiler_config.debug.run_eagerly = True
@@ -3432,6 +3432,9 @@ class ConcurrentCounter:
 def is_triton_kernels_available() -> bool:
     return importlib.util.find_spec("triton_kernels") is not None
 
+@lru_cache(maxsize=1)
+def is_arctic_inference_available() -> bool:
+    return importlib.util.find_spec("arctic_inference") is not None
 
 def check_cuda_result(raw_output):
     import cuda.bindings.runtime as cuda_rt
diff --git a/python/sglang/test/lora_utils.py b/python/sglang/test/lora_utils.py
index 1ff0f87b5..3c369a616 100644
--- a/python/sglang/test/lora_utils.py
+++ b/python/sglang/test/lora_utils.py
@@ -36,7 +36,7 @@ class LoRAModelCase:
 
 
 TORCH_DTYPES = [torch.float16]
-BACKENDS = ["triton", "csgmv"]
+BACKENDS = ["triton", "csgmv", "torch_native"]
 DEFAULT_PROMPTS = [
     "AI is a field of computer science focused on",
     """
diff --git a/python/sglang/test/runners.py b/python/sglang/test/runners.py
index e9b152ae9..70a2b18d5 100644
--- a/python/sglang/test/runners.py
+++ b/python/sglang/test/runners.py
@@ -121,7 +121,8 @@ def _get_sentence_transformer_embedding_model(
             modules=[word_embedding_model, pooling_model], truncate_dim=matryoshka_dim
         )
 
-    return model.cuda()
+    device = "npu" if is_npu() else "cuda"
+    return model.to(device)
 
 
 @dataclass
@@ -214,7 +215,7 @@ class HFRunner:
         **kwargs,
     ) -> torch.Tensor:
         if inputs_embeds is None:
-            inputs_embeds = self.model.model.embed_tokens(input_ids)
+            inputs_embeds = self.model.model.get_input_embeddings()(input_ids)
             if pixel_values is not None:
                 pixel_values = pixel_values.type(self.model.visual.get_dtype())
                 image_embeds = self.model.visual(
@@ -286,11 +287,12 @@ class HFRunner:
         elif self.model_type == "reward" or self.model_type == "cross_encoder":
             from transformers import AutoModelForSequenceClassification
 
+            device = "npu" if is_npu() else "cuda"
             self.model = AutoModelForSequenceClassification.from_pretrained(
                 model_path,
                 torch_dtype=torch_dtype,
                 trust_remote_code=self.needs_trust_remote_code(model_path),
-            ).cuda()
+            ).to(device)
         else:
             raise Exception(f"Unrecognized model type {self.model_type}")
         self.tokenizer = get_tokenizer(
@@ -347,9 +349,10 @@ class HFRunner:
                         logits = self.model.encode(prompts).tolist()
                     out_queue.put(ModelOutput(embed_logits=logits))
                 elif self.model_type == "cross_encoder":
+                    device = "npu" if is_npu() else "cuda"
                     inputs = self.tokenizer(
                         prompts, padding=True, return_tensors="pt"
-                    ).to("cuda")
+                    ).to(device)
                     scores = self.model(**inputs).logits
                     scores = scores.squeeze().tolist()
                     if not isinstance(scores, list):
@@ -358,13 +361,14 @@ class HFRunner:
 
                 elif self.model_type == "reward":
                     scores = []
+                    device = "npu" if is_npu() else "cuda"
                     for conv in prompts:
                         conv_formatted = self.tokenizer.apply_chat_template(
                             conv, tokenize=False, return_dict=False
                         )
                         conv_tokenized = self.tokenizer(
                             conv_formatted, return_tensors="pt"
-                        ).to("cuda")
+                        ).to(device)
                         scores.append(
                             float(self.model(**conv_tokenized).logits[0][0].item())
                         )
diff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py
index 2500d6193..2cbc7f7cf 100644
--- a/python/sglang/test/test_utils.py
+++ b/python/sglang/test/test_utils.py
@@ -628,6 +628,7 @@ def popen_launch_server(
             env=env,
             text=True,
             bufsize=1,
+            errors="ignore",
         )
 
         def _dump(src, sinks):
diff --git a/scripts/ci/npu_ci_install_dependency.sh b/scripts/ci/npu_ci_install_dependency.sh
index bbc27a6df..d9bab32c1 100755
--- a/scripts/ci/npu_ci_install_dependency.sh
+++ b/scripts/ci/npu_ci_install_dependency.sh
@@ -33,7 +33,7 @@ PYTORCH_VERSION="2.8.0"
 TORCHVISION_VERSION="0.23.0"
 ${PIP_INSTALL} torch==${PYTORCH_VERSION} torchvision==${TORCHVISION_VERSION} --index-url https://download.pytorch.org/whl/cpu
 
-PTA_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/torch_npu-2.8.0.post2.dev20251113-cp311-cp311-manylinux_2_28_aarch64.whl"
+PTA_URL="https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/torch_npu-2.8.0.post2.dev20251224-cp311-cp311-manylinux_2_28_aarch64.whl"
 ${PIP_INSTALL} ${PTA_URL}
 
 
@@ -49,17 +49,20 @@ wget -O "${BISHENG_NAME}" "${BISHENG_URL}" && chmod a+x "${BISHENG_NAME}" && "./
 
 
 ### Install sgl-kernel-npu
-SGL_KERNEL_NPU_TAG="20251206"
-git clone --depth 1 https://github.com/sgl-project/sgl-kernel-npu.git --branch ${SGL_KERNEL_NPU_TAG}
-(cd sgl-kernel-npu && bash ./build.sh && ${PIP_INSTALL} output/deep_ep*.whl output/sgl_kernel_npu*.whl && cd "$(python3 -m pip show deep-ep | grep -E '^Location:' | awk '{print $2}')" && ln -s deep_ep/deep_ep_cpp*.so)
+SGL_KERNEL_NPU_TAG="2025.12.29"
+mkdir sgl-kernel-npu
+(cd sgl-kernel-npu && wget https://github.com/sgl-project/sgl-kernel-npu/releases/download/${SGLANG_KERNEL_NPU_TAG}/sgl-kernel-npu_${SGLANG_KERNEL_NPU_TAG}_8.3.rc2_910b.zip \
+&& unzip sgl-kernel-npu_${SGLANG_KERNEL_NPU_TAG}_8.3.rc2_910b.zip \
+&& ${PIP_INSTALL} output/deep_ep*.whl output/sgl_kernel_npu*.whl \
+&& (cd "$(python3 -m pip show deep-ep | grep -E '^Location:' | awk '{print $2}')" && ln -s deep_ep/deep_ep_cpp*.so))
 
 
 ### Install CustomOps (TODO: to be removed once merged into sgl-kernel-npu)
-wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run
-chmod a+x ./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run
-./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run --quiet --install-path=/usr/local/Ascend/ascend-toolkit/latest/opp
-wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl
-pip install ./custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl
+wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run
+chmod a+x ./CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run
+./CANN-custom_ops-8.3.0.1-$DEVICE_TYPE-linux.aarch64.run --quiet --install-path=/usr/local/Ascend/ascend-toolkit/latest/opp
+wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/custom_ops-2.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl
+pip install ./custom_ops-2.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl
 
 ### Install SGLang
 rm -rf python/pyproject.toml && mv python/pyproject_other.toml python/pyproject.toml
diff --git a/sgl-model-gateway/benches/request_processing.rs b/sgl-model-gateway/benches/request_processing.rs
index 60d51faf0..dfdc3ecd2 100644
--- a/sgl-model-gateway/benches/request_processing.rs
+++ b/sgl-model-gateway/benches/request_processing.rs
@@ -58,6 +58,7 @@ fn default_generate_request() -> GenerateRequest {
         bootstrap_room: None,
         bootstrap_pair_key: None,
         data_parallel_rank: None,
+        data_parallel_rank_decode: None,
         background: false,
         conversation_id: None,
         priority: None,
diff --git a/sgl-model-gateway/bindings/python/sglang_router/mini_lb.py b/sgl-model-gateway/bindings/python/sglang_router/mini_lb.py
index 39e809358..ddb6753e0 100644
--- a/sgl-model-gateway/bindings/python/sglang_router/mini_lb.py
+++ b/sgl-model-gateway/bindings/python/sglang_router/mini_lb.py
@@ -5,6 +5,7 @@ Minimal HTTP load balancer for prefill and decode servers for testing.
 import asyncio
 import ipaddress
 import logging
+import os
 import random
 import urllib
 from http import HTTPStatus
@@ -63,11 +64,30 @@ class MiniLoadBalancer:
         self.decode_urls = router_args.decode_urls
         self.otlp_traces_endpoint = router_args.otlp_traces_endpoint
         self.enable_trace = router_args.enable_trace
+        self.dp_attention_round_robin_size_dict = dict.fromkeys(self.prefill_urls, 0)
+        self.dp_attention_round_robin_size_dict = {
+            url: i * 20000 for i, url in enumerate(self.prefill_urls)
+        }
         if self.enable_trace and not trace_package_imported:
             logger.warning(
                 "Tracing is not supported in this environment. Please install sglang."
             )
             self.enable_trace = False
+        self.req_nums = 0
+
+    def next_round_robin_num(self, prefill_server):
+        is_dp_round_robin = os.getenv("SGLANG_DP_ROUND_ROBIN", "0") == "1"
+        if is_dp_round_robin:
+            self.dp_attention_round_robin_size_dict[prefill_server] = (
+                self.dp_attention_round_robin_size_dict[prefill_server] + 1
+            )
+            bootstrap_room = self.dp_attention_round_robin_size_dict[prefill_server]
+        else:
+            bootstrap_room = random.randint(0, 2**63 - 1)
+            if lb.enable_trace:
+                trace_req_start(bootstrap_room, bootstrap_room, role="router")
+                trace_slice_start("mini_lb_launch", bootstrap_room)
+        return bootstrap_room
 
     def _validate_router_args(self, router_args: RouterArgs):
         logger.warning(
@@ -98,8 +118,14 @@ class MiniLoadBalancer:
     def select_pair(self):
         assert len(self.prefill_urls) > 0, "No prefill servers available"
         assert len(self.decode_urls) > 0, "No decode servers available"
-        pidx = random.randint(0, len(self.prefill_urls) - 1)
-        didx = random.randint(0, len(self.decode_urls) - 1)
+        is_instance_round_robin = os.getenv("SGLANG_INSTANCE_ROUND_ROBIN", "0") == "1"
+        if is_instance_round_robin:
+            pidx = self.req_nums % len(self.prefill_urls)
+            didx = self.req_nums % len(self.decode_urls)
+            self.req_nums = self.req_nums + 1
+        else:
+            pidx = random.randint(0, len(self.prefill_urls) - 1)
+            didx = random.randint(0, len(self.decode_urls) - 1)
         return (
             self.prefill_urls[pidx],
             self.prefill_bootstrap_ports[pidx],
@@ -400,7 +426,7 @@ async def handle_generate_request(request_data: dict):
                 "bootstrap_host": [hostname] * batch_size,
                 "bootstrap_port": [bootstrap_port] * batch_size,
                 "bootstrap_room": [
-                    _generate_bootstrap_room() for _ in range(batch_size)
+                    lb.next_round_robin_num(prefill_server) for _ in range(batch_size)
                 ],
             }
         )
@@ -409,7 +435,7 @@ async def handle_generate_request(request_data: dict):
             {
                 "bootstrap_host": hostname,
                 "bootstrap_port": bootstrap_port,
-                "bootstrap_room": _generate_bootstrap_room(),
+                "bootstrap_room": lb.next_round_robin_num(prefill_server),
             }
         )
 
@@ -434,7 +460,7 @@ async def _forward_to_backend(request_data: dict, endpoint_name: str):
         {
             "bootstrap_host": hostname,
             "bootstrap_port": bootstrap_port,
-            "bootstrap_room": _generate_bootstrap_room(),
+            "bootstrap_room": lb.next_round_robin_num(prefill_server),
         }
     )
 
@@ -464,14 +490,6 @@ async def handle_completion_request(request_data: dict):
     return await _forward_to_backend(request_data, "v1/completions")
 
 
-def _generate_bootstrap_room():
-    bootstrap_room = random.randint(0, 2**63 - 1)
-    if lb.enable_trace:
-        trace_req_start(bootstrap_room, bootstrap_room, role="router")
-        trace_slice_start("mini_lb_launch", bootstrap_room)
-    return bootstrap_room
-
-
 # We may utilize `GenerateReqInput`'s logic later
 def _get_request_batch_size(request):
     if (text := request.get("text")) is not None:
diff --git a/sgl-model-gateway/bindings/python/sglang_router/router.py b/sgl-model-gateway/bindings/python/sglang_router/router.py
index 4abe19d29..4efd4a0a1 100644
--- a/sgl-model-gateway/bindings/python/sglang_router/router.py
+++ b/sgl-model-gateway/bindings/python/sglang_router/router.py
@@ -76,6 +76,7 @@ class Router:
         port: Port number to bind the router server. Default: 3001
         worker_startup_timeout_secs: Timeout in seconds for worker startup and registration. Large models can take significant time to load into GPU memory. Default: 1800 (30 minutes)
         worker_startup_check_interval: Interval in seconds between checks for worker initialization. Default: 10
+        worker_load_check_interval: Interval in seconds between get loads for worker initialization. Default: 10
         cache_threshold: Cache threshold (0.0-1.0) for cache-aware routing. Routes to cached worker
             if the match rate exceeds threshold, otherwise routes to the worker with the smallest
             tree. Default: 0.5
@@ -88,6 +89,7 @@ class Router:
         max_payload_size: Maximum payload size in bytes. Default: 256MB
         max_tree_size: Maximum size of the approximation tree for cache-aware routing. Default: 2^24
         dp_aware: Enable data parallelism aware schedule. Default: False
+        dp_minimum_tokens_scheduler: Enable minimum tokens scheduler for data parallel group. Default: False
         enable_igw: Enable IGW (Inference-Gateway) mode for multi-model support. When enabled,
             the router can manage multiple models simultaneously with per-model load balancing
             policies. Default: False
diff --git a/sgl-model-gateway/bindings/python/sglang_router/router_args.py b/sgl-model-gateway/bindings/python/sglang_router/router_args.py
index 9fa3125b4..c36d97788 100644
--- a/sgl-model-gateway/bindings/python/sglang_router/router_args.py
+++ b/sgl-model-gateway/bindings/python/sglang_router/router_args.py
@@ -30,6 +30,7 @@ class RouterArgs:
     decode_policy: Optional[str] = None  # Specific policy for decode nodes in PD mode
     worker_startup_timeout_secs: int = 1800
     worker_startup_check_interval: int = 30
+    worker_load_check_interval: int = 10
     cache_threshold: float = 0.3
     balance_abs_threshold: int = 64
     balance_rel_threshold: float = 1.5
@@ -38,6 +39,7 @@ class RouterArgs:
     max_payload_size: int = 512 * 1024 * 1024  # 512MB default for large batches
     bucket_adjust_interval_secs: int = 5
     dp_aware: bool = False
+    dp_minimum_tokens_scheduler: bool = False
     enable_igw: bool = False  # Enable IGW (Inter-Gateway) mode for multi-model support
     api_key: Optional[str] = None
     log_dir: Optional[str] = None
@@ -227,6 +229,12 @@ class RouterArgs:
             default=RouterArgs.worker_startup_check_interval,
             help="Interval in seconds between checks for worker startup",
         )
+        parser.add_argument(
+            f"--{prefix}worker-load-check-interval",
+            type=int,
+            default=RouterArgs.worker_load_check_interval,
+            help="Interval in seconds between checks for worker startup",
+        )
         parser.add_argument(
             f"--{prefix}cache-threshold",
             type=float,
@@ -274,6 +282,11 @@ class RouterArgs:
             action="store_true",
             help="Enable data parallelism aware schedule",
         )
+        parser.add_argument(
+            f"--{prefix}dp-minimum-tokens-scheduler",
+            action="store_true",
+            help="Enable minimum tokens scheduler for data parallel group",
+        )
         parser.add_argument(
             f"--{prefix}enable-igw",
             action="store_true",
diff --git a/sgl-model-gateway/bindings/python/src/lib.rs b/sgl-model-gateway/bindings/python/src/lib.rs
index 79337d466..d7972d41e 100644
--- a/sgl-model-gateway/bindings/python/src/lib.rs
+++ b/sgl-model-gateway/bindings/python/src/lib.rs
@@ -158,6 +158,7 @@ struct Router {
     policy: PolicyType,
     worker_startup_timeout_secs: u64,
     worker_startup_check_interval: u64,
+    worker_load_check_interval: u64,
     cache_threshold: f32,
     balance_abs_threshold: usize,
     balance_rel_threshold: f32,
@@ -165,6 +166,7 @@ struct Router {
     max_tree_size: usize,
     max_payload_size: usize,
     dp_aware: bool,
+    dp_minimum_tokens_scheduler: bool,
     api_key: Option<String>,
     log_dir: Option<String>,
     log_level: Option<String>,
@@ -354,6 +356,7 @@ impl Router {
             .request_timeout_secs(self.request_timeout_secs)
             .worker_startup_timeout_secs(self.worker_startup_timeout_secs)
             .worker_startup_check_interval_secs(self.worker_startup_check_interval)
+            .worker_load_check_interval_secs(self.worker_load_check_interval)
             .max_concurrent_requests(self.max_concurrent_requests)
             .queue_size(self.queue_size)
             .queue_timeout_secs(self.queue_timeout_secs)
@@ -414,6 +417,7 @@ impl Router {
                 self.server_cert_path.as_ref(),
                 self.server_key_path.as_ref(),
             )
+            .dp_minimum_tokens_scheduler(self.dp_minimum_tokens_scheduler)
             .build()
     }
 }
@@ -428,6 +432,7 @@ impl Router {
         port = 3001,
         worker_startup_timeout_secs = 600,
         worker_startup_check_interval = 30,
+        worker_load_check_interval = 10,
         cache_threshold = 0.3,
         balance_abs_threshold = 64,
         balance_rel_threshold = 1.5,
@@ -435,6 +440,7 @@ impl Router {
         max_tree_size = 2usize.pow(26),
         max_payload_size = 512 * 1024 * 1024,
         dp_aware = false,
+        dp_minimum_tokens_scheduler = false,
         api_key = None,
         log_dir = None,
         log_level = None,
@@ -509,6 +515,7 @@ impl Router {
         port: u16,
         worker_startup_timeout_secs: u64,
         worker_startup_check_interval: u64,
+        worker_load_check_interval: u64,
         cache_threshold: f32,
         balance_abs_threshold: usize,
         balance_rel_threshold: f32,
@@ -516,6 +523,7 @@ impl Router {
         max_tree_size: usize,
         max_payload_size: usize,
         dp_aware: bool,
+        dp_minimum_tokens_scheduler: bool,
         api_key: Option<String>,
         log_dir: Option<String>,
         log_level: Option<String>,
@@ -603,6 +611,7 @@ impl Router {
             policy,
             worker_startup_timeout_secs,
             worker_startup_check_interval,
+            worker_load_check_interval,
             cache_threshold,
             balance_abs_threshold,
             balance_rel_threshold,
@@ -610,6 +619,7 @@ impl Router {
             max_tree_size,
             max_payload_size,
             dp_aware,
+            dp_minimum_tokens_scheduler,
             api_key,
             log_dir,
             log_level,
diff --git a/sgl-model-gateway/py_test/fixtures/router_manager.py b/sgl-model-gateway/py_test/fixtures/router_manager.py
index 10d0b631f..9f0d7f2c1 100644
--- a/sgl-model-gateway/py_test/fixtures/router_manager.py
+++ b/sgl-model-gateway/py_test/fixtures/router_manager.py
@@ -79,6 +79,8 @@ class RouterManager:
                 "api_key": "--api-key",
                 # Health/monitoring
                 "worker_startup_check_interval": "--worker-startup-check-interval",
+                # Loads/monitoring
+                "worker_load_check_interval": "--worker-load-check-interval",
                 # Cache-aware tuning
                 "cache_threshold": "--cache-threshold",
                 "balance_abs_threshold": "--balance-abs-threshold",
diff --git a/sgl-model-gateway/py_test/integration_mock/load_balancing/test_power_of_two.py b/sgl-model-gateway/py_test/integration_mock/load_balancing/test_power_of_two.py
index 0a8d9eab1..a4f8488f2 100644
--- a/sgl-model-gateway/py_test/integration_mock/load_balancing/test_power_of_two.py
+++ b/sgl-model-gateway/py_test/integration_mock/load_balancing/test_power_of_two.py
@@ -22,7 +22,7 @@ def test_power_of_two_prefers_less_loaded(mock_workers, router_manager):
     rh = router_manager.start_router(
         worker_urls=urls,
         policy="power_of_two",
-        extra={"worker_startup_check_interval": 1},
+        extra={"worker_load_check_interval": 1},
     )
 
     # Prime: fire a burst to create measurable load on slow worker, then wait for monitor tick
diff --git a/sgl-model-gateway/src/app_context.rs b/sgl-model-gateway/src/app_context.rs
index 7488991dc..759a6662e 100644
--- a/sgl-model-gateway/src/app_context.rs
+++ b/sgl-model-gateway/src/app_context.rs
@@ -450,6 +450,12 @@ impl AppContextBuilder {
     /// Create policy registry
     fn with_policy_registry(mut self, config: &RouterConfig) -> Self {
         self.policy_registry = Some(Arc::new(PolicyRegistry::new(config.policy.clone())));
+        if config.dp_minimum_tokens_scheduler {
+            self.policy_registry
+                .as_ref()
+                .unwrap()
+                .enable_dp_minimum_tokens_scheduler();
+        }
         self
     }
 
@@ -481,7 +487,7 @@ impl AppContextBuilder {
                 .expect("policy_registry must be set")
                 .clone(),
             client.clone(),
-            config.worker_startup_check_interval_secs,
+            config.worker_load_check_interval_secs,
         )));
         self
     }
diff --git a/sgl-model-gateway/src/config/builder.rs b/sgl-model-gateway/src/config/builder.rs
index 37180d57d..236d4aede 100644
--- a/sgl-model-gateway/src/config/builder.rs
+++ b/sgl-model-gateway/src/config/builder.rs
@@ -185,6 +185,10 @@ impl RouterConfigBuilder {
         self
     }
 
+    pub fn worker_load_check_interval_secs(mut self, interval: u64) -> Self {
+        self.config.worker_load_check_interval_secs = interval;
+        self
+    }
     // ==================== Rate Limiting ====================
 
     pub fn max_concurrent_requests(mut self, max: i32) -> Self {
@@ -463,6 +467,11 @@ impl RouterConfigBuilder {
         self
     }
 
+    pub fn dp_minimum_tokens_scheduler(mut self, enable: bool) -> Self {
+        self.config.dp_minimum_tokens_scheduler = enable;
+        self
+    }
+
     // ==================== Option Setters ====================
     // Accept Option<T> and only set if Some
 
diff --git a/sgl-model-gateway/src/config/types.rs b/sgl-model-gateway/src/config/types.rs
index a933f1a41..2a8ea35ee 100644
--- a/sgl-model-gateway/src/config/types.rs
+++ b/sgl-model-gateway/src/config/types.rs
@@ -19,7 +19,9 @@ pub struct RouterConfig {
     pub request_timeout_secs: u64,
     pub worker_startup_timeout_secs: u64,
     pub worker_startup_check_interval_secs: u64,
+    pub worker_load_check_interval_secs: u64,
     pub dp_aware: bool,
+    pub dp_minimum_tokens_scheduler: bool,
     pub api_key: Option<String>,
     pub discovery: Option<DiscoveryConfig>,
     pub metrics: Option<MetricsConfig>,
@@ -496,7 +498,9 @@ impl Default for RouterConfig {
             request_timeout_secs: 1800,        // 30 minutes
             worker_startup_timeout_secs: 1800, // 30 minutes for large model loading
             worker_startup_check_interval_secs: 30,
+            worker_load_check_interval_secs: 10,
             dp_aware: false,
+            dp_minimum_tokens_scheduler: false,
             api_key: None,
             discovery: None,
             metrics: None,
diff --git a/sgl-model-gateway/src/core/worker_manager.rs b/sgl-model-gateway/src/core/worker_manager.rs
index 4845c7f80..1499a73ce 100644
--- a/sgl-model-gateway/src/core/worker_manager.rs
+++ b/sgl-model-gateway/src/core/worker_manager.rs
@@ -221,15 +221,23 @@ impl WorkerManager {
                 let client = client.clone();
 
                 async move {
-                    let load = if is_http {
+                    let dp_rank_loads = if is_http {
                         Self::parse_load_response(&client, &url, api_key.as_deref()).await
+                    } else {
+                        HashMap::new()
+                    };
+
+                    let load = if !dp_rank_loads.is_empty() {
+                        dp_rank_loads.values().sum::<isize>()
                     } else {
                         -1
                     };
+
                     WorkerLoadInfo {
                         worker: url,
                         worker_type,
                         load,
+                        dp_rank_loads,
                     }
                 }
             })
@@ -251,7 +259,7 @@ impl WorkerManager {
         client: &reqwest::Client,
         url: &str,
         api_key: Option<&str>,
-    ) -> isize {
+    ) -> HashMap<isize, isize> {
         let load_url = format!("{}/get_load", url);
         let mut req = client.get(&load_url).timeout(REQUEST_TIMEOUT);
         if let Some(key) = api_key {
@@ -260,15 +268,27 @@ impl WorkerManager {
 
         match req.send().await {
             Ok(r) if r.status().is_success() => match r.json::<Value>().await {
-                Ok(json) if json.is_array() => json
-                    .as_array()
-                    .unwrap()
-                    .iter()
-                    .filter_map(|e| e.get("num_tokens").and_then(|v| v.as_i64()))
-                    .sum::<i64>() as isize,
-                _ => -1,
+                Ok(json) if json.is_array() => {
+                    let mut load_map = HashMap::new();
+
+                    for element in json.as_array().unwrap().iter() {
+                        if let (Some(dp_rank_value), Some(num_tokens_value)) = (
+                            element.get("dp_rank"),
+                            element.get("num_tokens")
+                        ) {
+                            if let (Some(dp_rank), Some(num_tokens)) = (
+                                dp_rank_value.as_i64(),
+                                num_tokens_value.as_i64()
+                            ) {
+                                load_map.insert(dp_rank as isize, num_tokens as isize);
+                            }
+                        }
+                    }
+                    load_map
+                }
+                _ => HashMap::new(),
             },
-            _ => -1,
+            _ => HashMap::new(),
         }
     }
 
@@ -389,19 +409,23 @@ impl LoadMonitor {
 
         loop {
             interval_timer.tick().await;
-
             let power_of_two_policies = policy_registry.get_all_power_of_two_policies();
 
-            if power_of_two_policies.is_empty() {
+            if power_of_two_policies.is_empty()
+                && !policy_registry.is_dp_minimum_tokens_scheduler_enabled()
+            {
                 debug!("No PowerOfTwo policies found, skipping load fetch");
                 continue;
             }
 
+            let all_policies = policy_registry.get_all_policies();
             let result = WorkerManager::get_all_worker_loads(&worker_registry, &client).await;
 
             let mut loads = HashMap::new();
+            let mut dp_rank_loads = HashMap::new();
             for load_info in result.loads {
-                loads.insert(load_info.worker, load_info.load);
+                loads.insert(load_info.worker.clone(), load_info.load);
+                dp_rank_loads.insert(load_info.worker, load_info.dp_rank_loads);
             }
 
             if !loads.is_empty() {
@@ -413,6 +437,9 @@ impl LoadMonitor {
                 for policy in &power_of_two_policies {
                     policy.update_loads(&loads);
                 }
+                for policy in &all_policies {
+                    policy.update_dp_loads(&dp_rank_loads)
+                }
                 let _ = tx.send(loads);
             } else {
                 warn!("No loads fetched from workers");
diff --git a/sgl-model-gateway/src/main.rs b/sgl-model-gateway/src/main.rs
index 9ad931f9b..49f16fdd9 100644
--- a/sgl-model-gateway/src/main.rs
+++ b/sgl-model-gateway/src/main.rs
@@ -157,6 +157,9 @@ struct CliArgs {
     #[arg(long, default_value_t = 30)]
     worker_startup_check_interval: u64,
 
+    #[arg(long, default_value_t = 1)]
+    worker_load_check_interval: u64,
+
     #[arg(long, default_value_t = 0.3)]
     cache_threshold: f32,
 
@@ -178,6 +181,9 @@ struct CliArgs {
     #[arg(long, default_value_t = false)]
     dp_aware: bool,
 
+    #[arg(long, default_value_t = false)]
+    dp_minimum_tokens_scheduler: bool,
+
     #[arg(long)]
     api_key: Option<String>,
 
@@ -618,6 +624,7 @@ impl CliArgs {
             .request_timeout_secs(self.request_timeout_secs)
             .worker_startup_timeout_secs(self.worker_startup_timeout_secs)
             .worker_startup_check_interval_secs(self.worker_startup_check_interval)
+            .worker_load_check_interval_secs(self.worker_load_check_interval)
             .max_concurrent_requests(self.max_concurrent_requests)
             .queue_size(self.queue_size)
             .queue_timeout_secs(self.queue_timeout_secs)
@@ -668,6 +675,7 @@ impl CliArgs {
             .maybe_tool_call_parser(self.tool_call_parser.as_ref())
             .maybe_mcp_config_path(self.mcp_config_path.as_ref())
             .dp_aware(self.dp_aware)
+            .dp_minimum_tokens_scheduler(self.dp_minimum_tokens_scheduler)
             .retries(!self.disable_retries)
             .circuit_breaker(!self.disable_circuit_breaker)
             .enable_wasm(self.enable_wasm)
diff --git a/sgl-model-gateway/src/policies/bucket.rs b/sgl-model-gateway/src/policies/bucket.rs
index c86b2fbc7..f0accda9e 100644
--- a/sgl-model-gateway/src/policies/bucket.rs
+++ b/sgl-model-gateway/src/policies/bucket.rs
@@ -10,7 +10,7 @@ use rand::Rng;
 use tracing::{debug, error, info, warn};
 use uuid::Uuid;
 
-use super::{get_healthy_worker_indices, BucketConfig, LoadBalancingPolicy};
+use super::{get_healthy_worker_indices, BucketConfig, DPLoadManager, LoadBalancingPolicy};
 use crate::core::Worker;
 
 #[derive(Debug)]
@@ -18,6 +18,7 @@ pub struct BucketPolicy {
     config: BucketConfig,
     buckets: Arc<DashMap<String, Arc<RwLock<Bucket>>>>,
     adjustment_handle: Option<thread::JoinHandle<()>>,
+    dp_load_manager: DPLoadManager,
 }
 
 impl Default for BucketPolicy {
@@ -72,6 +73,7 @@ impl BucketPolicy {
             config,
             buckets,
             adjustment_handle,
+            dp_load_manager: DPLoadManager::new(),
         }
     }
 
@@ -323,6 +325,18 @@ impl LoadBalancingPolicy for BucketPolicy {
     fn as_any(&self) -> &dyn std::any::Any {
         self
     }
+
+    fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        self.dp_load_manager.update_dp_loads(loads);
+    }
+
+    fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        self.dp_load_manager.get_lowest_dp_load(worker)
+    }
+
+    fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, tokens: isize) {
+        self.dp_load_manager.load_increment(worker, dp_rank, tokens);
+    }
 }
 
 #[derive(Debug, Clone)]
diff --git a/sgl-model-gateway/src/policies/cache_aware.rs b/sgl-model-gateway/src/policies/cache_aware.rs
index 6a6545367..237e97ec2 100644
--- a/sgl-model-gateway/src/policies/cache_aware.rs
+++ b/sgl-model-gateway/src/policies/cache_aware.rs
@@ -60,6 +60,7 @@
 */
 
 use std::{
+    collections::HashMap,
     sync::{
         atomic::{AtomicBool, Ordering},
         Arc,
@@ -72,7 +73,9 @@ use dashmap::DashMap;
 use rand::Rng;
 use tracing::debug;
 
-use super::{get_healthy_worker_indices, tree::Tree, CacheAwareConfig, LoadBalancingPolicy};
+use super::{
+    get_healthy_worker_indices, tree::Tree, CacheAwareConfig, DPLoadManager, LoadBalancingPolicy,
+};
 use crate::core::Worker;
 
 /// Cache-aware routing policy
@@ -88,6 +91,7 @@ pub struct CacheAwarePolicy {
     eviction_handle: Option<thread::JoinHandle<()>>,
     /// Flag to signal the eviction thread to stop
     shutdown_flag: Arc<AtomicBool>,
+    dp_load_manager: DPLoadManager,
 }
 
 impl CacheAwarePolicy {
@@ -151,14 +155,14 @@ impl CacheAwarePolicy {
             trees,
             eviction_handle,
             shutdown_flag,
+            dp_load_manager: DPLoadManager::new(),
         }
     }
 
     /// Initialize the tree with worker URLs (used only during initial setup)
     pub fn init_workers(&self, workers: &[Arc<dyn Worker>]) {
         // Group workers by model
-        let mut model_workers: std::collections::HashMap<String, Vec<&Arc<dyn Worker>>> =
-            std::collections::HashMap::new();
+        let mut model_workers: HashMap<String, Vec<&Arc<dyn Worker>>> = HashMap::new();
         for worker in workers {
             // Use "default" for unknown/empty model_ids for backward compatibility
             let model_id = worker.model_id();
@@ -423,6 +427,18 @@ impl LoadBalancingPolicy for CacheAwarePolicy {
     fn as_any(&self) -> &dyn std::any::Any {
         self
     }
+
+    fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        self.dp_load_manager.update_dp_loads(loads);
+    }
+
+    fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        self.dp_load_manager.get_lowest_dp_load(worker)
+    }
+
+    fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, tokens: isize) {
+        self.dp_load_manager.load_increment(worker, dp_rank, tokens);
+    }
 }
 
 impl Default for CacheAwarePolicy {
diff --git a/sgl-model-gateway/src/policies/mod.rs b/sgl-model-gateway/src/policies/mod.rs
index f74483d6a..3203f1eb2 100644
--- a/sgl-model-gateway/src/policies/mod.rs
+++ b/sgl-model-gateway/src/policies/mod.rs
@@ -3,7 +3,13 @@
 //! This module provides a unified abstraction for routing policies that work
 //! across both regular and prefill-decode (PD) routing modes.
 
-use std::{fmt::Debug, sync::Arc};
+use std::{
+    collections::HashMap,
+    fmt::Debug,
+    sync::{Arc, RwLock},
+};
+
+use tracing::debug;
 
 use crate::core::Worker;
 
@@ -58,10 +64,22 @@ pub trait LoadBalancingPolicy: Send + Sync + Debug {
     /// Update worker load information
     ///
     /// This is called periodically with current load information for load-aware policies.
-    fn update_loads(&self, _loads: &std::collections::HashMap<String, isize>) {
+    fn update_loads(&self, _loads: &HashMap<String, isize>) {
+        // Default: no-op for policies that don't use load information
+    }
+
+    fn update_dp_loads(&self, _loads: &HashMap<String, HashMap<isize, isize>>) {
         // Default: no-op for policies that don't use load information
     }
 
+    fn get_lowest_dp_load(&self, _worker: &dyn Worker) -> Option<isize> {
+        None
+    }
+
+    fn load_increment(&self, _worker: &dyn Worker, _dp_rank: isize, _tokens: isize) {
+        // Default
+    }
+
     /// Reset any internal state
     ///
     /// This is useful for policies that maintain state (e.g., round-robin counters).
@@ -112,6 +130,51 @@ impl Default for BucketConfig {
     }
 }
 
+/// Configuration for cache-aware policy
+#[derive(Debug, Default)]
+pub struct DPLoadManager {
+    dp_cached_loads: RwLock<HashMap<String, HashMap<isize, isize>>>,
+}
+
+impl DPLoadManager {
+    pub fn new() -> Self {
+        Self {
+            dp_cached_loads: RwLock::new(HashMap::new()),
+        }
+    }
+
+    pub fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        debug!("RoundRobinPolicy update_dp_loads map:{:?}", loads);
+        if let Ok(mut cached) = self.dp_cached_loads.write() {
+            *cached = loads.clone();
+        }
+    }
+
+    pub fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        if let Ok(cached_loads) = self.dp_cached_loads.read() {
+            if let Some(loads) = cached_loads.get(worker.url()) {
+                return loads
+                    .iter()
+                    .min_by_key(|&(_, load)| load)
+                    .map(|(&rand_id, _)| rand_id);
+            }
+        }
+        None
+    }
+
+    pub fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, increment: isize) {
+        // Add an increment to the load of dp group,
+        // to prevent all request from being scheduled to the same DP group during the interval between two load reports.
+        if let Ok(mut cached_loads) = self.dp_cached_loads.write() {
+            if let Some(loads) = cached_loads.get_mut(worker.url()) {
+                if let Some(dp_load) = loads.get_mut(&dp_rank) {
+                    *dp_load += increment;
+                }
+            }
+        }
+    }
+}
+
 /// Helper function to filter healthy workers and return their indices
 pub(crate) fn get_healthy_worker_indices(workers: &[Arc<dyn Worker>]) -> Vec<usize> {
     workers
@@ -122,6 +185,97 @@ pub(crate) fn get_healthy_worker_indices(workers: &[Arc<dyn Worker>]) -> Vec<usi
         .collect()
 }
 
+#[cfg(test)]
+mod dp_load_manager_tests {
+    use super::*;
+    use crate::core::{BasicWorkerBuilder, WorkerType};
+
+    #[test]
+    fn test_new_dp_load_manager_instance() {
+        let dp_load_manager = DPLoadManager::new();
+        let cached = dp_load_manager.dp_cached_loads.read().unwrap();
+        assert!(cached.is_empty());
+    }
+
+    #[test]
+    fn test_update_dp_load() {
+        let manager = DPLoadManager::new();
+        let mut loads = HashMap::new();
+
+        // insert worker1_load
+        let mut worker1_load = HashMap::new();
+        worker1_load.insert(0, 2);
+        worker1_load.insert(1, 1);
+        loads.insert("http://worker1:8080".to_string(), worker1_load);
+
+        // insert worker2.load
+        let mut worker2_load = HashMap::new();
+        worker2_load.insert(0, 3);
+        loads.insert("http://worker2:8080".to_string(), worker2_load);
+
+        // update
+        manager.update_dp_loads(&loads);
+
+        // assert
+        let cached = manager.dp_cached_loads.read().unwrap();
+        assert_eq!(cached.len(), 2);
+
+        let worker2_cache = cached.get("http://worker2:8080").unwrap();
+        assert_eq!(worker2_cache.get(&0), Some(&3));
+    }
+
+    #[test]
+    fn test_get_lowest_dp_load() {
+        let worker1 = BasicWorkerBuilder::new("http://worker1:8080")
+            .worker_type(WorkerType::Regular)
+            .api_key("test_api_key2")
+            .build();
+
+        let manager = DPLoadManager::new();
+        let mut loads = HashMap::new();
+        // insert worker1_load
+        let mut worker1_load = HashMap::new();
+        worker1_load.insert(0, 2);
+        worker1_load.insert(1, 1);
+        worker1_load.insert(3, 3);
+        loads.insert(worker1.url().to_string(), worker1_load);
+        manager.update_dp_loads(&loads);
+
+        // Verify that the worker1 with the lowest load is dp_rank = 1
+        assert_eq!(manager.get_lowest_dp_load(&worker1), Some(1));
+    }
+
+    #[test]
+    fn test_load_increment() {
+        let worker2 = BasicWorkerBuilder::new("http://worker2:8080")
+            .worker_type(WorkerType::Regular)
+            .api_key("test_api_key2")
+            .build();
+
+        let manager = DPLoadManager::new();
+        manager.load_increment(&worker2, 0, 5);
+        let cached = manager.dp_cached_loads.read().expect("Rwlock read1 failed");
+        assert!(cached.get(worker2.url()).is_none());
+        drop(cached);
+
+        // insert worker2.load
+        let mut worker2_load = HashMap::new();
+        worker2_load.insert(0, 2);
+        let mut loads = HashMap::new();
+        loads.insert(worker2.url().to_string(), worker2_load);
+        manager.update_dp_loads(&loads);
+
+        // load increment
+        manager.load_increment(&worker2, 0, 5);
+        let cached = manager.dp_cached_loads.read().expect("Rwlock read2 failed");
+        let worker2_cache = cached
+            .get(worker2.url())
+            .expect("worker2 not found in cache");
+        // 2 + 5 = 7
+        assert_eq!(worker2_cache.get(&0), Some(&7));
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
diff --git a/sgl-model-gateway/src/policies/power_of_two.rs b/sgl-model-gateway/src/policies/power_of_two.rs
index 802a51194..6b82cb435 100644
--- a/sgl-model-gateway/src/policies/power_of_two.rs
+++ b/sgl-model-gateway/src/policies/power_of_two.rs
@@ -8,7 +8,7 @@ use std::{
 use rand::Rng;
 use tracing::debug;
 
-use super::{get_healthy_worker_indices, LoadBalancingPolicy};
+use super::{get_healthy_worker_indices, DPLoadManager, LoadBalancingPolicy};
 use crate::core::Worker;
 
 /// Power-of-two choices policy
@@ -19,12 +19,14 @@ use crate::core::Worker;
 pub struct PowerOfTwoPolicy {
     /// Cached load information from external monitoring
     cached_loads: RwLock<HashMap<String, isize>>,
+    dp_load_manager: DPLoadManager,
 }
 
 impl PowerOfTwoPolicy {
     pub fn new() -> Self {
         Self {
             cached_loads: RwLock::new(HashMap::new()),
+            dp_load_manager: DPLoadManager::new(),
         }
     }
 }
@@ -117,6 +119,18 @@ impl LoadBalancingPolicy for PowerOfTwoPolicy {
     fn as_any(&self) -> &dyn std::any::Any {
         self
     }
+
+    fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        self.dp_load_manager.update_dp_loads(loads);
+    }
+
+    fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        self.dp_load_manager.get_lowest_dp_load(worker)
+    }
+
+    fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, tokens: isize) {
+        self.dp_load_manager.load_increment(worker, dp_rank, tokens);
+    }
 }
 
 impl Default for PowerOfTwoPolicy {
diff --git a/sgl-model-gateway/src/policies/random.rs b/sgl-model-gateway/src/policies/random.rs
index 12f0ac1dd..6c1a7dc12 100644
--- a/sgl-model-gateway/src/policies/random.rs
+++ b/sgl-model-gateway/src/policies/random.rs
@@ -1,21 +1,25 @@
 //! Random load balancing policy
 
-use std::sync::Arc;
+use std::{collections::HashMap, sync::Arc};
 
 use rand::Rng;
 
-use super::{get_healthy_worker_indices, LoadBalancingPolicy};
+use super::{get_healthy_worker_indices, DPLoadManager, LoadBalancingPolicy};
 use crate::core::Worker;
 
 /// Random selection policy
 ///
 /// Selects workers randomly with uniform distribution among healthy workers.
 #[derive(Debug, Default)]
-pub struct RandomPolicy;
+pub struct RandomPolicy {
+    dp_load_manager: DPLoadManager,
+}
 
 impl RandomPolicy {
     pub fn new() -> Self {
-        Self
+        Self {
+            dp_load_manager: DPLoadManager::new(),
+        }
     }
 }
 
@@ -44,6 +48,18 @@ impl LoadBalancingPolicy for RandomPolicy {
     fn as_any(&self) -> &dyn std::any::Any {
         self
     }
+
+    fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        self.dp_load_manager.update_dp_loads(loads);
+    }
+
+    fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        self.dp_load_manager.get_lowest_dp_load(worker)
+    }
+
+    fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, tokens: isize) {
+        self.dp_load_manager.load_increment(worker, dp_rank, tokens);
+    }
 }
 
 #[cfg(test)]
diff --git a/sgl-model-gateway/src/policies/registry.rs b/sgl-model-gateway/src/policies/registry.rs
index ee6f0135a..be98e1cf6 100644
--- a/sgl-model-gateway/src/policies/registry.rs
+++ b/sgl-model-gateway/src/policies/registry.rs
@@ -1,4 +1,9 @@
-use std::sync::{Arc, OnceLock};
+use std::{
+    sync::{
+        atomic::{AtomicBool, Ordering},
+        Arc, OnceLock,
+    },
+};
 
 use dashmap::DashMap;
 use tracing::{debug, info, warn};
@@ -32,6 +37,9 @@ pub struct PolicyRegistry {
 
     /// Decode policy for PD mode (set once at startup, lock-free reads via OnceLock)
     decode_policy: Arc<OnceLock<Arc<dyn LoadBalancingPolicy>>>,
+
+    /// Enable minimum tokens scheduler for dp group
+    dp_minimum_tokens_scheduler: Arc<AtomicBool>,
 }
 
 impl PolicyRegistry {
@@ -45,9 +53,19 @@ impl PolicyRegistry {
             default_policy,
             prefill_policy: Arc::new(OnceLock::new()),
             decode_policy: Arc::new(OnceLock::new()),
+            dp_minimum_tokens_scheduler: Arc::new(AtomicBool::new(false)),
         }
     }
 
+    pub fn enable_dp_minimum_tokens_scheduler(&self) {
+        self.dp_minimum_tokens_scheduler
+            .store(true, Ordering::Relaxed);
+    }
+
+    pub fn is_dp_minimum_tokens_scheduler_enabled(&self) -> bool {
+        self.dp_minimum_tokens_scheduler.load(Ordering::Relaxed)
+    }
+
     /// Called when a worker is added
     /// Returns the policy that should be used for this worker's model
     pub fn on_worker_added(
@@ -304,6 +322,40 @@ impl PolicyRegistry {
         power_of_two_policies
     }
 
+    pub fn get_all_policies(&self) -> Vec<Arc<dyn LoadBalancingPolicy>> {
+        let mut all_policies = Vec::new();
+
+        all_policies.push(Arc::clone(&self.default_policy));
+
+        // Get prefill and decode policies (lock-free via OnceLock::get)
+        let prefill_policy_opt = self.prefill_policy.get();
+        let decode_policy_opt = self.decode_policy.get();
+
+        if let Some(policy) = prefill_policy_opt {
+            if !Arc::ptr_eq(policy, &self.default_policy) {
+                all_policies.push(Arc::clone(policy));
+            }
+        }
+
+        if let Some(policy) = decode_policy_opt {
+            if !Arc::ptr_eq(policy, &self.default_policy)
+                && !prefill_policy_opt.is_some_and(|p| Arc::ptr_eq(p, policy))
+            {
+                all_policies.push(Arc::clone(policy));
+            }
+        }
+
+        for entry in self.model_policies.iter() {
+            let policy = entry.value();
+            let already_added = all_policies.iter().any(|p| Arc::ptr_eq(p, policy));
+            if !already_added {
+                all_policies.push(Arc::clone(policy));
+            }
+        }
+
+        all_policies
+    }
+
     /// Initialize cache-aware policy with workers if applicable
     /// This should be called after workers are registered for a model
     pub fn init_cache_aware_policy(&self, model_id: &str, workers: &[Arc<dyn Worker>]) {
diff --git a/sgl-model-gateway/src/policies/round_robin.rs b/sgl-model-gateway/src/policies/round_robin.rs
index 739f18ca6..a3278802d 100644
--- a/sgl-model-gateway/src/policies/round_robin.rs
+++ b/sgl-model-gateway/src/policies/round_robin.rs
@@ -1,11 +1,14 @@
 //! Round-robin load balancing policy
 
-use std::sync::{
-    atomic::{AtomicUsize, Ordering},
-    Arc,
+use std::{
+    collections::HashMap,
+    sync::{
+        atomic::{AtomicUsize, Ordering},
+        Arc,
+    },
 };
 
-use super::{get_healthy_worker_indices, LoadBalancingPolicy};
+use super::{get_healthy_worker_indices, DPLoadManager, LoadBalancingPolicy};
 use crate::core::Worker;
 
 /// Round-robin selection policy
@@ -14,12 +17,14 @@ use crate::core::Worker;
 #[derive(Debug, Default)]
 pub struct RoundRobinPolicy {
     counter: AtomicUsize,
+    dp_load_manager: DPLoadManager,
 }
 
 impl RoundRobinPolicy {
     pub fn new() -> Self {
         Self {
             counter: AtomicUsize::new(0),
+            dp_load_manager: DPLoadManager::new(),
         }
     }
 }
@@ -54,6 +59,18 @@ impl LoadBalancingPolicy for RoundRobinPolicy {
     fn as_any(&self) -> &dyn std::any::Any {
         self
     }
+
+    fn update_dp_loads(&self, loads: &HashMap<String, HashMap<isize, isize>>) {
+        self.dp_load_manager.update_dp_loads(loads);
+    }
+
+    fn get_lowest_dp_load(&self, worker: &dyn Worker) -> Option<isize> {
+        self.dp_load_manager.get_lowest_dp_load(worker)
+    }
+
+    fn load_increment(&self, worker: &dyn Worker, dp_rank: isize, tokens: isize) {
+        self.dp_load_manager.load_increment(worker, dp_rank, tokens);
+    }
 }
 
 #[cfg(test)]
diff --git a/sgl-model-gateway/src/protocols/generate.rs b/sgl-model-gateway/src/protocols/generate.rs
index d5819095a..8abedd7a3 100644
--- a/sgl-model-gateway/src/protocols/generate.rs
+++ b/sgl-model-gateway/src/protocols/generate.rs
@@ -132,6 +132,9 @@ pub struct GenerateRequest {
     #[serde(skip_serializing_if = "Option::is_none")]
     pub data_parallel_rank: Option<i32>,
 
+    #[serde(skip_serializing_if = "Option::is_none")]
+    pub data_parallel_rank_decode: Option<i32>,
+
     /// Background response
     #[serde(default)]
     pub background: bool,
diff --git a/sgl-model-gateway/src/protocols/worker_spec.rs b/sgl-model-gateway/src/protocols/worker_spec.rs
index c8bf79ee9..598e14622 100644
--- a/sgl-model-gateway/src/protocols/worker_spec.rs
+++ b/sgl-model-gateway/src/protocols/worker_spec.rs
@@ -358,4 +358,6 @@ pub struct WorkerLoadInfo {
     pub worker_type: Option<String>,
     /// Current load (-1 indicates failure to fetch)
     pub load: isize,
+    /// Current dp rand load
+    pub dp_rank_loads: HashMap<isize, isize>,
 }
diff --git a/sgl-model-gateway/src/routers/http/pd_router.rs b/sgl-model-gateway/src/routers/http/pd_router.rs
index 6494814b1..e05122cad 100644
--- a/sgl-model-gateway/src/routers/http/pd_router.rs
+++ b/sgl-model-gateway/src/routers/http/pd_router.rs
@@ -333,6 +333,24 @@ impl PDRouter {
                             Err(e) => return Self::handle_serialization_error(e),
                         };
 
+                        if self
+                            .policy_registry
+                            .is_dp_minimum_tokens_scheduler_enabled()
+                        {
+                            // data_parallel_rank
+                            json_request = match self
+                                .select_data_parallel_rank(
+                                    json_request,
+                                    prefill.as_ref(),
+                                    decode.as_ref(),
+                                    context.request_text.as_deref(),
+                                )
+                                .await
+                            {
+                                Ok(v) => v,
+                                Err(e) => return Self::handle_serialization_error(e),
+                            };
+                        }
                         let response = self
                             .execute_dual_dispatch_internal(
                                 headers,
@@ -688,6 +706,59 @@ impl PDRouter {
         prefill_policy.needs_request_text() || decode_policy.needs_request_text()
     }
 
+    async fn select_data_parallel_rank(
+        &self,
+        mut original: Value,
+        prefill_worker: &dyn Worker,
+        decode_worker: &dyn Worker,
+        request_text: Option<&str>,
+    ) -> Result<Value, String> {
+        let obj = original
+            .as_object_mut()
+            .ok_or_else(|| "Request must be a JSON object".to_string())?;
+
+        let length = match request_text {
+            Some(s) => s.len(),
+            None => 0,
+        };
+        let prefill_policy = self.policy_registry.get_prefill_policy();
+        let lowest_prefill_dp_rank = prefill_policy.get_lowest_dp_load(prefill_worker);
+        let decode_policy = self.policy_registry.get_decode_policy();
+        let lowest_decode_dp_rank = decode_policy.get_lowest_dp_load(decode_worker);
+        obj.insert(
+            "data_parallel_rank".to_string(),
+            match lowest_prefill_dp_rank {
+                Some(v) => Value::from(v),
+                None => Value::Null,
+            },
+        );
+        // During the prefill and decode stages, requests may be scheduled to different dp_rank
+        // data_parallel_rank_decode specifies which dp_rank the request should be scheduled to for decode
+        // data_parallel_rank specifies which dp_rank the request is in for preill
+        obj.insert(
+            "data_parallel_rank_decode".to_string(),
+            match lowest_decode_dp_rank {
+                Some(v) => Value::from(v),
+                None => Value::Null,
+            },
+        );
+        let prompt_len = length
+            .try_into()
+            .map_err(|e| format!("Failed to convert length tp isize:{}", e))?;
+        debug!(
+            "select_data_parallel_rank obj:{:?}, prompt_len:{}",
+            obj, prompt_len
+        );
+        if let Some(dp_rank) = lowest_prefill_dp_rank {
+            prefill_policy.load_increment(prefill_worker, dp_rank, prompt_len);
+        }
+        if let Some(dp_rank) = lowest_decode_dp_rank {
+            decode_policy.load_increment(decode_worker, dp_rank, prompt_len);
+        }
+
+        Ok(original)
+    }
+
     async fn select_pd_pair(
         &self,
         request_text: Option<&str>,
diff --git a/sgl-model-gateway/tests/test_openai_routing.rs b/sgl-model-gateway/tests/test_openai_routing.rs
index 282522d5e..f586e878c 100644
--- a/sgl-model-gateway/tests/test_openai_routing.rs
+++ b/sgl-model-gateway/tests/test_openai_routing.rs
@@ -630,6 +630,7 @@ async fn test_unsupported_endpoints() {
         bootstrap_room: None,
         bootstrap_pair_key: None,
         data_parallel_rank: None,
+        data_parallel_rank_decode: None,
         background: false,
         conversation_id: None,
         priority: None,
@@ -813,6 +814,7 @@ async fn test_openai_router_circuit_breaker() {
         assert!(
             response.status() == StatusCode::INTERNAL_SERVER_ERROR
                 || response.status() == StatusCode::SERVICE_UNAVAILABLE
+                || response.status() == StatusCode::GATEWAY_TIMEOUT
         );
     }
 }
diff --git a/test/manual/lora/__init__.py b/test/manual/lora/__init__.py
new file mode 100644
index 000000000..dc2249b96
--- /dev/null
+++ b/test/manual/lora/__init__.py
@@ -0,0 +1,6 @@
+from utils import reference_sgmv_expand, reference_sgmv_shrink
+
+__all__ = [
+    "reference_sgmv_shrink",
+    "reference_sgmv_expand",
+]
diff --git a/test/manual/lora/test_chunked_sgmv_backend.py b/test/manual/lora/test_chunked_sgmv_backend.py
index 2cfde12db..385241157 100644
--- a/test/manual/lora/test_chunked_sgmv_backend.py
+++ b/test/manual/lora/test_chunked_sgmv_backend.py
@@ -1,9 +1,10 @@
 import random
 import unittest
 from enum import Enum
-from typing import Dict, List, Optional, Tuple
+from typing import List, Optional, Tuple
 
 import torch
+from utils import reference_sgmv_expand, reference_sgmv_shrink
 
 from sglang.srt.lora.backend.chunked_backend import ChunkedSgmvLoRABackend
 from sglang.srt.lora.triton_ops import (
@@ -22,12 +23,6 @@ def reset_kernel_cache():
     _chunked_lora_expand_kernel._clear_cache()
 
 
-def safe_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
-    """Matrix multiplication with mixed precision handling for float16"""
-    result = torch.matmul(a.float(), b.float())
-    return result.to(a.dtype)
-
-
 class BatchComposition(Enum):
     UNIFORM = "uniform"
     MIXED = "mixed"
@@ -38,150 +33,7 @@ class BatchComposition(Enum):
 class BatchMode(Enum):
     PREFILL = "prefill"
     DECODE = "decode"
-
-
-def reference_sgmv_shrink(
-    x: torch.Tensor,
-    weights: torch.Tensor,
-    batch_info: LoRABatchInfo,
-    seq_lengths: List[int],
-    lora_assignments: List[str],
-    num_slices: int = 1,
-) -> torch.Tensor:
-    """
-    Simple sequence-level reference implementation of SGMV shrink operation.
-
-    Args:
-        x: (total_seq_len, input_dim) - Input activations
-        weights: (num_loras, num_slices * max_rank, input_dim) - LoRA A weights
-        batch_info: Batch information (only used for lora_ranks)
-        seq_lengths: Length of each sequence
-        lora_assignments: LoRA name for each sequence
-        num_slices: Number of slices (3 for QKV, 2 for gate_up, 1 for others)
-
-    Returns:
-        output: (total_seq_len, num_slices * max_rank) - Intermediate activations
-    """
-    if weights.numel() == 0:
-        total_seq_len = x.shape[0]
-        return torch.zeros(total_seq_len, 0, dtype=x.dtype, device=x.device)
-
-    total_seq_len, input_dim = x.shape
-    num_loras, weight_out_dim, _ = weights.shape
-    max_rank = weight_out_dim // num_slices
-
-    output = torch.zeros(
-        total_seq_len, num_slices * max_rank, dtype=x.dtype, device=x.device
-    )
-
-    unique_loras = sorted(set(lora_assignments))
-    lora_name_to_idx = {name: idx for idx, name in enumerate(unique_loras)}
-    lora_ranks = batch_info.lora_ranks.cpu().numpy()
-
-    token_offset = 0
-    for seq_len, lora_name in zip(seq_lengths, lora_assignments):
-        if seq_len == 0:
-            continue
-
-        lora_idx = lora_name_to_idx[lora_name]
-        rank = lora_ranks[lora_idx]
-
-        if rank > 0:
-            x_seq = x[token_offset : token_offset + seq_len, :]
-            w_seq = weights[lora_idx, : num_slices * rank, :]
-
-            result = safe_matmul(x_seq, w_seq.t())
-            output[token_offset : token_offset + seq_len, : num_slices * rank] = result
-
-        token_offset += seq_len
-
-    return output
-
-
-def reference_sgmv_expand(
-    x: torch.Tensor,
-    weights: torch.Tensor,
-    batch_info: LoRABatchInfo,
-    seq_lengths: List[int],
-    lora_assignments: List[str],
-    slice_offsets: torch.Tensor,
-    max_slice_size: int,
-    base_output: Optional[torch.Tensor] = None,
-) -> torch.Tensor:
-    """
-    Simple sequence-level reference implementation of SGMV expand operation.
-
-    Args:
-        x: (total_seq_len, num_slices * max_rank) - Intermediate activations
-        weights: (num_loras, output_dim, max_rank) - LoRA B weights
-        batch_info: Batch information (only used for lora_ranks)
-        seq_lengths: Length of each sequence
-        lora_assignments: LoRA name for each sequence
-        slice_offsets: Tensor defining slice boundaries
-        max_slice_size: Maximum slice size for chunking
-        base_output: Optional base output to accumulate into
-
-    Returns:
-        output: (total_seq_len, total_output_dim) - Final output
-    """
-    if weights.numel() == 0:
-        total_seq_len = x.shape[0]
-        total_output_dim = slice_offsets[-1].item() if len(slice_offsets) > 0 else 0
-        return torch.zeros(
-            total_seq_len, total_output_dim, dtype=x.dtype, device=x.device
-        )
-
-    total_seq_len, _ = x.shape
-
-    num_slices = len(slice_offsets) - 1
-
-    if base_output is not None:
-        output = base_output.clone()
-    else:
-        total_output_dim = slice_offsets[-1].item()
-        output = torch.zeros(
-            total_seq_len, total_output_dim, dtype=x.dtype, device=x.device
-        )
-
-    unique_loras = sorted(set(lora_assignments))
-    lora_name_to_idx = {name: idx for idx, name in enumerate(unique_loras)}
-    lora_ranks = batch_info.lora_ranks.cpu().numpy()
-
-    token_offset = 0
-    for seq_len, lora_name in zip(seq_lengths, lora_assignments):
-        if seq_len == 0:
-            continue
-
-        lora_idx = lora_name_to_idx[lora_name]
-        lora_rank = lora_ranks[lora_idx]
-
-        if lora_rank > 0:
-            # Extract sequence intermediate activations
-            x_seq = x[
-                token_offset : token_offset + seq_len, : num_slices * lora_rank
-            ]  # (seq_len, num_slices * rank)
-
-            for slice_idx in range(num_slices):
-                slice_start_input = slice_idx * lora_rank
-                slice_end_input = (slice_idx + 1) * lora_rank
-
-                slice_start_output = slice_offsets[slice_idx].item()
-                slice_end_output = slice_offsets[slice_idx + 1].item()
-
-                x_slice = x_seq[:, slice_start_input:slice_end_input]  # (seq_len, rank)
-                w_slice = weights[
-                    lora_idx, slice_start_output:slice_end_output, :lora_rank
-                ]  # (slice_dim, rank)
-
-                result = safe_matmul(x_slice, w_slice.t())  # (seq_len, slice_dim)
-                output[
-                    token_offset : token_offset + seq_len,
-                    slice_start_output:slice_end_output,
-                ] += result
-
-        token_offset += seq_len
-
-    return output
+    TARGET_VERIFY = "verify"
 
 
 class TestChunkedSGMV(unittest.TestCase):
@@ -196,7 +48,7 @@ class TestChunkedSGMV(unittest.TestCase):
         chunked_output: torch.Tensor,
         reference_output: torch.Tensor,
         seq_lengths: List[int],
-        lora_assignments: List[str],
+        lora_assignments: List[int],
         batch_info: LoRABatchInfo,
         num_slices: int,
         test_name: str,
@@ -207,19 +59,15 @@ class TestChunkedSGMV(unittest.TestCase):
         The chunked SGMV shrink kernel only guarantees correctness for
         output[seq_start:seq_end, :rank * num_slices] for each sequence.
         """
-        # Create mapping from LoRA names to indices and ranks
-        unique_loras = sorted(set(lora_assignments))
-        lora_name_to_idx = {name: idx for idx, name in enumerate(unique_loras)}
         lora_ranks = batch_info.lora_ranks.cpu().numpy()
 
         token_offset = 0
-        for seq_idx, (seq_len, lora_name) in enumerate(
-            zip(seq_lengths, lora_assignments)
+        for seq_idx, (lora_idx, seq_len) in enumerate(
+            zip(lora_assignments, seq_lengths)
         ):
             if seq_len == 0:
                 continue
 
-            lora_idx = lora_name_to_idx[lora_name]
             rank = lora_ranks[lora_idx]
 
             if rank > 0:
@@ -238,7 +86,7 @@ class TestChunkedSGMV(unittest.TestCase):
                     reference_seq,
                     rtol=self.RTOL,
                     atol=self.ATOL,
-                    msg=f"Shrink operation failed for {test_name}, sequence {seq_idx} ({lora_name})",
+                    msg=f"Shrink operation failed for {test_name}, sequence {seq_idx} ({lora_idx})",
                 )
 
             token_offset += seq_len
@@ -318,23 +166,22 @@ class TestChunkedSGMV(unittest.TestCase):
 
     def create_batch_info(
         self,
+        lora_names: List[str],
         seq_lengths: List[int],
-        lora_assignments: List[Optional[str]],
+        lora_assignments: List[Optional[int]],
         batch_mode: BatchMode = BatchMode.PREFILL,
     ) -> LoRABatchInfo:
         """Create LoRABatchInfo using the same logic as chunked backend"""
-        unique_loras = sorted(set(lora_assignments))
-        lora_name_to_idx = {name: idx for idx, name in enumerate(unique_loras)}
-
-        seq_weight_indices = [lora_name_to_idx[name] for name in lora_assignments]
-
-        lora_ranks = [self.lora_configs[name][0] for name in unique_loras]
+        lora_ranks = [self.lora_configs[name][0] for name in lora_names]
 
         def create_mock_batch():
             # Create a minimal mock ForwardBatch for the test
             class MockForwardBatch:
-                def __init__(self, batch_size, seq_lengths):
+                def __init__(self, batch_size, seq_lengths, device):
                     self.batch_size = batch_size
+                    self.extend_seq_lens = torch.tensor(
+                        seq_lengths, dtype=torch.int32, device=device
+                    )
                     self.extend_seq_lens_cpu = seq_lengths
                     self.forward_mode = MockForwardMode()
 
@@ -342,13 +189,22 @@ class TestChunkedSGMV(unittest.TestCase):
                 def is_extend(self):
                     return batch_mode == BatchMode.PREFILL
 
-            return MockForwardBatch(len(seq_lengths), seq_lengths)
+                def is_decode(self):
+                    return batch_mode == BatchMode.DECODE
+
+                def is_target_verify(self):
+                    return batch_mode == BatchMode.TARGET_VERIFY
+
+                def is_prefill(self):
+                    return self.is_extend()
+
+            return MockForwardBatch(len(seq_lengths), seq_lengths, self.device)
 
         mock_batch = create_mock_batch()
 
         # Use the same functions as chunked backend
         permutation, weights_reordered = ChunkedSgmvLoRABackend._get_permutation(
-            seq_weight_indices, mock_batch
+            lora_assignments, mock_batch
         )
 
         # Create a minimal backend instance to access _get_segments_info
@@ -363,7 +219,7 @@ class TestChunkedSGMV(unittest.TestCase):
             chunk_size=CHUNK_SIZE,
         )
 
-        scalings = [1.0] * len(unique_loras)
+        scalings = [1.0] * len(lora_names)
         seg_indptr_tensor = seg_indptr.to(self.device)
         weight_indices_tensor = weight_indices_list.to(self.device)
         lora_ranks_tensor = (
@@ -437,7 +293,7 @@ class TestChunkedSGMV(unittest.TestCase):
         include_missing_k: bool = False,
     ) -> Tuple[
         torch.Tensor,
-        Dict[str, Tuple[torch.Tensor, torch.Tensor]],
+        List[Tuple[torch.Tensor, torch.Tensor]],
         LoRABatchInfo,
         List[int],
         List[str],
@@ -451,20 +307,21 @@ class TestChunkedSGMV(unittest.TestCase):
             batch_size, batch_mode, 1, self.max_seq_len
         )
         if batch_composition == BatchComposition.UNIFORM:
-            lora_assignments = ["lora_A"] * batch_size
+            lora_names = ["lora_A"]
+            lora_assignments = [lora_names.index("lora_A")] * batch_size
         elif batch_composition == BatchComposition.MIXED:
             lora_names = ["lora_A", "lora_B", "lora_C", None]
-            lora_assignments = [
-                lora_names[i % len(lora_names)] for i in range(batch_size)
-            ]
+            lora_assignments = [(i % len(lora_names)) for i in range(batch_size)]
         elif batch_composition == BatchComposition.SKEWED:
+            lora_names = ["lora_A", "lora_B"]
             num_minority = max(1, batch_size // 8)
-            lora_assignments = ["lora_A"] * num_minority + ["lora_B"] * (
-                batch_size - num_minority
-            )
+            lora_assignments = [lora_names.index("lora_A")] * num_minority + [
+                lora_names.index("lora_B")
+            ] * (batch_size - num_minority)
             random.shuffle(lora_assignments)
         elif batch_composition == BatchComposition.NONE:
-            lora_assignments = [None] * batch_size
+            lora_names = [None]
+            lora_assignments = [0] * batch_size
         else:
             raise ValueError(f"Unknown batch composition: {batch_composition}")
 
@@ -473,39 +330,45 @@ class TestChunkedSGMV(unittest.TestCase):
             total_seq_len, self.input_dim, dtype=self.dtype, device=self.device
         )
 
-        normalized_assignments = [
-            name if name is not None else "_NO_LORA_" for name in lora_assignments
+        normalized_lora_names = [
+            "_NO_LORA_" if name is None else name for name in lora_names
         ]
-        unique_loras = set(normalized_assignments)
-        weights = {}
-        for lora_name in unique_loras:
-            weights[lora_name] = self.create_lora_weights(lora_name, include_missing_k)
+        weights = []
+        for lora_name in normalized_lora_names:
+            weights.append(self.create_lora_weights(lora_name, include_missing_k))
 
         batch_info = self.create_batch_info(
-            seq_lengths, normalized_assignments, batch_mode
+            normalized_lora_names, seq_lengths, lora_assignments, batch_mode
         )
 
-        return x, weights, batch_info, seq_lengths, normalized_assignments
+        return x, weights, batch_info, seq_lengths, lora_assignments
 
     def run_test_comparison(
         self,
         x: torch.Tensor,
-        weights: Dict[str, Tuple[torch.Tensor, torch.Tensor]],
+        weights: List[Tuple[torch.Tensor, torch.Tensor]],
         batch_info: LoRABatchInfo,
         seq_lengths: List[int],
-        lora_assignments: List[str],
+        lora_assignments: List[int],
         test_name: str,
     ):
         """Run comparison between chunked and reference implementations"""
         if not weights:  # Handle case with no LoRA weights
             return
 
+        lora_assignments_tensor = torch.tensor(
+            lora_assignments, dtype=torch.int32, device="cpu"
+        )
+        seq_lengths_tensor = torch.tensor(seq_lengths, dtype=torch.int32, device="cpu")
+        lora_ranks_tensor = batch_info.lora_ranks.detach().cpu()
+        scalings_tensor = batch_info.scalings.detach().cpu()
+
         # Stack LoRA A weights
-        lora_a_weights = [weights[name][0] for name in sorted(weights.keys())]
+        lora_a_weights = [weight[0] for weight in weights]
         stacked_lora_a = self.stack_lora_weights(lora_a_weights, is_lora_a=True)
 
         # Stack LoRA B weights
-        lora_b_weights = [weights[name][1] for name in sorted(weights.keys())]
+        lora_b_weights = [weight[1] for weight in weights]
         stacked_lora_b = self.stack_lora_weights(lora_b_weights, is_lora_a=False)
 
         # Test shrink operation
@@ -513,7 +376,13 @@ class TestChunkedSGMV(unittest.TestCase):
             x, stacked_lora_a, batch_info, num_slices=3
         )
         reference_shrink = reference_sgmv_shrink(
-            x, stacked_lora_a, batch_info, seq_lengths, lora_assignments, num_slices=3
+            x,
+            stacked_lora_a,
+            lora_assignments_tensor,
+            seq_lengths_tensor,
+            lora_ranks_tensor,
+            scalings_tensor,
+            num_slices=3,
         )
 
         # Only compare valid portions of shrink output (first rank * num_slices columns per sequence)
@@ -539,11 +408,10 @@ class TestChunkedSGMV(unittest.TestCase):
         reference_expand = reference_sgmv_expand(
             reference_shrink,
             stacked_lora_b,
-            batch_info,
-            seq_lengths,
-            lora_assignments,
+            lora_assignments_tensor,
+            seq_lengths_tensor,
+            lora_ranks_tensor,
             self.slice_offsets,
-            self.max_slice_size,
         )
 
         torch.testing.assert_close(
@@ -564,7 +432,16 @@ class TestChunkedSGMV(unittest.TestCase):
                     self.create_test_batch(BatchComposition.UNIFORM, batch_size)
                 )
 
-                lora_a_weights = [weights[name][0] for name in sorted(weights.keys())]
+                lora_assignments_tensor = torch.tensor(
+                    lora_assignments, dtype=torch.int32, device="cpu"
+                )
+                seq_lengths_tensor = torch.tensor(
+                    seq_lengths, dtype=torch.int32, device="cpu"
+                )
+                lora_ranks_tensor = batch_info.lora_ranks.detach().cpu()
+                scalings_tensor = batch_info.scalings.detach().cpu()
+
+                lora_a_weights = [weight[0] for weight in weights]
                 stacked_lora_a = self.stack_lora_weights(lora_a_weights, is_lora_a=True)
 
                 chunked_shrink = chunked_sgmv_lora_shrink_forward(
@@ -573,9 +450,10 @@ class TestChunkedSGMV(unittest.TestCase):
                 reference_shrink = reference_sgmv_shrink(
                     x,
                     stacked_lora_a,
-                    batch_info,
-                    seq_lengths,
-                    lora_assignments,
+                    lora_assignments_tensor,
+                    seq_lengths_tensor,
+                    lora_ranks_tensor,
+                    scalings_tensor,
                     num_slices=3,
                 )
 
@@ -591,19 +469,29 @@ class TestChunkedSGMV(unittest.TestCase):
                     self.create_test_batch(BatchComposition.UNIFORM, batch_size)
                 )
 
-                lora_a_weights = [weights[name][0] for name in sorted(weights.keys())]
+                lora_assignments_tensor = torch.tensor(
+                    lora_assignments, dtype=torch.int32, device="cpu"
+                )
+                seq_lengths_tensor = torch.tensor(
+                    seq_lengths, dtype=torch.int32, device="cpu"
+                )
+                lora_ranks_tensor = batch_info.lora_ranks.detach().cpu()
+                scalings_tensor = batch_info.scalings.detach().cpu()
+
+                lora_a_weights = [weight[0] for weight in weights]
                 stacked_lora_a = self.stack_lora_weights(lora_a_weights, is_lora_a=True)
 
                 intermediate = reference_sgmv_shrink(
                     x,
                     stacked_lora_a,
-                    batch_info,
-                    seq_lengths,
-                    lora_assignments,
+                    lora_assignments_tensor,
+                    seq_lengths_tensor,
+                    lora_ranks_tensor,
+                    scalings_tensor,
                     num_slices=3,
                 )
 
-                lora_b_weights = [weights[name][1] for name in sorted(weights.keys())]
+                lora_b_weights = [weight[1] for weight in weights]
                 stacked_lora_b = self.stack_lora_weights(
                     lora_b_weights, is_lora_a=False
                 )
@@ -619,11 +507,10 @@ class TestChunkedSGMV(unittest.TestCase):
                 reference_expand = reference_sgmv_expand(
                     intermediate,
                     stacked_lora_b,
-                    batch_info,
-                    seq_lengths,
-                    lora_assignments,
+                    lora_assignments_tensor,
+                    seq_lengths_tensor,
+                    lora_ranks_tensor,
                     self.slice_offsets,
-                    self.max_slice_size,
                 )
 
                 torch.testing.assert_close(
diff --git a/test/manual/lora/test_lora_cuda_graph.py b/test/manual/lora/test_lora_cuda_graph.py
index 221f2b660..c110d4144 100644
--- a/test/manual/lora/test_lora_cuda_graph.py
+++ b/test/manual/lora/test_lora_cuda_graph.py
@@ -17,7 +17,7 @@ import os
 import unittest
 from typing import List
 
-from lora_utils import (
+from sglang.test.lora_utils import (
     ALL_OTHER_LORA_MODELS,
     CI_LORA_MODELS,
     DEFAULT_PROMPTS,
@@ -26,7 +26,6 @@ from lora_utils import (
     run_lora_test_by_batch,
     run_lora_test_one_by_one,
 )
-
 from sglang.test.test_utils import CustomTestCase, is_in_ci
 
 TEST_CUDA_GRAPH_PADDING_PROMPTS = [
diff --git a/test/manual/lora/test_lora_ops.py b/test/manual/lora/test_lora_ops.py
new file mode 100644
index 000000000..63c88680d
--- /dev/null
+++ b/test/manual/lora/test_lora_ops.py
@@ -0,0 +1,218 @@
+import random
+import unittest
+
+import torch
+from utils import reference_sgmv_expand, reference_sgmv_shrink
+
+from sglang.srt.lora.torch_ops.lora_ops import sgemm_lora_a_fwd, sgemm_lora_b_fwd
+from sglang.test.test_utils import CustomTestCase
+
+
+class TestLoraOps(CustomTestCase):
+    def test_sgemm_lora_a_fwd(self):
+        batch_size = 2
+        input_dim = 1024
+        num_loras = 3
+        dtype = torch.float32
+
+        possible_lora_ranks = [8, 16, 32, 64, 128, 256]
+        lora_ranks = random.sample(
+            possible_lora_ranks,
+            counts=[num_loras] * len(possible_lora_ranks),
+            k=num_loras,
+        )
+
+        max_lora_rank = max(lora_ranks)
+
+        possible_lora_scaling = [0.25, 0.5, 1.0, 2.0, 4.0]
+        lora_scaling = random.sample(
+            possible_lora_scaling,
+            counts=[num_loras] * len(possible_lora_scaling),
+            k=num_loras,
+        )
+
+        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
+        lora_a_weights = torch.randn(num_loras, max_lora_rank, input_dim, dtype=dtype)
+        lora_indices_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+        seq_len_tensor = torch.ones(batch_size, dtype=torch.int32, device="cpu")
+        lora_ranks_tensor = torch.tensor(lora_ranks, dtype=torch.int32, device="cpu")
+        lora_scaling_tensor = torch.tensor(
+            lora_scaling, dtype=torch.float16, device="cpu"
+        )
+
+        expect_output = reference_sgmv_shrink(
+            inputs,
+            lora_a_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            lora_scaling_tensor,
+        )
+
+        actual_output = sgemm_lora_a_fwd(
+            inputs,
+            lora_a_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            lora_scaling_tensor,
+        )
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_sgemm_lora_b_fwd(self):
+        batch_size = 2
+        output_dim = 1024
+        num_loras = 3
+        dtype = torch.float32
+
+        possible_lora_ranks = [8, 16, 32, 64, 128, 256]
+        lora_ranks = random.sample(
+            possible_lora_ranks,
+            counts=[num_loras] * len(possible_lora_ranks),
+            k=num_loras,
+        )
+
+        max_lora_rank = max(lora_ranks)
+
+        inputs = torch.randn(batch_size, max_lora_rank, dtype=dtype)
+        lora_b_weights = torch.randn(num_loras, output_dim, max_lora_rank, dtype=dtype)
+        lora_ranks_tensor = torch.tensor(lora_ranks, dtype=torch.int32, device="cpu")
+        seq_len_tensor = torch.ones(batch_size, dtype=torch.int32, device="cpu")
+        lora_indices_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+        slice_offsets = torch.tensor([0, output_dim], dtype=torch.int32, device="cpu")
+
+        expect_output = reference_sgmv_expand(
+            inputs,
+            lora_b_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets,
+        )
+
+        actual_output = sgemm_lora_b_fwd(
+            inputs,
+            lora_b_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets,
+        )
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_sgemm_lora_a_fwd_expand(self):
+        batch_size = 2
+        input_dim = 1024
+        num_loras = 3
+        dtype = torch.float32
+
+        possible_lora_ranks = [8, 16, 32, 64, 128, 256]
+        lora_ranks = random.sample(
+            possible_lora_ranks,
+            counts=[num_loras] * len(possible_lora_ranks),
+            k=num_loras,
+        )
+
+        max_lora_rank = max(lora_ranks)
+
+        possible_lora_scaling = [0.25, 0.5, 1.0, 2.0, 4.0]
+        lora_scaling = random.sample(
+            possible_lora_scaling,
+            counts=[num_loras] * len(possible_lora_scaling),
+            k=num_loras,
+        )
+
+        seq_len_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+
+        seq_len = sum(seq_len_tensor)
+
+        inputs = torch.randn(seq_len, input_dim, dtype=dtype)
+        lora_a_weights = torch.randn(num_loras, max_lora_rank, input_dim, dtype=dtype)
+        lora_indices_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+        lora_ranks_tensor = torch.tensor(lora_ranks, dtype=torch.int32, device="cpu")
+        lora_scaling_tensor = torch.tensor(
+            lora_scaling, dtype=torch.float16, device="cpu"
+        )
+
+        expect_output = reference_sgmv_shrink(
+            inputs,
+            lora_a_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            lora_scaling_tensor,
+        )
+
+        actual_output = sgemm_lora_a_fwd(
+            inputs,
+            lora_a_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            lora_scaling_tensor,
+        )
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_sgemm_lora_b_fwd_expand(self):
+        batch_size = 2
+        output_dim = 1024
+        num_loras = 3
+        dtype = torch.float32
+
+        possible_lora_ranks = [8, 16, 32, 64, 128, 256]
+        lora_ranks = random.sample(
+            possible_lora_ranks,
+            counts=[num_loras] * len(possible_lora_ranks),
+            k=num_loras,
+        )
+
+        max_lora_rank = max(lora_ranks)
+
+        seq_len_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+
+        seq_len = sum(seq_len_tensor)
+
+        inputs = torch.randn(seq_len, max_lora_rank, dtype=dtype)
+        lora_b_weights = torch.randn(num_loras, output_dim, max_lora_rank, dtype=dtype)
+        lora_ranks_tensor = torch.tensor(lora_ranks, dtype=torch.int32, device="cpu")
+        lora_indices_tensor = torch.randint(
+            num_loras, (batch_size,), dtype=torch.int32, device="cpu"
+        )
+        slice_offsets = torch.tensor([0, output_dim], dtype=torch.int32, device="cpu")
+
+        expect_output = reference_sgmv_expand(
+            inputs,
+            lora_b_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets,
+        )
+
+        actual_output = sgemm_lora_b_fwd(
+            inputs,
+            lora_b_weights,
+            lora_indices_tensor,
+            seq_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets,
+        )
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/test/manual/lora/test_lora_qwen3_vl.py b/test/manual/lora/test_lora_qwen3_vl.py
index 93afc2913..cef364991 100644
--- a/test/manual/lora/test_lora_qwen3_vl.py
+++ b/test/manual/lora/test_lora_qwen3_vl.py
@@ -2,10 +2,14 @@ import random
 import unittest
 from typing import Sequence
 
-from lora_utils import TORCH_DTYPES, LoRAAdaptor, LoRAModelCase, ensure_reproducibility
-
 from sglang.srt.models.qwen3_vl import Qwen3VLForConditionalGeneration
 from sglang.srt.models.qwen3_vl_moe import Qwen3VLMoeForConditionalGeneration
+from sglang.test.lora_utils import (
+    TORCH_DTYPES,
+    LoRAAdaptor,
+    LoRAModelCase,
+    ensure_reproducibility,
+)
 from sglang.test.runners import HFRunner, SRTRunner
 from sglang.test.test_utils import CustomTestCase, calculate_rouge_l
 
diff --git a/test/manual/lora/test_lora_spec_decoding.py b/test/manual/lora/test_lora_spec_decoding.py
index f1970c727..e00fafa75 100644
--- a/test/manual/lora/test_lora_spec_decoding.py
+++ b/test/manual/lora/test_lora_spec_decoding.py
@@ -15,13 +15,12 @@
 import multiprocessing as mp
 import unittest
 
-from lora_utils import (
+from sglang.test.lora_utils import (
     CI_MULTI_LORA_MODELS,
     LoRAAdaptor,
     LoRAModelCase,
     run_lora_multiple_batch_on_model_cases,
 )
-
 from sglang.test.test_utils import CustomTestCase
 
 LORA_MODELS_QWEN3 = [
diff --git a/test/manual/lora/test_torch_backend.py b/test/manual/lora/test_torch_backend.py
new file mode 100644
index 000000000..4de23ffc9
--- /dev/null
+++ b/test/manual/lora/test_torch_backend.py
@@ -0,0 +1,244 @@
+import unittest
+
+import torch
+from utils import reference_sgmv_expand, reference_sgmv_shrink
+
+from sglang.srt.lora.backend.torch_backend import TorchNativeLoRABackend
+from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode
+from sglang.test.test_utils import CustomTestCase
+
+
+class TestTorchNativeLoRABackend(CustomTestCase):
+
+    device = "cpu"
+    weight_indices = [0, 1]
+    lora_ranks = [1, 1]
+    scalings = [1.0, 0.5]
+    seq_lens = [1, 1]
+    use_cuda_graph = False
+
+    forward_batch = ForwardBatch(
+        forward_mode=ForwardMode.EXTEND,
+        batch_size=2,
+        input_ids=torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32),
+        req_pool_indices=None,
+        seq_lens=None,
+        out_cache_loc=None,
+        seq_lens_sum=6,
+        extend_seq_lens=torch.tensor(seq_lens, dtype=torch.int32),
+        extend_seq_lens_cpu=seq_lens,
+    )
+
+    @classmethod
+    def setUpClass(cls):
+        cls.backend = TorchNativeLoRABackend(max_loras_per_batch=2, device=cls.device)
+        cls.backend.prepare_lora_batch(
+            forward_batch=cls.forward_batch,
+            weight_indices=cls.weight_indices,
+            lora_ranks=cls.lora_ranks,
+            scalings=cls.scalings,
+            use_cuda_graph=cls.use_cuda_graph,
+        )
+
+    def test_run_lora_a_sgemm(self):
+        batch_size = 2
+        input_dim = 4
+        output_dim = 6
+        num_loras = 3
+        dtype = torch.float32
+
+        x = torch.randn(batch_size, input_dim, dtype=dtype)
+        weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
+
+        weight_indices_tensor = torch.tensor(
+            self.weight_indices, dtype=torch.int32, device=self.device
+        )
+
+        seg_len_tensor = torch.tensor(
+            self.seq_lens, dtype=torch.int32, device=self.device
+        )
+
+        lora_ranks_tensor = torch.tensor(
+            self.lora_ranks, dtype=torch.int32, device=self.device
+        )
+
+        scalings_tensor = torch.tensor(
+            self.scalings, dtype=torch.float, device=self.device
+        )
+
+        expect_output = reference_sgmv_shrink(
+            x,
+            weights,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            scalings_tensor,
+        )
+
+        actual_output = self.backend.run_lora_a_sgemm(x, weights)
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_run_lora_b_sgemm(self):
+        batch_size = 2
+        input_dim = 6
+        output_dim = 4
+        num_loras = 3
+        dtype = torch.float32
+
+        x = torch.randn(batch_size, input_dim, dtype=dtype)
+        weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
+        _, weight_out_dim, _ = weights.shape
+
+        weight_indices_tensor = torch.tensor(
+            self.weight_indices, dtype=torch.int32, device=self.device
+        )
+
+        seg_len_tensor = torch.tensor(
+            self.seq_lens, dtype=torch.int32, device=self.device
+        )
+
+        lora_ranks_tensor = torch.tensor(
+            self.lora_ranks, dtype=torch.int32, device=self.device
+        )
+
+        expect_output = reference_sgmv_expand(
+            x,
+            weights,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets=torch.tensor(
+                [0, weight_out_dim], dtype=torch.int32, device="cpu"
+            ),
+        )
+
+        actual_output = self.backend.run_lora_b_sgemm(x, weights)
+
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_run_qkv_lora(self):
+        batch_size = 2
+        num_loras = 3
+        input_dim = 6
+        output_offset = [0, 3, 6, 9, 12]
+        output_dim = output_offset[-1]
+        num_slices = len(output_offset) - 1
+        max_lora_rank = max(self.lora_ranks)
+        dtype = torch.float32
+
+        x = torch.randn(batch_size, input_dim, dtype=dtype)
+        output_offset_cpu = torch.tensor(output_offset, dtype=torch.int32)
+        qkv_lora_a = torch.randn(
+            num_loras, max_lora_rank * num_slices, input_dim, dtype=dtype
+        )
+        qkv_lora_b = torch.randn(
+            num_loras, output_dim, max_lora_rank * num_slices, dtype=dtype
+        )
+
+        weight_indices_tensor = torch.tensor(
+            self.weight_indices, dtype=torch.int32, device=self.device
+        )
+
+        seg_len_tensor = torch.tensor(
+            self.seq_lens, dtype=torch.int32, device=self.device
+        )
+
+        lora_ranks_tensor = torch.tensor(
+            self.lora_ranks, dtype=torch.int32, device=self.device
+        )
+
+        scalings_tensor = torch.tensor(
+            self.scalings, dtype=torch.float, device=self.device
+        )
+
+        expect_lora_a_output = reference_sgmv_shrink(
+            x,
+            qkv_lora_a,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            scalings_tensor,
+            num_slices,
+        )
+
+        expect_output = reference_sgmv_expand(
+            expect_lora_a_output,
+            qkv_lora_b,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            output_offset_cpu,
+        )
+
+        actual_output = self.backend.run_qkv_lora(
+            x, qkv_lora_a, qkv_lora_b, None, output_offset_cpu, 0
+        )
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+    def test_run_gate_up_lora(self):
+        batch_size = 2
+        input_dim = 6
+        output_dim = 4
+        num_loras = 3
+        dtype = torch.float32
+
+        max_lora_rank = max(self.lora_ranks)
+
+        num_slices = 2
+
+        x = torch.randn(batch_size, input_dim, dtype=dtype)
+        gate_up_lora_a = torch.randn(
+            num_loras, max_lora_rank * num_slices, input_dim, dtype=dtype
+        )
+        gate_up_lora_b = torch.randn(
+            num_loras, output_dim, max_lora_rank * num_slices, dtype=dtype
+        )
+
+        _, weight_out_dim, _ = gate_up_lora_b.shape
+        slice_size = weight_out_dim // num_slices
+        output_offset = torch.tensor(
+            [0, slice_size, weight_out_dim], dtype=torch.int32, device="cpu"
+        )
+
+        weight_indices_tensor = torch.tensor(
+            self.weight_indices, dtype=torch.int32, device=self.device
+        )
+
+        seg_len_tensor = torch.tensor(
+            self.seq_lens, dtype=torch.int32, device=self.device
+        )
+
+        lora_ranks_tensor = torch.tensor(
+            self.lora_ranks, dtype=torch.int32, device=self.device
+        )
+
+        scalings_tensor = torch.tensor(
+            self.scalings, dtype=torch.float, device=self.device
+        )
+
+        expect_lora_a_output = reference_sgmv_shrink(
+            x,
+            gate_up_lora_a,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            scalings_tensor,
+            num_slices,
+        )
+
+        expect_output = reference_sgmv_expand(
+            expect_lora_a_output,
+            gate_up_lora_b,
+            weight_indices_tensor,
+            seg_len_tensor,
+            lora_ranks_tensor,
+            slice_offsets=output_offset,
+        )
+
+        actual_output = self.backend.run_gate_up_lora(x, gate_up_lora_a, gate_up_lora_b)
+        self.assertTrue(torch.allclose(actual_output, expect_output))
+
+
+if __name__ == "__main__":
+    unittest.main()
diff --git a/test/manual/lora/utils.py b/test/manual/lora/utils.py
new file mode 100644
index 000000000..c17a470d6
--- /dev/null
+++ b/test/manual/lora/utils.py
@@ -0,0 +1,148 @@
+from typing import Optional
+
+import torch
+
+
+def safe_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
+    """Matrix multiplication with mixed precision handling for float16"""
+    result = torch.matmul(a.float(), b.float())
+    return result.to(a.dtype)
+
+
+def reference_sgmv_shrink(
+    x: torch.Tensor,
+    weights: torch.Tensor,
+    weight_indices: torch.Tensor,
+    seq_lengths: torch.Tensor,
+    lora_ranks: torch.Tensor,
+    lora_scalings: torch.Tensor,
+    num_slices: int = 1,
+) -> torch.Tensor:
+    """
+    Simple sequence-level reference implementation of SGMV shrink operation.
+
+    Args:
+        x: (total_seq_len, input_dim) - Input activations
+        weights: (num_loras, num_slices * max_rank, input_dim) - LoRA A weights
+        weight_indices: LoRA idx for each sequence
+        seq_lengths: Length of each sequence
+        lora_ranks: LoRA rank for each LoRA adapters
+        lora_scalings: LoRA scaling for each LoRA adapters
+        num_slices: Number of slices (3 for QKV, 2 for gate_up, 1 for others)
+
+    Returns:
+        output: (total_seq_len, num_slices * max_rank) - Intermediate activations
+    """
+    if weights.numel() == 0:
+        total_seq_len = x.shape[0]
+        return torch.zeros(total_seq_len, 0, dtype=x.dtype, device=x.device)
+
+    total_seq_len, _ = x.shape
+    _, weight_out_dim, _ = weights.shape
+    max_rank = weight_out_dim // num_slices
+
+    output = torch.zeros(
+        total_seq_len, num_slices * max_rank, dtype=x.dtype, device=x.device
+    )
+
+    token_offset = 0
+    for lora_idx, seq_len, rank, scaling in zip(
+        weight_indices,
+        seq_lengths,
+        lora_ranks[weight_indices],
+        lora_scalings[weight_indices],
+    ):
+        if seq_len == 0:
+            continue
+
+        if rank > 0:
+            x_seq = x[token_offset : token_offset + seq_len, :]
+            w_seq = weights[lora_idx, : num_slices * rank, :]
+
+            result = safe_matmul(x_seq, w_seq.t())
+            output[token_offset : token_offset + seq_len, : num_slices * rank] = (
+                scaling * result
+            )
+
+        token_offset += seq_len
+
+    return output
+
+
+def reference_sgmv_expand(
+    x: torch.Tensor,
+    weights: torch.Tensor,
+    weight_indices: torch.Tensor,
+    seq_lengths: torch.Tensor,
+    lora_ranks: torch.Tensor,
+    slice_offsets: torch.Tensor,
+    base_output: Optional[torch.Tensor] = None,
+) -> torch.Tensor:
+    """
+    Simple sequence-level reference implementation of SGMV expand operation.
+
+    Args:
+        x: (total_seq_len, num_slices * max_rank) - Intermediate activations
+        weights: (num_loras, output_dim, max_rank) - LoRA B weights
+        weight_indices: LoRA idx for each sequence
+        seq_lengths: Length of each sequence
+        lora_ranks: LoRA rank for each LoRA adapters
+        slice_offsets: Tensor defining slice boundaries
+        base_output: Optional base output to accumulate into
+
+    Returns:
+        output: (total_seq_len, total_output_dim) - Final output
+    """
+    if weights.numel() == 0:
+        total_seq_len = x.shape[0]
+        total_output_dim = slice_offsets[-1].item() if len(slice_offsets) > 0 else 0
+        return torch.zeros(
+            total_seq_len, total_output_dim, dtype=x.dtype, device=x.device
+        )
+
+    total_seq_len, _ = x.shape
+
+    num_slices = len(slice_offsets) - 1
+
+    if base_output is not None:
+        output = base_output.clone()
+    else:
+        total_output_dim = slice_offsets[-1].item()
+        output = torch.zeros(
+            total_seq_len, total_output_dim, dtype=x.dtype, device=x.device
+        )
+
+    token_offset = 0
+    for lora_idx, seq_len, rank in zip(
+        weight_indices, seq_lengths, lora_ranks[weight_indices]
+    ):
+        if seq_len == 0:
+            continue
+
+        if rank > 0:
+            # Extract sequence intermediate activations
+            x_seq = x[
+                token_offset : token_offset + seq_len, : num_slices * rank
+            ]  # (seq_len, num_slices * rank)
+
+            for slice_idx in range(num_slices):
+                slice_start_input = slice_idx * rank
+                slice_end_input = (slice_idx + 1) * rank
+
+                slice_start_output = slice_offsets[slice_idx].item()
+                slice_end_output = slice_offsets[slice_idx + 1].item()
+
+                x_slice = x_seq[:, slice_start_input:slice_end_input]  # (seq_len, rank)
+                w_slice = weights[
+                    lora_idx, slice_start_output:slice_end_output, :rank
+                ]  # (slice_dim, rank)
+
+                result = safe_matmul(x_slice, w_slice.t())  # (seq_len, slice_dim)
+                output[
+                    token_offset : token_offset + seq_len,
+                    slice_start_output:slice_end_output,
+                ] += result
+
+        token_offset += seq_len
+
+    return output
diff --git a/test/manual/test_lora_ops.py b/test/manual/test_lora_ops.py
deleted file mode 100644
index e5018b54f..000000000
--- a/test/manual/test_lora_ops.py
+++ /dev/null
@@ -1,287 +0,0 @@
-import unittest
-
-import torch
-
-from sglang.srt.lora.torch_ops.lora_ops import (
-    bgmv_expand,
-    bgmv_expand_slice,
-    bgmv_shrink,
-    sgmv_expand,
-    sgmv_expand_slice,
-    sgmv_shrink,
-)
-from sglang.test.test_utils import CustomTestCase
-
-
-class TestLoraOps(CustomTestCase):
-    def test_sgmv_expand(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        seq_len_tensor = torch.ones(batch_size, dtype=torch.int32)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        add_inputs = True
-
-        total_seq_len, _ = inputs.shape
-        exploded_indices = torch.repeat_interleave(
-            lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-        )
-        expect_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        bgmv_expand(inputs, lora_b_weights, expect_output, exploded_indices, add_inputs)
-
-        actual_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        sgmv_expand(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            seq_len_tensor,
-            lora_indices_tensor,
-            add_inputs,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_bgmv_expand(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-
-        selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=dtype)
-        selected_loras = selected_loras.squeeze(dim=1)
-        inputs = inputs.to(dtype=dtype)
-        outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-        limit = batch_size
-        common_len = min(outputs.shape[1], output_dim)
-        expect_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        expect_output[:, :common_len] = outputs[:limit, :common_len]
-
-        actual_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        bgmv_expand(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            lora_indices_tensor,
-            add_inputs=False,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_bgmv_expand_add_residual(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-
-        selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=dtype)
-        selected_loras = selected_loras.squeeze(dim=1)
-        inputs = inputs.to(dtype=dtype)
-        outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-        limit = batch_size
-        common_len = min(outputs.shape[1], output_dim)
-        expect_output = torch.randn(batch_size, output_dim, dtype=dtype)
-        actual_output = expect_output.clone()
-
-        expect_output[:, :common_len] += outputs[:limit, :common_len]
-
-        bgmv_expand(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            lora_indices_tensor,
-            add_inputs=True,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_sgmv_shrink(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_a_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        seq_len_tensor = torch.ones(batch_size, dtype=torch.int32)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        scaling = 0.9
-
-        total_seq_len, _ = inputs.shape
-        exploded_indices = torch.repeat_interleave(
-            lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-        )
-        expect_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        bgmv_shrink(inputs, lora_a_weights, expect_output, exploded_indices, scaling)
-
-        actual_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        sgmv_shrink(
-            inputs,
-            lora_a_weights,
-            actual_output,
-            seq_len_tensor,
-            lora_indices_tensor,
-            scaling,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_bgmv_shrink(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_a_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        scaling = 0.9
-
-        selected_loras = lora_a_weights[lora_indices_tensor].to(dtype=dtype)
-        inputs = inputs.to(dtype=dtype)
-        outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-
-        expect_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        expect_output[:, : outputs.shape[1]] = scaling * outputs[:]
-
-        actual_output = torch.zeros(batch_size, output_dim, dtype=dtype)
-        bgmv_shrink(
-            inputs,
-            lora_a_weights,
-            actual_output,
-            lora_indices_tensor,
-            scaling=scaling,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_sgmv_expand_slice(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        output_dim_slice = 12
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        seq_len_tensor = torch.ones(batch_size, dtype=torch.int32)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        slice_offset = 2
-        slice_size = 6
-        add_inputs = False
-
-        total_seq_len, _ = inputs.shape
-        exploded_indices = torch.repeat_interleave(
-            lora_indices_tensor, seq_len_tensor, output_size=total_seq_len
-        )
-        expect_output = torch.randn(batch_size, output_dim_slice, dtype=dtype)
-        actual_output = expect_output.clone()
-        bgmv_expand_slice(
-            inputs,
-            lora_b_weights,
-            expect_output,
-            exploded_indices,
-            slice_offset,
-            slice_size,
-            add_inputs,
-        )
-
-        sgmv_expand_slice(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            seq_len_tensor,
-            lora_indices_tensor,
-            slice_offset,
-            slice_size,
-            add_inputs,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_bgmv_expand_slice(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        output_dim_slice = 12
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        slice_offset = 2
-        slice_size = 6
-
-        selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=dtype)
-        inputs = inputs.to(dtype=dtype)
-        outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-        expect_output = torch.zeros(batch_size, output_dim_slice, dtype=dtype)
-        expect_output[:, slice_offset : slice_offset + slice_size] = outputs[:]
-
-        actual_output = torch.zeros(batch_size, output_dim_slice, dtype=dtype)
-        bgmv_expand_slice(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            lora_indices_tensor,
-            slice_offset,
-            slice_size,
-            add_inputs=False,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_bgmv_expand_slice_add_residual(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        output_dim_slice = 12
-        num_loras = 3
-        dtype = torch.float32
-
-        inputs = torch.randn(batch_size, input_dim, dtype=dtype)
-        lora_b_weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        lora_indices_tensor = torch.randint(0, num_loras, (batch_size,))
-        slice_offset = 2
-        slice_size = 6
-
-        selected_loras = lora_b_weights[lora_indices_tensor].to(dtype=dtype)
-        inputs = inputs.to(dtype=dtype)
-        outputs = torch.einsum("bi, boi -> bo", inputs, selected_loras)
-        expect_output = torch.randn(batch_size, output_dim_slice, dtype=dtype)
-        actual_output = expect_output.clone()
-        expect_output[:, slice_offset : slice_offset + slice_size] += outputs[:]
-
-        bgmv_expand_slice(
-            inputs,
-            lora_b_weights,
-            actual_output,
-            lora_indices_tensor,
-            slice_offset,
-            slice_size,
-            add_inputs=True,
-        )
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/test/manual/test_torch_backend.py b/test/manual/test_torch_backend.py
deleted file mode 100644
index 6ca4ee54c..000000000
--- a/test/manual/test_torch_backend.py
+++ /dev/null
@@ -1,224 +0,0 @@
-import unittest
-
-import torch
-
-from sglang.srt.lora.backend.torch_backend import TorchNativeLoRABackend
-from sglang.srt.lora.torch_ops.lora_ops import (
-    sgmv_expand,
-    sgmv_expand_slice,
-    sgmv_shrink,
-)
-from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode
-from sglang.test.test_utils import CustomTestCase
-
-
-class TestTorchNativeLoRABackend(CustomTestCase):
-
-    device = "cpu"
-    forward_batch = ForwardBatch(
-        forward_mode=ForwardMode.EXTEND,
-        batch_size=2,
-        input_ids=torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32),
-        req_pool_indices=None,
-        seq_lens=None,
-        out_cache_loc=None,
-        seq_lens_sum=6,
-        extend_seq_lens=torch.tensor([1, 1], dtype=torch.int32),
-        extend_seq_lens_cpu=[1, 1],
-    )
-    weight_indices = [0, 1]
-    lora_ranks = [1, 1]
-    scalings = [1.0, 0.5]
-    use_cuda_graph = False
-
-    @classmethod
-    def setUpClass(cls):
-        cls.backend = TorchNativeLoRABackend(max_loras_per_batch=2, device=cls.device)
-        cls.backend.prepare_lora_batch(
-            forward_batch=cls.forward_batch,
-            weight_indices=cls.weight_indices,
-            lora_ranks=cls.lora_ranks,
-            scalings=cls.scalings,
-            use_cuda_graph=cls.use_cuda_graph,
-        )
-
-    def test_run_lora_a_sgemm(self):
-        batch_size = 2
-        input_dim = 4
-        output_dim = 6
-        num_loras = 3
-        dtype = torch.float32
-
-        x = torch.randn(batch_size, input_dim, dtype=dtype)
-        weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-
-        total_seq_len, _ = x.shape
-        _, weight_output_dim, _ = weights.shape
-        output_tensor = torch.zeros(
-            (total_seq_len, weight_output_dim), dtype=dtype, device=self.device
-        )
-        sgmv_shrink(
-            x,
-            weights,
-            output_tensor,
-            self.backend.batch_info.seg_lens,
-            self.backend.batch_info.weight_indices,
-            1.0,
-        )
-        scaling = torch.repeat_interleave(
-            self.backend.batch_info.scalings[self.backend.batch_info.weight_indices],
-            self.backend.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        expect_output = output_tensor * scaling
-
-        actual_output = self.backend.run_lora_a_sgemm(x, weights)
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_run_lora_b_sgemm(self):
-        batch_size = 2
-        input_dim = 6
-        output_dim = 4
-        num_loras = 3
-        dtype = torch.float32
-
-        x = torch.randn(batch_size, input_dim, dtype=dtype)
-        weights = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-
-        total_seq_len, _ = x.shape
-        _, weight_output_dim, _ = weights.shape
-        output_tensor = torch.zeros(
-            (total_seq_len, weight_output_dim), dtype=dtype, device=self.device
-        )
-        sgmv_expand(
-            x,
-            weights,
-            output_tensor,
-            self.backend.batch_info.seg_lens,
-            self.backend.batch_info.weight_indices,
-            True,
-        )
-        expect_output = output_tensor
-
-        actual_output = self.backend.run_lora_b_sgemm(x, weights)
-
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_run_qkv_lora(self):
-        batch_size = 2
-        input_dim = 6
-        output_dim = 4
-        num_loras = 3
-        dtype = torch.float32
-
-        x = torch.randn(batch_size, input_dim, dtype=dtype)
-        qkv_lora_a = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        qkv_lora_b = torch.randn(num_loras, input_dim, output_dim, dtype=dtype)
-        output_offset_cpu = torch.tensor([0, 3, 6, 9, 12], dtype=torch.int32)
-
-        num_slices = 3
-        total_seq_len, _ = x.shape
-        _, weight_intermediate_dim, _ = qkv_lora_a.shape
-        _, weight_out_dim, _ = qkv_lora_b.shape
-        max_rank = weight_intermediate_dim // num_slices
-        output_tensor = torch.zeros(
-            (total_seq_len, weight_out_dim), device=x.device, dtype=x.dtype
-        )
-        lora_a_output = torch.zeros(
-            total_seq_len, weight_intermediate_dim, dtype=x.dtype, device=x.device
-        )
-        sgmv_shrink(
-            x,
-            qkv_lora_a,
-            lora_a_output,
-            self.backend.batch_info.seg_lens,
-            self.backend.batch_info.weight_indices,
-            1.0,
-        )
-        scaling = torch.repeat_interleave(
-            self.backend.batch_info.scalings[self.backend.batch_info.weight_indices],
-            self.backend.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        lora_a_output = lora_a_output * scaling
-        for slice_id in range(num_slices):
-            slice_offset = output_offset_cpu[slice_id]
-            slice_offset_next = output_offset_cpu[slice_id + 1]
-            slice_size = slice_offset_next - slice_offset
-            sgmv_expand_slice(
-                lora_a_output[:, (max_rank * slice_id) : (max_rank * (slice_id + 1))],
-                qkv_lora_b[:, slice_offset:slice_offset_next],
-                output_tensor,
-                self.backend.batch_info.seg_lens,
-                self.backend.batch_info.weight_indices,
-                slice_offset,
-                slice_size,
-                True,
-            )
-        expect_output = output_tensor
-        actual_output = self.backend.run_qkv_lora(
-            x, qkv_lora_a, qkv_lora_b, None, output_offset_cpu, 0
-        )
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-    def test_run_gate_up_lora(self):
-        batch_size = 2
-        input_dim = 6
-        output_dim = 4
-        num_loras = 3
-        dtype = torch.float32
-
-        num_slices = 2
-
-        x = torch.randn(batch_size, input_dim, dtype=dtype)
-        gate_up_lora_a = torch.randn(num_loras, output_dim, input_dim, dtype=dtype)
-        gate_up_lora_b = torch.randn(
-            num_loras, output_dim, output_dim // num_slices, dtype=dtype
-        )
-
-        total_seq_len, _ = x.shape
-        _, weight_intermediate_dim, _ = gate_up_lora_a.shape
-        _, weight_out_dim, _ = gate_up_lora_b.shape
-        slice_size = weight_out_dim // num_slices
-        max_rank = weight_intermediate_dim // num_slices
-        output_tensor = torch.zeros(
-            (total_seq_len, weight_out_dim), device=x.device, dtype=x.dtype
-        )
-        lora_a_output = torch.zeros(
-            total_seq_len, weight_intermediate_dim, dtype=x.dtype, device=x.device
-        )
-        sgmv_shrink(
-            x,
-            gate_up_lora_a,
-            lora_a_output,
-            self.backend.batch_info.seg_lens,
-            self.backend.batch_info.weight_indices,
-            1.0,
-        )
-        scaling = torch.repeat_interleave(
-            self.backend.batch_info.scalings[self.backend.batch_info.weight_indices],
-            self.backend.batch_info.seg_lens,
-            output_size=total_seq_len,
-        ).unsqueeze(-1)
-        lora_a_output = lora_a_output * scaling
-        slice_offset = 0
-        for slice_id in range(num_slices):
-            sgmv_expand_slice(
-                lora_a_output[:, (max_rank * slice_id) : (max_rank * (slice_id + 1))],
-                gate_up_lora_b[:, slice_offset : slice_offset + slice_size],
-                output_tensor,
-                self.backend.batch_info.seg_lens,
-                self.backend.batch_info.weight_indices,
-                slice_offset,
-                slice_size,
-                True,
-            )
-            slice_offset += slice_size
-        expect_output = output_tensor
-        actual_output = self.backend.run_gate_up_lora(x, gate_up_lora_a, gate_up_lora_b)
-        self.assertTrue(torch.allclose(actual_output, expect_output))
-
-
-if __name__ == "__main__":
-    unittest.main()
-- 
2.52.0

