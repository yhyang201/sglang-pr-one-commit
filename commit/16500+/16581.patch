From a3220e8d0e417a4b559e1afe79c814ec929f096e Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:50:59 +0000
Subject: [PATCH] feat: Squash PR #16581 changes

---
 python/sglang/srt/configs/model_config.py | 19 +++++++++++++++++++
 python/sglang/srt/models/glm4_moe.py      |  2 ++
 2 files changed, 21 insertions(+)

diff --git a/python/sglang/srt/configs/model_config.py b/python/sglang/srt/configs/model_config.py
index aa10cb08d..31f918462 100644
--- a/python/sglang/srt/configs/model_config.py
+++ b/python/sglang/srt/configs/model_config.py
@@ -765,6 +765,25 @@ class ModelConfig:
                     self.quantization = quantization_override
                     break
 
+            # NextN/MTP draft models for these architectures are not FP4 quantized,
+            # disable quantization to avoid loader errors
+            nextn_architectures_without_fp4 = [
+                "DeepseekV3ForCausalLMNextN",
+                "Glm4MoeForCausalLMNextN",
+                "BailingMoeForCausalLMNextN",
+            ]
+            if (
+                self.is_draft_model
+                and self.hf_config.architectures[0] in nextn_architectures_without_fp4
+                and self.quantization == "modelopt_fp4"
+            ):
+                logger.info(
+                    f"Disabling modelopt_fp4 quantization for draft model "
+                    f"({self.hf_config.architectures[0]}) as NextN layers are not FP4 quantized."
+                )
+                self.quantization = None
+                return
+
             # Verify quantization configurations.
             if self.quantization is None:
                 self.quantization = quant_method
diff --git a/python/sglang/srt/models/glm4_moe.py b/python/sglang/srt/models/glm4_moe.py
index be71d0d28..0737b13c1 100644
--- a/python/sglang/srt/models/glm4_moe.py
+++ b/python/sglang/srt/models/glm4_moe.py
@@ -63,6 +63,7 @@ from sglang.srt.layers.moe import (
 from sglang.srt.layers.moe.ep_moe.layer import get_moe_impl_class
 from sglang.srt.layers.moe.fused_moe_triton.layer import FusedMoE
 from sglang.srt.layers.moe.topk import TopK
+from sglang.srt.layers.moe.utils import RoutingMethodType
 from sglang.srt.layers.quantization.base_config import QuantizationConfig
 from sglang.srt.layers.quantization.fp8_kernel import is_fp8_fnuz
 from sglang.srt.layers.radix_attention import RadixAttention
@@ -375,6 +376,7 @@ class Glm4MoeSparseMoeBlock(nn.Module):
             intermediate_size=config.moe_intermediate_size,
             quant_config=quant_config,
             routed_scaling_factor=self.routed_scaling_factor,
+            routing_method_type=RoutingMethodType.DeepSeekV3,
             prefix=add_prefix("experts", prefix),
         )
 
-- 
2.52.0

