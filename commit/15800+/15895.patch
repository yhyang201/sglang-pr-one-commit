From ba879700dac91fda3c00dce4e47e7dd8d026539c Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Tue, 20 Jan 2026 06:25:50 +0000
Subject: [PATCH] feat: Squash PR #15895 changes

---
 .../configs/models/dits/zimage.py             |   5 +-
 .../configs/pipeline_configs/__init__.py      |   6 +-
 .../configs/pipeline_configs/zimage.py        | 202 ++++
 .../multimodal_gen/configs/sample/zimage.py   |   7 +
 python/sglang/multimodal_gen/registry.py      |  20 +-
 .../runtime/models/dits/zimage.py             | 920 +++++++++++++++++-
 .../runtime/pipelines/zimage_pipeline.py      | 387 +++++++-
 .../pipelines_core/composed_pipeline_base.py  |   8 +-
 .../runtime/pipelines_core/schedule_batch.py  |  11 +
 .../pipelines_core/stages/text_encoding.py    |  11 +
 10 files changed, 1518 insertions(+), 59 deletions(-)

diff --git a/python/sglang/multimodal_gen/configs/models/dits/zimage.py b/python/sglang/multimodal_gen/configs/models/dits/zimage.py
index de3275852..8b0e819fa 100644
--- a/python/sglang/multimodal_gen/configs/models/dits/zimage.py
+++ b/python/sglang/multimodal_gen/configs/models/dits/zimage.py
@@ -2,7 +2,7 @@
 
 # SPDX-License-Identifier: Apache-2.0
 from dataclasses import dataclass, field
-from typing import Tuple
+from typing import Optional, Tuple
 
 from sglang.multimodal_gen.configs.models.dits.base import DiTArchConfig, DiTConfig
 
@@ -26,6 +26,9 @@ class ZImageArchConfig(DiTArchConfig):
     axes_dims: Tuple[int, int, int] = (32, 48, 48)
     axes_lens: Tuple[int, int, int] = (1024, 512, 512)
 
+    # Optional: set to enable SigLIP support for Omni
+    siglip_feat_dim: Optional[int] = None
+
     stacked_params_mapping: list[tuple[str, str, str]] = field(
         default_factory=lambda: [
             # (param_name, shard_name, shard_id)
diff --git a/python/sglang/multimodal_gen/configs/pipeline_configs/__init__.py b/python/sglang/multimodal_gen/configs/pipeline_configs/__init__.py
index 48e2ffe25..da977c476 100644
--- a/python/sglang/multimodal_gen/configs/pipeline_configs/__init__.py
+++ b/python/sglang/multimodal_gen/configs/pipeline_configs/__init__.py
@@ -26,7 +26,10 @@ from sglang.multimodal_gen.configs.pipeline_configs.wan import (
     WanT2V480PConfig,
     WanT2V720PConfig,
 )
-from sglang.multimodal_gen.configs.pipeline_configs.zimage import ZImagePipelineConfig
+from sglang.multimodal_gen.configs.pipeline_configs.zimage import (
+    ZImageOmniPipelineConfig,
+    ZImagePipelineConfig,
+)
 
 __all__ = [
     "DiffusersGenericPipelineConfig",
@@ -44,4 +47,5 @@ __all__ = [
     "WanI2V720PConfig",
     "SelfForcingWanT2V480PConfig",
     "ZImagePipelineConfig",
+    "ZImageOmniPipelineConfig",
 ]
diff --git a/python/sglang/multimodal_gen/configs/pipeline_configs/zimage.py b/python/sglang/multimodal_gen/configs/pipeline_configs/zimage.py
index 68f00a667..a2f8af7cf 100644
--- a/python/sglang/multimodal_gen/configs/pipeline_configs/zimage.py
+++ b/python/sglang/multimodal_gen/configs/pipeline_configs/zimage.py
@@ -157,3 +157,205 @@ class ZImagePipelineConfig(ImagePipelineConfig):
                 batch,
             ),
         }
+
+
+def zimage_omni_preprocess_text(prompt: str):
+    """
+    Simply return, delay process in tokenize_prompt
+        since we need a `self` object to take metadatas
+    """
+    return prompt
+
+
+def zimage_omni_postprocess_text(
+    outputs: BaseEncoderOutput, _text_inputs
+) -> torch.Tensor:
+    """
+    assert batch_size == 1
+
+    Basic mode:
+        single text embeds
+        torch.tensor
+    Omni mode:
+        Omni mode break a prompt_str into list of slice.
+        which will be concat together cat([torch.tensor, ...])
+
+    Returns:
+        torch.tensor: a flatten tensor for single batch and concat all split prompts.
+    """
+
+    prompt_list_lengths = getattr(_text_inputs, "prompt_list_lengths", None)
+    if prompt_list_lengths is None:
+        # Basic mode
+        device = outputs.hidden_states[-2].device
+        prompt_mask = _text_inputs.attention_mask.to(device).bool()
+        embeds = outputs.hidden_states[-2][0][prompt_mask[0]]
+        raise NotImplementedError("useless, which equals to else case below")
+    else:
+        assert len(prompt_list_lengths) == 1, "Single batch only."
+
+        device = outputs.hidden_states[-2].device
+        embeddings_list = []
+        start_idx = 0
+        batch_embeddings = []
+        end_idx = start_idx + prompt_list_lengths[0]  # single batch
+        prompt_embeds = outputs.hidden_states[-2]
+        prompt_masks = _text_inputs.attention_mask.to(device).bool()
+        for j in range(start_idx, end_idx):
+            batch_embeddings.append(prompt_embeds[j][prompt_masks[j]])
+        embeddings_list.append(batch_embeddings)
+        # assert single batch
+        embeds = torch.cat(embeddings_list[0], dim=0)
+
+    return embeds
+
+
+@dataclass
+class ZImageOmniPipelineConfig(ZImagePipelineConfig):
+    preprocess_text_funcs: tuple[Callable, ...] = field(
+        default_factory=lambda: (zimage_omni_preprocess_text,)
+    )
+
+    postprocess_text_funcs: tuple[Callable, ...] = field(
+        default_factory=lambda: (zimage_omni_postprocess_text,)
+    )
+
+    # TODO: review
+    # ugly hack
+    # pos token_lens, neg token_lens, pos token_lens, neg token_lens
+    # token_lens[0] for pos token_lens
+    # token_lens[1] for neg token_lens
+    # maybe bug in serving case
+    token_lens = []
+
+    def _apply_zimage_omni_template(
+        self, prompts: list[str], num_condition_images: int
+    ):
+        """
+        Args:
+            prompts (list[str]): 1d list of strings
+        Returns
+            processed_text_list (list[list[str]]): 2d list of strings
+                a single prompt_str was break into list of strings, split by <|vision_start|>
+        """
+        processed_text_list: list[list[str]] = []
+        for prompt_str in prompts:
+            if num_condition_images == 0:
+                prompt_str = [
+                    "<|im_start|>user\n"
+                    + prompt_str
+                    + "<|im_end|>\n<|im_start|>assistant\n"
+                ]
+                processed_text_list.append(prompt_str)
+            else:
+                prompt_list = ["<|im_start|>user\n<|vision_start|>"]
+                prompt_list += ["<|vision_end|><|vision_start|>"] * (
+                    num_condition_images - 1
+                )
+                prompt_list += [
+                    "<|vision_end|>"
+                    + prompt_str
+                    + "<|im_end|>\n<|im_start|>assistant\n<|vision_start|>"
+                ]
+                prompt_list += ["<|vision_end|><|im_end|>"]
+                processed_text_list.append(prompt_list)
+        return processed_text_list
+
+    def tokenize_prompt(self, prompts, tokenizer, tok_kwargs) -> dict:
+        """
+        template was inject in preprocess, no apply_chat_template now.
+        """
+        prompts = self._apply_zimage_omni_template(
+            prompts, tok_kwargs.get("num_condition_images", 0)
+        )
+        assert (
+            isinstance(prompts, list)
+            and isinstance(prompts[0], list)
+            and isinstance(prompts[0][0], str)
+        ), "Process type mismatch."
+
+        # all batch flattened
+        flattened_prompt = []
+        prompt_list_lengths = []
+
+        # do flatten and record metadata
+        for i in range(len(prompts)):
+            # record freg numbers
+            prompt_list_lengths.append(len(prompts[i]))
+            # all batch flattened prompts
+            flattened_prompt.extend(prompts[i])
+
+        inputs = tokenizer(
+            flattened_prompt,
+            padding="max_length",
+            max_length=512,  # TODO (yhyang201): set max length according to config
+            truncation=True,
+            return_tensors="pt",
+        )
+
+        # TODO: hack
+        # prompt_list_lengths is used to reconstruct flattened_prompt into batching
+        inputs.prompt_list_lengths = prompt_list_lengths
+        token_lens = inputs.attention_mask.sum(dim=-1).tolist()
+
+        self.token_lens.append(token_lens)
+
+        return inputs
+
+    # TODO: hack
+    # pos and freq is online compute
+    def prepare_pos_cond_kwargs(self, batch, device, rotary_emb, dtype):
+        pos_cond_kwargs = {
+            "condition_latents": batch.condition_latents,
+            "token_lens": self.token_lens[0],
+        }
+        if (
+            batch.condition_siglip_embeds is not None
+            and batch.condition_latents is not None
+        ):
+            assert len(batch.condition_siglip_embeds) == 1, "Single batch only for now."
+            assert len(batch.condition_latents) == 1, "Single batch only for now."
+
+            current_batch_size = len(batch.condition_latents)
+
+            condition_latents_model_input = batch.condition_latents
+            # Create noise mask: 0 for condition images (clean), 1 for target image (noisy)
+            image_noise_mask = [
+                [0] * len(condition_latents_model_input[i]) + [1]
+                for i in range(current_batch_size)
+            ]
+
+            pos_cond_kwargs["siglip_feats"] = batch.condition_siglip_embeds
+            pos_cond_kwargs["image_noise_mask"] = image_noise_mask
+
+        return pos_cond_kwargs
+
+    def prepare_neg_cond_kwargs(self, batch, device, rotary_emb, dtype):
+        neg_cond_kwargs = {
+            "condition_latents": batch.negative_condition_latents,
+            "token_lens": self.token_lens[1],
+        }
+        if (
+            batch.negative_condition_siglip_embeds is not None
+            and batch.negative_condition_latents is not None
+        ):
+            assert (
+                len(batch.negative_condition_siglip_embeds) == 1
+            ), "Single batch only for now."
+            assert (
+                len(batch.negative_condition_latents) == 1
+            ), "Single batch only for now."
+
+            current_batch_size = len(batch.negative_condition_latents)
+
+            condition_latents_model_input = batch.negative_condition_latents
+            # Create noise mask: 0 for condition images (clean), 1 for target image (noisy)
+            image_noise_mask = [
+                [0] * len(condition_latents_model_input[i]) + [1]
+                for i in range(current_batch_size)
+            ]
+
+            neg_cond_kwargs["siglip_feats"] = batch.negative_condition_siglip_embeds
+            neg_cond_kwargs["image_noise_mask"] = image_noise_mask
+
+        return neg_cond_kwargs
diff --git a/python/sglang/multimodal_gen/configs/sample/zimage.py b/python/sglang/multimodal_gen/configs/sample/zimage.py
index f4530e977..8c46ab20d 100644
--- a/python/sglang/multimodal_gen/configs/sample/zimage.py
+++ b/python/sglang/multimodal_gen/configs/sample/zimage.py
@@ -31,3 +31,10 @@ class ZImageSamplingParams(SamplingParams):
             ],
         )
     )
+
+
+@dataclass
+class ZImageOmniSamplingParams(ZImageSamplingParams):
+    # zimage-omni default
+    num_inference_steps: int = 50
+    guidance_scale: float = 5.0
diff --git a/python/sglang/multimodal_gen/registry.py b/python/sglang/multimodal_gen/registry.py
index 2f09062af..0e8822da7 100644
--- a/python/sglang/multimodal_gen/registry.py
+++ b/python/sglang/multimodal_gen/registry.py
@@ -35,6 +35,7 @@ from sglang.multimodal_gen.configs.pipeline_configs import (
     WanI2V720PConfig,
     WanT2V480PConfig,
     WanT2V720PConfig,
+    ZImageOmniPipelineConfig,
     ZImagePipelineConfig,
 )
 from sglang.multimodal_gen.configs.pipeline_configs.base import PipelineConfig
@@ -88,7 +89,10 @@ from sglang.multimodal_gen.configs.sample.wan import (
     WanT2V_1_3B_SamplingParams,
     WanT2V_14B_SamplingParams,
 )
-from sglang.multimodal_gen.configs.sample.zimage import ZImageSamplingParams
+from sglang.multimodal_gen.configs.sample.zimage import (
+    ZImageOmniSamplingParams,
+    ZImageSamplingParams,
+)
 from sglang.multimodal_gen.runtime.pipelines_core.composed_pipeline_base import (
     ComposedPipelineBase,
 )
@@ -562,13 +566,25 @@ def _register_configs():
             lambda hf_id: "flux.2" in hf_id.lower() and "klein" not in hf_id.lower()
         ],
     )
+    # Z-Image
     register_configs(
         sampling_param_cls=ZImageSamplingParams,
         pipeline_config_cls=ZImagePipelineConfig,
         hf_model_paths=[
             "Tongyi-MAI/Z-Image-Turbo",
         ],
-        model_detectors=[lambda hf_id: "z-image" in hf_id.lower()],
+        model_detectors=[
+            lambda hf_id: "z-image" in hf_id.lower()
+            and "z-image-omni" not in hf_id.lower()
+        ],
+    )
+    register_configs(
+        sampling_param_cls=ZImageOmniSamplingParams,
+        pipeline_config_cls=ZImageOmniPipelineConfig,
+        hf_model_paths=[
+            "Tongyi-MAI/Z-Image-Omni-Base",
+        ],
+        model_detectors=[lambda hf_id: "z-image-omni" in hf_id.lower()],
     )
     # Qwen-Image
     register_configs(
diff --git a/python/sglang/multimodal_gen/runtime/models/dits/zimage.py b/python/sglang/multimodal_gen/runtime/models/dits/zimage.py
index 2addd365d..760739ee4 100644
--- a/python/sglang/multimodal_gen/runtime/models/dits/zimage.py
+++ b/python/sglang/multimodal_gen/runtime/models/dits/zimage.py
@@ -1,5 +1,5 @@
 import math
-from typing import Any, List, Optional, Tuple
+from typing import Any, List, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
@@ -86,6 +86,34 @@ class TimestepEmbedder(nn.Module):
         return t_emb
 
 
+def select_per_token(
+    value_noisy: torch.Tensor,
+    value_clean: torch.Tensor,
+    noise_mask: torch.Tensor,
+    seq_len: int,
+) -> torch.Tensor:
+    """
+    Fused value_noisy and value_clean in to a single tensor.
+    where if noise_mask == 1, fill value_noisy
+    else fill value_clean(t_embedder(ones))
+
+    Args:
+        value_noisy (tensor): t_embedder(t)
+        value_clean (tensor): t_embedder(ones_like(t))
+        noise_mask (tensor): 0/1
+        seqlen (int): seqlen
+    Returns
+        (tensor): tensor fill with value_noisy and value_clean.
+            shape was expand to (batch, seqlen, dim)
+    """
+    noise_mask_expanded = noise_mask.unsqueeze(-1)  # (batch, seq_len, 1)
+    return torch.where(
+        noise_mask_expanded == 1,
+        value_noisy.unsqueeze(1).expand(-1, seq_len, -1),
+        value_clean.unsqueeze(1).expand(-1, seq_len, -1),
+    )
+
+
 class FeedForward(nn.Module):
     def __init__(self, dim: int, hidden_dim: int):
         super().__init__()
@@ -255,15 +283,66 @@ class ZImageTransformerBlock(nn.Module):
         x: torch.Tensor,
         freqs_cis: Tuple[torch.Tensor, torch.Tensor],
         adaln_input: Optional[torch.Tensor] = None,
+        noise_mask: Optional[torch.Tensor] = None,
+        adaln_noisy: Optional[torch.Tensor] = None,
+        adaln_clean: Optional[torch.Tensor] = None,
     ):
         if self.modulation:
-            assert adaln_input is not None
-            scale_msa_gate, _ = self.adaLN_modulation(adaln_input)
-            scale_msa, gate_msa, scale_mlp, gate_mlp = scale_msa_gate.unsqueeze(
-                1
-            ).chunk(4, dim=2)
-            gate_msa, gate_mlp = gate_msa.tanh(), gate_mlp.tanh()
-            scale_msa, scale_mlp = 1.0 + scale_msa, 1.0 + scale_mlp
+            seq_len = x.shape[1]
+            if noise_mask is not None:
+                # Per-token modulation: different modulation for noisy/clean tokens
+                assert adaln_noisy is not None
+                assert adaln_clean is not None
+                mod_noisy, _ = self.adaLN_modulation(adaln_noisy)
+                mod_clean, _ = self.adaLN_modulation(adaln_clean)
+
+                scale_msa_noisy, gate_msa_noisy, scale_mlp_noisy, gate_mlp_noisy = (
+                    mod_noisy.chunk(4, dim=1)
+                )
+                scale_msa_clean, gate_msa_clean, scale_mlp_clean, gate_mlp_clean = (
+                    mod_clean.chunk(4, dim=1)
+                )
+
+                gate_msa_noisy, gate_mlp_noisy = (
+                    gate_msa_noisy.tanh(),
+                    gate_mlp_noisy.tanh(),
+                )
+                gate_msa_clean, gate_mlp_clean = (
+                    gate_msa_clean.tanh(),
+                    gate_mlp_clean.tanh(),
+                )
+
+                scale_msa_noisy, scale_mlp_noisy = (
+                    1.0 + scale_msa_noisy,
+                    1.0 + scale_mlp_noisy,
+                )
+                scale_msa_clean, scale_mlp_clean = (
+                    1.0 + scale_msa_clean,
+                    1.0 + scale_mlp_clean,
+                )
+
+                scale_msa = select_per_token(
+                    scale_msa_noisy, scale_msa_clean, noise_mask, seq_len
+                )
+                scale_mlp = select_per_token(
+                    scale_mlp_noisy, scale_mlp_clean, noise_mask, seq_len
+                )
+                gate_msa = select_per_token(
+                    gate_msa_noisy, gate_msa_clean, noise_mask, seq_len
+                )
+                gate_mlp = select_per_token(
+                    gate_mlp_noisy, gate_mlp_clean, noise_mask, seq_len
+                )
+
+            else:
+                # Global modulation: same modulation for all tokens (avoid double select)
+                assert adaln_input is not None
+                scale_msa_gate, _ = self.adaLN_modulation(adaln_input)
+                scale_msa, gate_msa, scale_mlp, gate_mlp = scale_msa_gate.unsqueeze(
+                    1
+                ).chunk(4, dim=2)
+                gate_msa, gate_mlp = gate_msa.tanh(), gate_mlp.tanh()
+                scale_msa, scale_mlp = 1.0 + scale_msa, 1.0 + scale_mlp
 
             # Attention block
             attn_out = self.attention(
@@ -310,10 +389,22 @@ class FinalLayer(nn.Module):
             ReplicatedLinear(min(hidden_size, ADALN_EMBED_DIM), hidden_size, bias=True),
         )
 
-    def forward(self, x, c):
-        scale, _ = self.adaLN_modulation(c)
-        scale = 1.0 + scale
-        x = self.norm_final(x) * scale.unsqueeze(1)
+    def forward(self, x, c=None, noise_mask=None, c_noisy=None, c_clean=None):
+        seq_len = x.shape[1]
+
+        if noise_mask is not None:
+            # Per-token modulation
+            scale_noisy = 1.0 + self.adaLN_modulation(c_noisy)[0]
+            scale_clean = 1.0 + self.adaLN_modulation(c_clean)[0]
+            scale = select_per_token(scale_noisy, scale_clean, noise_mask, seq_len)
+        else:
+            # Original global modulation
+            assert c is not None, "Either c or (c_noisy, c_clean) must be provided"
+            scale, _ = self.adaLN_modulation(c)
+            scale = 1.0 + scale
+            scale = scale.unsqueeze(1)
+
+        x = self.norm_final(x) * scale
         x, _ = self.linear(x)
         return x
 
@@ -412,6 +503,7 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         self.all_f_patch_size = arch_config.all_f_patch_size
         self.dim = arch_config.dim
         self.n_heads = arch_config.num_attention_heads
+        self.siglip_feat_dim = arch_config.siglip_feat_dim
 
         self.rope_theta = arch_config.rope_theta
         self.t_scale = arch_config.t_scale
@@ -468,6 +560,33 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
                 for layer_id in range(arch_config.n_refiner_layers)
             ]
         )
+
+        # Optional SigLIP components (for Omni variant)
+        if self.siglip_feat_dim is not None:
+            self.siglip_embedder = nn.Sequential(
+                RMSNorm(self.siglip_feat_dim, eps=arch_config.norm_eps),
+                nn.Linear(self.siglip_feat_dim, self.dim, bias=True),
+            )
+            self.siglip_refiner = nn.ModuleList(
+                [
+                    ZImageTransformerBlock(
+                        2000 + layer_id,
+                        self.dim,
+                        self.n_heads,
+                        arch_config.n_kv_heads,
+                        arch_config.norm_eps,
+                        arch_config.qk_norm,
+                        modulation=False,
+                    )
+                    for layer_id in range(arch_config.n_refiner_layers)
+                ]
+            )
+            self.siglip_pad_token = nn.Parameter(torch.empty((1, self.dim)))
+        else:
+            self.siglip_embedder = None
+            self.siglip_refiner = None
+            self.siglip_pad_token = None
+
         self.t_embedder = TimestepEmbedder(
             min(self.dim, ADALN_EMBED_DIM), mid_size=1024
         )
@@ -503,24 +622,61 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         )
         self.layer_names = ["layers"]
 
+    # TODO: review
+    # Copied from diffusers.models.transformers.transformer_z_image.unpatchify
     def unpatchify(
-        self, x: List[torch.Tensor], size: List[Tuple], patch_size, f_patch_size
+        self,
+        x: List[torch.Tensor],
+        size: List[Tuple],
+        patch_size,
+        f_patch_size,
+        x_pos_offsets: Optional[List[Tuple[int, int]]] = None,
     ) -> List[torch.Tensor]:
         pH = pW = patch_size
         pF = f_patch_size
         bsz = len(x)
         assert len(size) == bsz
-        for i in range(bsz):
-            F, H, W = size[i]
-            ori_len = (F // pF) * (H // pH) * (W // pW)
-            # "f h w pf ph pw c -> c (f pf) (h ph) (w pw)"
-            x[i] = (
-                x[i][:ori_len]
-                .view(F // pF, H // pH, W // pW, pF, pH, pW, self.out_channels)
-                .permute(6, 0, 3, 1, 4, 2, 5)
-                .reshape(self.out_channels, F, H, W)
-            )
-        return x
+
+        if x_pos_offsets is not None:
+            # Omni: extract target image from unified sequence (cond_images + target)
+            result = []
+            for i in range(bsz):
+                unified_x = x[i][x_pos_offsets[i][0] : x_pos_offsets[i][1]]
+                cu_len = 0
+                x_item = None
+                for j in range(len(size[i])):
+                    if size[i][j] is None:
+                        ori_len = 0
+                        pad_len = SEQ_MULTI_OF
+                        cu_len += pad_len + ori_len
+                    else:
+                        F, H, W = size[i][j]
+                        ori_len = (F // pF) * (H // pH) * (W // pW)
+                        pad_len = (-ori_len) % SEQ_MULTI_OF
+                        x_item = (
+                            unified_x[cu_len : cu_len + ori_len]
+                            .view(
+                                F // pF, H // pH, W // pW, pF, pH, pW, self.out_channels
+                            )
+                            .permute(6, 0, 3, 1, 4, 2, 5)
+                            .reshape(self.out_channels, F, H, W)
+                        )
+                        cu_len += ori_len + pad_len
+                result.append(x_item)  # Return only the last (target) image
+            return result
+        else:
+            # Original mode: simple unpatchify
+            for i in range(bsz):
+                F, H, W = size[i]
+                ori_len = (F // pF) * (H // pH) * (W // pW)
+                # "f h w pf ph pw c -> c (f pf) (h ph) (w pw)"
+                x[i] = (
+                    x[i][:ori_len]
+                    .view(F // pF, H // pH, W // pW, pF, pH, pW, self.out_channels)
+                    .permute(6, 0, 3, 1, 4, 2, 5)
+                    .reshape(self.out_channels, F, H, W)
+                )
+            return x
 
     @staticmethod
     def create_coordinate_grid(size, start=None, device=None):
@@ -534,6 +690,212 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         grids = torch.meshgrid(axes, indexing="ij")
         return torch.stack(grids, dim=-1)
 
+    def patchify_and_embed_omni(
+        self,
+        all_image: List[List[torch.Tensor]],
+        all_cap_feats: List[List[torch.Tensor]],
+        patch_size: int,
+        f_patch_size: int,
+        all_siglip_feats: List[List[torch.Tensor]],
+        images_noise_mask: List[List[int]],
+    ):
+        """
+        Patchify and padding for omni mode: multiple images per batch item with noise masks.
+        Process siglip.
+
+        Args:
+            all_image (List[tensor]): [condition..., latent]
+            all_cap_feats (List[tensor]): TODO
+            patch_size (int): patch_size for h and w
+            f_patch_size (int): patch_size for f
+            all_siglip_feats (List[tensor]): [num_images...] * batch_size
+        Returns:
+
+        """
+        # TODO: reivew, bsz==1 only
+        # TODO: hard code batch_size == 1 only fow now.
+        assert len(all_image) == len(all_cap_feats) == 1
+        bsz = len(all_image)
+
+        device = all_image[0][-1].device
+        dtype = all_image[0][-1].dtype
+
+        images = all_image[0]  # List of [C, F, H, W]
+        cap_feats = all_cap_feats[0]  # List of [L, D]
+        siglips = all_siglip_feats[0]  # List of [H, W, C] TODO: review
+
+        all_image_out = []
+        all_image_size = []
+        all_cap_feats_out = []
+        all_sig_out = []
+
+        # TODO:
+        # review, usage of mask
+
+        all_image_noise_mask = []
+        all_cap_noise_mask = []
+        all_sig_noise_mask = []
+
+        all_image_len, all_cap_len, all_sig_len = [], [], []
+
+        # ------------ Process Caption ------------
+        # TODO:
+        # support bsz==1 only
+        i = 0
+
+        all_cap_feat_list = []
+        all_cap_noise_mask_list = []
+        cap_lens = []
+        for j, cap_feat in enumerate(cap_feats):
+            # cap_ori_len = cap_feat.size(0)
+            # cap_padding_len = (-cap_ori_len) % SEQ_MULTI_OF
+
+            # # padded feature
+            # cap_padded_feat = torch.cat(
+            #     [cap_feat, cap_feat[-1:].repeat(cap_padding_len, 1)],
+            #     dim=0,
+            # )
+
+            # noise_val == 1 fill noise
+            # noise_val == 0 fill empty
+            noise_val = images_noise_mask[i][j] if j < len(images_noise_mask[i]) else 1
+            cap_padded_feat, cap_len, cap_nm = self._pad_and_prepare_noise_mask(
+                cap_feat,
+                noise_val,
+            )
+
+            all_cap_feat_list.append(cap_padded_feat)
+            all_cap_noise_mask_list.extend(cap_nm)
+            cap_lens.append(cap_len)
+
+        all_cap_feats_out.append(torch.cat(all_cap_feat_list, dim=0))
+        all_cap_noise_mask.append(all_cap_noise_mask_list)
+        all_cap_len.append(cap_lens)
+
+        # ------------ Process Image ------------
+        all_image_padded_feat_list = []
+        all_image_noise_mask_list = []
+        image_lens = []
+        image_sizes = []
+        for j, image in enumerate(images):
+            image, (F, H, W), _ = self._patchify_image(
+                image, patch_size=patch_size, f_patch_size=f_patch_size
+            )
+            image_sizes.append((F, H, W))
+
+            # image_ori_len = image.size(0)
+            # image_padding_len = (-image_ori_len) % SEQ_MULTI_OF
+            # # padded feature
+            # image_padded_feat = torch.cat(
+            #     [image, image[-1:].repeat(image_padding_len, 1)],
+            #     dim=0,
+            # )
+
+            noise_val = images_noise_mask[i][j]
+            image_padded_feat, image_len, image_nm = self._pad_and_prepare_noise_mask(
+                image,
+                noise_val,
+            )
+
+            all_image_padded_feat_list.append(image_padded_feat)
+            all_image_noise_mask_list.extend(image_nm)
+            image_lens.append(image_len)
+        all_image_size.append(image_sizes)
+        all_image_out.append(torch.cat(all_image_padded_feat_list, dim=0))
+        all_image_noise_mask.append(all_image_noise_mask_list)
+        all_image_len.append(image_lens)
+
+        # ------------ Process Siglip ------------
+        all_sig_feats_list = []
+        all_sig_noise_mask_list = []
+        sig_lens = []
+        for j, sig_item in enumerate(siglips):
+            noise_val = images_noise_mask[i][j]
+            if sig_item is None:
+                sig_len = SEQ_MULTI_OF
+                sig_padded_feat = torch.zeros(
+                    (sig_len, self.config.siglip_feat_dim), dtype=dtype, device=device
+                )
+                sig_nm = [noise_val] * sig_len
+
+            else:
+                sig_H, sig_W, sig_C = sig_item.size()
+                # TODO: review.
+                # why reshape(sig_H * sig_W, sig_C) after permute
+                # "patchify"
+                sig_flat = sig_item.permute(2, 0, 1).reshape(sig_H * sig_W, sig_C)
+
+                # sig_out, sig_pos, sig_mask, sig_len, sig_nm = self._pad_with_ids(
+                #     sig_flat, (1, sig_H, sig_W), (cap_end_pos[j] + 1, 0, 0), device, noise_val
+                # )
+
+                # TODO: nm
+                # noise mask
+
+                # sig_ori_len = sig_flat.size(0)
+                # sig_padding_len = (-sig_ori_len) % SEQ_MULTI_OF
+                # # padded feature
+                # sig_padded_feat = torch.cat(
+                #     [sig_flat, sig_flat[-1:].repeat(sig_padding_len, 1)],
+                #     dim=0,
+                # )
+
+                sig_padded_feat, sig_len, sig_nm = self._pad_and_prepare_noise_mask(
+                    sig_flat,
+                    noise_val,
+                )
+
+            all_sig_feats_list.append(sig_padded_feat)
+            all_sig_noise_mask_list.extend(sig_nm)
+            sig_lens.append(sig_len)
+        all_sig_out.append(torch.cat(all_sig_feats_list, dim=0))
+        all_sig_noise_mask.append(all_sig_noise_mask_list)
+        all_sig_len.append(sig_lens)
+
+        # Compute x position offsets
+        all_image_pos_offsets = [
+            (sum(all_cap_len[i]), sum(all_cap_len[i]) + sum(all_image_len[i]))
+            for i in range(bsz)
+        ]
+
+        return (
+            all_image_out,
+            all_cap_feats_out,
+            all_image_size,
+            all_sig_out,
+            all_image_pos_offsets,
+            all_image_noise_mask,
+            all_cap_noise_mask,
+            all_sig_noise_mask,
+        )
+
+    def _pad_and_prepare_noise_mask(
+        self,
+        feat: torch.Tensor,
+        noise_mask_val: Optional[int] = None,
+    ):
+        """
+        noise_mask_val == 1 fill noise
+        noise_mask_val == 0 fill empty
+
+        Args:
+        Returns:
+        """
+        ori_len = feat.size(0)
+        padding_len = (-ori_len) % SEQ_MULTI_OF
+        total_len = ori_len + padding_len
+        # padded feature
+        padded_feat = torch.cat(
+            [feat, feat[-1:].repeat(padding_len, 1)],
+            dim=0,
+        )
+        # TODO: review
+        noise_mask = (
+            [noise_mask_val] * total_len if noise_mask_val is not None else None
+        )  # token level
+
+        return padded_feat, total_len, noise_mask
+
     def patchify_and_embed(
         self,
         all_image: List[torch.Tensor],
@@ -541,13 +903,26 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         patch_size: int,
         f_patch_size: int,
     ):
+        """
+        Patchify and padding for basic mode: single image per batch item.
+
+        Args:
+
+        Returns:
+            all_image_out (list): List of padded image_feat(patchified image).
+            all_cap_feats_out (list): List of padded caption_feat
+            all_image_size (list): List of image size (F, H, W)
+        """
         assert len(all_image) == len(all_cap_feats) == 1
 
+        # TODO: reivew
+        # hard code batch size = 1 for now.?
+
         image = all_image[0]  # C, F, H, W
         cap_feat = all_cap_feats[0]  # L, D
-        pH = pW = patch_size
-        pF = f_patch_size
-        device = image.device
+        # pH = pW = patch_size
+        # pF = f_patch_size
+        # device = image.device
 
         all_image_out = []
         all_image_size = []
@@ -565,15 +940,21 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         all_cap_feats_out.append(cap_padded_feat)
 
         # ------------ Process Image ------------
-        C, F, H, W = image.size()
-        all_image_size.append((F, H, W))
+        # C, F, H, W = image.size()
 
-        F_tokens, H_tokens, W_tokens = F // pF, H // pH, W // pW
-        image = image.view(C, F_tokens, pF, H_tokens, pH, W_tokens, pW)
+        # all_image_size.append((F, H, W))
+        # F_tokens, H_tokens, W_tokens = F // pF, H // pH, W // pW
+        # image = image.view(C, F_tokens, pF, H_tokens, pH, W_tokens, pW)
         # "c f pf h ph w pw -> (f h w) (pf ph pw c)"
-        image = image.permute(1, 3, 5, 2, 4, 6, 0).reshape(
-            F_tokens * H_tokens * W_tokens, pF * pH * pW * C
+        # image = image.permute(1, 3, 5, 2, 4, 6, 0).reshape(
+        #     F_tokens * H_tokens * W_tokens, pF * pH * pW * C
+        # )
+
+        image, (F, H, W), _ = self._patchify_image(
+            image, patch_size=patch_size, f_patch_size=f_patch_size
         )
+        all_image_size.append((F, H, W))
+
         image_ori_len = image.size(0)
         image_padding_len = (-image_ori_len) % SEQ_MULTI_OF
 
@@ -590,33 +971,401 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
             all_image_size,
         )
 
+    def _patchify_image(self, image: torch.Tensor, patch_size: int, f_patch_size: int):
+        """
+        Patchify a single image tensor: (C, F, H, W) -> (num_patches, patch_dim).
+
+        Args:
+
+        Returns:
+            image_feat (tensor): patchify image
+            image_size (tuple[int, int, int]): image size in (F, H, W)
+            image_num_tokens (tuple[int, int, int]): image num tokens (F // pF, H // pH, W // pW)
+        """
+
+        pH, pW, pF = patch_size, patch_size, f_patch_size
+        C, F, H, W = image.size()
+        F_tokens, H_tokens, W_tokens = F // pF, H // pH, W // pW
+        image = image.view(C, F_tokens, pF, H_tokens, pH, W_tokens, pW)
+        image = image.permute(1, 3, 5, 2, 4, 6, 0).reshape(
+            F_tokens * H_tokens * W_tokens, pF * pH * pW * C
+        )
+        return image, (F, H, W), (F_tokens, H_tokens, W_tokens)
+
+    def _build_unified_sequence(
+        self,
+        x: torch.Tensor,
+        x_freqs: Tuple[torch.Tensor, torch.Tensor],  # (cos, sin)
+        x_seqlens: List[int],
+        x_noise_mask: Optional[List[List[int]]],
+        cap: torch.Tensor,
+        cap_freqs: Tuple[torch.Tensor, torch.Tensor],
+        cap_seqlens: List[int],
+        cap_noise_mask: Optional[List[List[int]]],
+        siglip: Optional[torch.Tensor],
+        siglip_freqs: Optional[Tuple[torch.Tensor, torch.Tensor]],
+        siglip_seqlens: Optional[List[int]],
+        siglip_noise_mask: Optional[List[List[int]]],
+        omni_mode: bool,
+        device: torch.device,
+    ):
+        """
+        Concat feats into a single.
+
+        Build unified sequence: x, cap, and optionally siglip.
+        - Basic mode order: [x, cap]
+        - Omni mode order: [cap, x, siglip]
+        """
+
+        # TODO: single batch only for now.
+        bsz = len(x_seqlens)
+        unified = []
+        unified_freqs_cos = []
+        unified_freqs_sin = []
+        unified_noise_mask = []
+
+        for i in range(bsz):
+            x_len, cap_len = x_seqlens[i], cap_seqlens[i]
+
+            if omni_mode:
+                # Omni: [cap, x, siglip]
+                if siglip is not None and siglip_seqlens is not None:
+                    sig_len = siglip_seqlens[i]
+                    unified.append(
+                        torch.cat([cap[i][:cap_len], x[i][:x_len], siglip[i][:sig_len]])
+                    )
+                    # TODO: review, hack
+                    unified_freqs_cos.append(
+                        torch.cat(
+                            [
+                                cap_freqs[0][:cap_len],
+                                x_freqs[0][:x_len],
+                                siglip_freqs[0][:sig_len],
+                            ]
+                        )
+                    )
+                    unified_freqs_sin.append(
+                        torch.cat(
+                            [
+                                cap_freqs[1][:cap_len],
+                                x_freqs[1][:x_len],
+                                siglip_freqs[1][:sig_len],
+                            ]
+                        )
+                    )
+                    unified_noise_mask.append(
+                        torch.tensor(
+                            cap_noise_mask[i] + x_noise_mask[i] + siglip_noise_mask[i],
+                            dtype=torch.long,
+                            device=device,
+                        )
+                    )
+                else:
+                    unified.append(torch.cat([cap[i][:cap_len], x[i][:x_len]]))
+                    unified_freqs_cos.append(
+                        torch.cat([cap_freqs[0][:cap_len], x_freqs[0][:x_len]])
+                    )
+                    unified_freqs_sin.append(
+                        torch.cat([cap_freqs[1][:cap_len], x_freqs[1][:x_len]])
+                    )
+                    unified_noise_mask.append(
+                        torch.tensor(
+                            cap_noise_mask[i] + x_noise_mask[i],
+                            dtype=torch.long,
+                            device=device,
+                        )
+                    )
+            else:
+                # Basic: [x, cap]
+                unified.append(torch.cat([x[i][:x_len], cap[i][:cap_len]]))
+                unified_freqs_cos.append(
+                    torch.cat([x_freqs[0][:x_len], cap_freqs[0][:cap_len]])
+                )
+                unified_freqs_sin.append(
+                    torch.cat([x_freqs[1][:x_len], cap_freqs[1][:cap_len]])
+                )
+
+        # TODO: single batch only
+        # no batch pad
+        assert len(unified) == 1, "Single batch only for now."
+        assert len(unified_freqs_cos) == 1, "Single batch only for now."
+        unified = unified[0].unsqueeze(0)
+        # unified_freqs = unified_freqs[0].unsqueeze(0)
+        unified_freqs = (unified_freqs_cos[0], unified_freqs_sin[0])
+
+        # Noise mask
+        noise_mask_tensor = None
+        if omni_mode:
+            assert len(unified_noise_mask) == 1, "Single batch only for now."
+            noise_mask_tensor = unified_noise_mask[0][: unified.shape[1]].unsqueeze(0)
+
+        return unified, unified_freqs, noise_mask_tensor
+
+    # TODO: copy to configs/zimage.py later
+    def get_freqs_cis(
+        self,
+        prompt_embeds: List[torch.Tensor],
+        images: List[torch.Tensor],
+        siglips: List[torch.Tensor],
+        device,
+        rotary_emb,
+    ):
+        def _get_pos_ids(
+            ori_len: int,
+            pos_grid_size: Tuple,
+            pos_start: Tuple,
+            device: torch.device,
+        ):
+            pad_len = (-ori_len) % SEQ_MULTI_OF
+            total_len = ori_len + pad_len
+
+            # Pos IDs
+            ori_pos_ids = self.create_coordinate_grid(
+                size=pos_grid_size, start=pos_start, device=device
+            ).flatten(0, 2)
+            if pad_len > 0:
+                pad_pos_ids = (
+                    self.create_coordinate_grid(
+                        size=(1, 1, 1), start=(0, 0, 0), device=device
+                    )
+                    .flatten(0, 2)
+                    .repeat(pad_len, 1)
+                )
+                pos_ids = torch.cat([ori_pos_ids, pad_pos_ids], dim=0)
+            else:
+                pos_ids = ori_pos_ids
+
+            return pos_ids
+
+        # TODO: assert batch size == 1
+
+        # TODO: hard code....
+        PATCH_SIZE = 2
+        F_PATCH_SIZE = 1
+        SEQ_MULTI_OF = 32
+
+        # cap_start_pos for cap pos ids
+        cap_cu_len = 1
+        # cap_end_pos + 0 for image pos ids
+        # cap_end_pos + 1 for image siglip ids
+        cap_end_pos = []
+
+        image_size = []
+
+        cap_pos_ids_list = []
+        image_pos_ids_list = []
+        siglip_pos_ids_list = []
+
+        for cap_item in prompt_embeds:
+            cap_padded_pos_ids = _get_pos_ids(
+                len(cap_item),
+                (len(cap_item) + (-len(cap_item)) % SEQ_MULTI_OF, 1, 1),
+                (cap_cu_len, 0, 0),
+                device,
+            )
+            cap_cu_len += len(cap_item)
+            cap_end_pos.append(cap_cu_len)
+            cap_cu_len += 2  # for image vae and siglip tokens
+            cap_pos_ids_list.append(cap_padded_pos_ids)
+
+        for j, image in enumerate(images):
+            if image is not None:
+                pH, pW, pF = PATCH_SIZE, PATCH_SIZE, F_PATCH_SIZE
+                C, F, H, W = image.size()
+                F_t, H_t, W_t = F // pF, H // pH, W // pW
+                image = image.view(C, F_t, pF, H_t, pH, W_t, pW)
+                image = image.permute(1, 3, 5, 2, 4, 6, 0).reshape(
+                    F_t * H_t * W_t, pF * pH * pW * C
+                )
+                image_pos = _get_pos_ids(
+                    F_t * H_t * W_t, (F_t, H_t, W_t), (cap_end_pos[j], 0, 0), device
+                )
+                image_size.append((F, H, W))
+            else:
+                # TODO: review
+                # when will this happen?
+                image_len = SEQ_MULTI_OF
+                image_pos = (
+                    self.create_coordinate_grid((1, 1, 1), (0, 0, 0), device)
+                    .flatten(0, 2)
+                    .repeat(image_len, 1)
+                )
+                image_size.append(None)
+
+            image_pos_ids_list.append(image_pos)
+
+        # TODO: ugly hack
+        for j, sig_item in enumerate(siglips if siglips is not None else []):
+            if sig_item is not None:
+                # TODO: review
+                # diff
+                # shape = (1, xx , xx)???
+                sig_H, sig_W, sig_C = sig_item.size()
+                sig_flat = sig_item.permute(2, 0, 1).reshape(sig_H * sig_W, sig_C)
+                sig_pos = _get_pos_ids(
+                    len(sig_flat),
+                    (1, sig_H, sig_W),
+                    (cap_end_pos[j] + 1, 0, 0),
+                    device,
+                )
+                # Scale position IDs to match x resolution
+                if image_size[j] is not None:
+                    sig_pos = sig_pos.float()
+                    sig_pos[..., 1] = (
+                        sig_pos[..., 1] / max(sig_H - 1, 1) * (image_size[j][1] - 1)
+                    )
+                    sig_pos[..., 2] = (
+                        sig_pos[..., 2] / max(sig_W - 1, 1) * (image_size[j][2] - 1)
+                    )
+                    sig_pos = sig_pos.to(torch.int32)
+            else:
+                # TODO: review
+                # when will this happen?
+                sig_len = SEQ_MULTI_OF
+                sig_pos = (
+                    self.create_coordinate_grid((1, 1, 1), (0, 0, 0), device)
+                    .flatten(0, 2)
+                    .repeat(sig_len, 1)
+                )
+            siglip_pos_ids_list.append(sig_pos)
+
+        cap_freqs_cis = rotary_emb(torch.cat(cap_pos_ids_list, dim=0))
+        x_freqs_cis = rotary_emb(torch.cat(image_pos_ids_list, dim=0))
+        siglip_freqs_cis = (
+            rotary_emb(torch.cat(siglip_pos_ids_list, dim=0))
+            if len(siglip_pos_ids_list) != 0
+            else None
+        )
+        return (cap_freqs_cis, x_freqs_cis, siglip_freqs_cis)
+
     def forward(
         self,
-        hidden_states: List[torch.Tensor],
-        encoder_hidden_states: List[torch.Tensor],
+        hidden_states: Union[List[torch.Tensor], List[List[torch.Tensor]]],
+        encoder_hidden_states: Union[List[torch.Tensor], List[List[torch.Tensor]]],
         timestep,
         guidance=0,
         patch_size=2,
         f_patch_size=1,
         freqs_cis=None,
+        siglip_feats: Optional[List[List[torch.Tensor]]] = None,
+        image_noise_mask: Optional[List[List[int]]] = None,
+        condition_latents: Optional[List[List[torch.Tensor]]] = None,
+        token_lens: List[int] = None,
         **kwargs,
     ):
+        # TODO: ignore control net for now.
         assert patch_size in self.all_patch_size
         assert f_patch_size in self.all_f_patch_size
 
+        # TODO: hard code bsz1
+        assert len(hidden_states) == 1, f"{len(hidden_states)=}"
+        assert len(encoder_hidden_states) == 1, f"{len(encoder_hidden_states)=}"
+        assert siglip_feats is None or len(siglip_feats) == 1, f"{len(siglip_feats)=}"
+
+        batch_size = len(hidden_states)
+        # construct a 2d list for omni mode
+        # dim0 = batch_size
+        # dim1 = latents: condition images + 1 target image
+        if condition_latents is not None:
+            assert batch_size == 1, "Only batch_size=1 is tested for now."
+            assert isinstance(condition_latents, list) and isinstance(
+                condition_latents[0], list
+            ), "condition_latents should be a 2d list."
+            assert len(condition_latents) == batch_size, "batch size mismatch"
+            hidden_states = [
+                condition_latents[i] + [hidden_states[i]] for i in range(batch_size)
+            ]
+
         x = hidden_states
+
+        # TODO: review
+        # ugly?
+        # can be
+        # omni_mode = condition_latents is not None
+        omni_mode = isinstance(x[0], list)
+        device = x[0][-1].device if omni_mode else x[0].device
+
+        if omni_mode:
+            # TODO: review
+            # ugly hack
+            encoder_hidden_states = [
+                list(encoder_hidden_states[0].split_with_sizes(token_lens, dim=0))
+            ]
         cap_feats = encoder_hidden_states
         timestep = 1000.0 - timestep
         t = timestep
-        bsz = 1
-        device = x[0].device
-        t = self.t_embedder(t)
-        adaln_input = t.type_as(x)
-        (
-            x,
-            cap_feats,
-            x_size,
-        ) = self.patchify_and_embed(x, cap_feats, patch_size, f_patch_size)
+
+        if omni_mode:
+            # Dual embeddings: noisy (t) and clean (t=1)
+            t_noisy = self.t_embedder(t).type_as(x[0][-1])
+            t_clean = self.t_embedder(torch.ones_like(t) * self.t_scale).type_as(
+                x[0][-1]
+            )
+            adaln_input = None
+        else:
+            # Single embedding for all tokens
+            adaln_input = self.t_embedder(t).type_as(x[0])
+            t_noisy = t_clean = None
+
+        # TODO: overwrite freqs_cis for debug
+        # TODO: single batch only
+        # compute freqs before patchify
+        # TODO: ugly
+        if freqs_cis is None:
+            # omni-mode online compute
+            # TODO: try pre-compute
+            freqs_cis = self.get_freqs_cis(
+                prompt_embeds=(
+                    encoder_hidden_states[0]
+                    if isinstance(encoder_hidden_states[0], list)
+                    else encoder_hidden_states
+                ),
+                images=(
+                    hidden_states[0]
+                    if isinstance(hidden_states[0], list)
+                    else hidden_states
+                ),
+                siglips=siglip_feats[0] if siglip_feats is not None else None,
+                device=device,
+                rotary_emb=getattr(self, "rotary_emb", None),
+            )
+
+        # Patchify
+        if omni_mode:
+            (
+                x,
+                cap_feats,
+                x_size,
+                siglip_feats,
+                x_pos_offsets,
+                x_noise_mask,
+                cap_noise_mask,
+                siglip_noise_mask,
+            ) = self.patchify_and_embed_omni(
+                x,
+                cap_feats,
+                patch_size,
+                f_patch_size,
+                siglip_feats,
+                image_noise_mask,
+            )
+        else:
+            (
+                x,
+                cap_feats,
+                x_size,
+            ) = self.patchify_and_embed(x, cap_feats, patch_size, f_patch_size)
+            x_pos_offsets = x_noise_mask = cap_noise_mask = siglip_noise_mask = None
+
+        # TODO: debug assert
+        assert isinstance(x, list) and isinstance(x[0], torch.Tensor), f"{type(x)=}"
+        assert isinstance(cap_feats, list) and isinstance(
+            cap_feats[0], torch.Tensor
+        ), f"{type(cap_feats)=}"
+        assert siglip_feats is None or (
+            isinstance(siglip_feats, list) and isinstance(siglip_feats[0], torch.Tensor)
+        ), f"{type(siglip_feats)=}"
+        assert len(x_size) == 1, f"bsz should be 1. got {len(x_size)=}"
 
         x = torch.cat(x, dim=0)
         x, _ = self.all_x_embedder[f"{patch_size}-{f_patch_size}"](x)
@@ -624,8 +1373,14 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
 
         x = x.unsqueeze(0)
         x_freqs_cis = x_freqs_cis
+        x_noise_tensor = None
+        # TODO: ugly hack
+        if x_noise_mask is not None:
+            x_noise_tensor = torch.stack(
+                [torch.tensor(m, dtype=torch.long, device=device) for m in x_noise_mask]
+            )
         for layer in self.noise_refiner:
-            x = layer(x, x_freqs_cis, adaln_input)
+            x = layer(x, x_freqs_cis, adaln_input, x_noise_tensor, t_noisy, t_clean)
 
         cap_feats = torch.cat(cap_feats, dim=0)
 
@@ -637,20 +1392,81 @@ class ZImageTransformer2DModel(CachableDiT, OffloadableDiTMixin):
         for layer in self.context_refiner:
             cap_feats = layer(cap_feats, cap_freqs_cis)
 
-        unified = torch.cat([x, cap_feats], dim=1)
-        unified_freqs_cis = (
-            torch.cat([x_freqs_cis[0], cap_freqs_cis[0]], dim=0),
-            torch.cat([x_freqs_cis[1], cap_freqs_cis[1]], dim=0),
+        siglip_freqs_cis = siglip_seqlens = siglip_freqs = None
+
+        if (
+            omni_mode
+            and siglip_feats[0] is not None
+            and self.siglip_embedder is not None
+        ):
+            siglip_freqs_cis = freqs_cis[2]
+            # process siglip
+            # TODO: review shape
+            siglip_feats = torch.cat(siglip_feats, dim=0)
+            siglip_feats = siglip_feats.unsqueeze(0)
+
+            siglip_seqlens = [len(si) for si in siglip_feats]
+            # embed
+            siglip_feats = self.siglip_embedder(siglip_feats)
+            # TODO:
+            # single batch
+            # no pad
+
+            for layer in self.siglip_refiner:
+                siglip_feats = layer(siglip_feats, siglip_freqs_cis)
+
+        assert len(x) == 1, "single batch only."
+        assert len(cap_feats) == 1, "single batch only."
+        assert isinstance(
+            x_freqs_cis, Tuple
+        ), f"should be tuple of (cos, sin), got {type(x_freqs_cis)}"
+        assert isinstance(
+            cap_freqs_cis, Tuple
+        ), f"should be tuple of (cos, sin), got {type(cap_freqs_cis)}"
+        assert siglip_freqs_cis is None or isinstance(
+            siglip_freqs_cis, Tuple
+        ), f"should be tuple of (cos, sin), got {type(siglip_freqs_cis)}"
+
+        x_seqlens = [len(xi) for xi in x]
+        cap_seqlens = [len(ci) for ci in cap_feats]
+        unified, unified_freqs_cis, unified_noise_tensor = self._build_unified_sequence(
+            x=x,
+            x_freqs=x_freqs_cis,
+            x_seqlens=x_seqlens,
+            x_noise_mask=x_noise_mask,
+            cap=cap_feats,
+            cap_freqs=cap_freqs_cis,
+            cap_seqlens=cap_seqlens,
+            cap_noise_mask=cap_noise_mask,
+            siglip=siglip_feats,
+            siglip_freqs=siglip_freqs_cis,
+            siglip_seqlens=siglip_seqlens,
+            siglip_noise_mask=siglip_noise_mask,
+            omni_mode=omni_mode,
+            device=device,
         )
 
         for layer in self.layers:
-            unified = layer(unified, unified_freqs_cis, adaln_input)
+            unified = layer(
+                unified,
+                unified_freqs_cis,
+                adaln_input,
+                unified_noise_tensor,
+                t_noisy,
+                t_clean,
+            )
+            # TODO: controlnet is ignore for now.
+            # if controlnet ...
 
         unified = self.all_final_layer[f"{patch_size}-{f_patch_size}"](
-            unified, adaln_input
+            unified,
+            adaln_input,
+            noise_mask=unified_noise_tensor,
+            c_noisy=t_noisy,
+            c_clean=t_clean,
         )
         unified = list(unified.unbind(dim=0))
-        x = self.unpatchify(unified, x_size, patch_size, f_patch_size)
+        x = self.unpatchify(unified, x_size, patch_size, f_patch_size, x_pos_offsets)
 
         return -x[0]
 
diff --git a/python/sglang/multimodal_gen/runtime/pipelines/zimage_pipeline.py b/python/sglang/multimodal_gen/runtime/pipelines/zimage_pipeline.py
index f8fd441de..ccc8a8d6e 100644
--- a/python/sglang/multimodal_gen/runtime/pipelines/zimage_pipeline.py
+++ b/python/sglang/multimodal_gen/runtime/pipelines/zimage_pipeline.py
@@ -1,7 +1,10 @@
 # Copied and adapted from: https://github.com/hao-ai-lab/FastVideo
 # SPDX-License-Identifier: Apache-2.0
 
+import torch
 
+from sglang.multimodal_gen.runtime.distributed import get_local_torch_device
+from sglang.multimodal_gen.runtime.models.vision_utils import load_image, load_video
 from sglang.multimodal_gen.runtime.pipelines_core import LoRAPipeline, Req
 from sglang.multimodal_gen.runtime.pipelines_core.composed_pipeline_base import (
     ComposedPipelineBase,
@@ -15,6 +18,7 @@ from sglang.multimodal_gen.runtime.pipelines_core.stages import (
     TextEncodingStage,
     TimestepPreparationStage,
 )
+from sglang.multimodal_gen.runtime.pipelines_core.stages.base import PipelineStage
 from sglang.multimodal_gen.runtime.server_args import ServerArgs
 from sglang.multimodal_gen.runtime.utils.logging_utils import init_logger
 
@@ -113,4 +117,385 @@ class ZImagePipeline(LoRAPipeline, ComposedPipelineBase):
         )
 
 
-EntryClass = ZImagePipeline
+class _ImageProcessStage(PipelineStage):
+    def __init__(
+        self,
+        image_processor,
+        vae_scale_factor,
+    ) -> None:
+        self.image_processor = image_processor
+        self.vae_scale_factor = vae_scale_factor
+
+    def forward(
+        self,
+        batch: Req,
+        server_args: ServerArgs,
+    ) -> Req:
+        """
+        Args:
+            batch.height
+            batch.width
+            batch.image_path
+        Returns:
+            batch.condition_image
+            batch.original_condition_image_size
+        """
+        # 3. Process condition images. Copied from diffusers.pipelines.flux2.pipeline_flux2
+        condition_image = []
+        resized_images = []
+        height = batch.height
+        width = batch.width
+
+        if batch.image_path is not None:
+            for path in batch.image_path:
+                if path.endswith(".mp4"):
+                    raise NotImplementedError("unimplemented")
+                    img = load_video(path)[0]
+                else:
+                    img = load_image(path)
+
+                # check img input
+                self.image_processor.check_image_input(img)
+
+                image_width, image_height = img.size
+                # resize to (1024, 1024) if (height, width) is not define
+                if image_width * image_height > 1024 * 1024:
+                    if height is not None and width is not None:
+                        img = self.image_processor._resize_to_target_area(
+                            img, height * width
+                        )
+                    else:
+                        img = self.image_processor._resize_to_target_area(
+                            img, 1024 * 1024
+                        )
+                    image_width, image_height = img.size
+                resized_images.append(img)
+
+                multiple_of = self.vae_scale_factor * 2
+                image_width = (image_width // multiple_of) * multiple_of
+                image_height = (image_height // multiple_of) * multiple_of
+                img = self.image_processor.preprocess(
+                    img, height=image_height, width=image_width, resize_mode="crop"
+                )
+                condition_image.append(img)
+
+            if len(condition_image) > 0:
+                height = height or image_height
+                width = width or image_width
+
+        vae_scale = self.vae_scale_factor * 2
+        if height % vae_scale != 0:
+            raise ValueError(
+                f"Height must be divisible by {vae_scale} (got {height}). "
+                f"Please adjust the height to a multiple of {vae_scale}."
+            )
+        if width % vae_scale != 0:
+            raise ValueError(
+                f"Width must be divisible by {vae_scale} (got {width}). "
+                f"Please adjust the width to a multiple of {vae_scale}."
+            )
+
+        # TODO:
+        # hard code with image size
+        # dulplicate with input_validation_stage
+        batch.height = image_height
+        batch.width = image_width
+
+        # TODO: hard code debug
+        # should be condition_images(a list)
+        # condition_images -> prepare_image_latents
+        batch.condition_image = condition_image
+        # prepare for
+        # resized_images -> prepare_siglip_embeds
+        batch.resized_images = resized_images
+        return batch
+
+
+class _PrepareImageLatentsStage(PipelineStage):
+    def __init__(self, vae) -> None:
+        self.vae = vae
+
+    def forward(
+        self,
+        batch: Req,
+        server_args: ServerArgs,
+    ) -> Req:
+        """
+        Args:
+            batch.condition_image
+        Returns
+            TODO
+        """
+        image_latents = []
+        images = batch.condition_image
+        num_images_per_prompt = (
+            len(batch.image_path) if batch.image_path is not None else 0
+        )
+        assert batch.batch_size == 1, f"under test: {batch.batch_size=}"
+
+        # TODO: skip
+        # review
+        if num_images_per_prompt == 0:
+            return batch
+
+        batch_size = batch.batch_size * num_images_per_prompt
+        # TODO: hardcode debug
+        # a graceful way to do that?
+        dtype = (
+            batch.prompt_embeds[0].dtype
+            if not isinstance(batch.prompt_embeds[0], list)
+            else batch.prompt_embeds[0][0].dtype
+        )
+        device = get_local_torch_device()
+
+        # TODO: hard code debug
+        # do nothing and skip
+        if images is None:
+            return batch
+
+        self.vae = self.vae.to(device)
+
+        for image in images:
+            image = image.to(device=device, dtype=dtype)
+            image_latent = (
+                # TODO: hard code to vae(fp32) dtype. reivew
+                self.vae.encode(image.to(torch.float32)).latent_dist.mode()[0]
+                - self.vae.config.shift_factor
+            ) * self.vae.config.scaling_factor
+            image_latent = image_latent.unsqueeze(1).to(dtype)
+            image_latents.append(image_latent)  # (16, 128, 128)
+
+        # image_latents = [image_latents] * batch_size
+        # TODO: review no copy?
+        # num_images_per_prompt * batch_size
+        # a 2d list
+        # dim0 = batch_size
+        # dim1 = num_images
+        # condition_latents = [image_latents.copy() for _ in range(batch_size)]
+        condition_latents = [image_latents for _ in range(batch_size)]
+        # TODO: review
+        # casting
+        condition_latents = [
+            [lat.to(dtype) for lat in lats] for lats in condition_latents
+        ]
+
+        # negative_condition_latents = [
+        #     [None for lat in lats] for lats in condition_latents
+        # ]
+        # TODO: review, flag by None
+        negative_condition_latents = None
+        if batch.do_classifier_free_guidance:
+            # negative_condition_latents = [
+            #     [lat.clone() for lat in batch] for batch in condition_latents
+            # ]
+            # TODO: review, readonly assert?
+            negative_condition_latents = condition_latents
+
+        # TODO: comment usage
+        # for condition_latents_model_input = condition_latents + negative_condition_latents
+        # in denoising loop
+        batch.condition_latents = condition_latents
+        batch.negative_condition_latents = negative_condition_latents
+
+        return batch
+
+
+class _PrepareSiglipStage(PipelineStage):
+    def __init__(self, transformer, siglip, siglip_processor) -> None:
+        self.transformer = transformer
+        self.siglip = siglip
+        self.siglip_processor = siglip_processor
+
+    def forward(
+        self,
+        batch: Req,
+        server_args: ServerArgs,
+    ) -> Req:
+        """
+        Args:
+            batch.resized_images
+        Returns:
+            TODO
+        """
+        siglip_embeds = []
+        images = batch.resized_images
+        # TODO: hard code
+        if images is None:
+            return batch
+
+        # TODO: hard code
+        device = get_local_torch_device()
+        # TODO: review?
+        # self.siglip_processor = self.siglip_processor.to(device)
+        self.siglip = self.siglip.to(device)
+
+        num_images_per_prompt = (
+            len(batch.image_path) if batch.image_path is not None else 0
+        )
+        batch_size = batch.batch_size * num_images_per_prompt
+        # TODO: hard code
+        dtype = torch.bfloat16
+        do_classifier_free_guidance = batch.do_classifier_free_guidance
+
+        for image in images:
+            siglip_inputs = (
+                self.siglip_processor(images=[image], return_tensors="pt")
+                .to(device)
+                .to(dtype)
+            )
+            shape = siglip_inputs.spatial_shapes[0]
+            hidden_state = self.siglip(**siglip_inputs).last_hidden_state
+            B, N, C = hidden_state.shape
+            hidden_state = hidden_state[:, : shape[0] * shape[1]]
+            hidden_state = hidden_state.view(shape[0], shape[1], C)
+            siglip_embeds.append(hidden_state)
+
+        # siglip_embeds = [siglip_embeds] * batch_size
+        # TODO: 2D list.
+        # where
+        #  len(l) == batch size
+        #  len(l[0]) == image nums
+        condition_siglip_embeds = [siglip_embeds.copy() for _ in range(batch_size)]
+
+        # TODO: review
+        ## ====================
+        # dtype cast
+        condition_siglip_embeds = [
+            [se for se in sels] for sels in condition_siglip_embeds
+        ]
+
+        # TODO: placeholder and end with None
+        # pad None at the end
+        condition_siglip_embeds = [
+            None if sels == [] else sels + [None] for sels in condition_siglip_embeds
+        ]
+
+        negative_condition_siglip_embeds = None
+        if do_classifier_free_guidance:
+            # negative_condition_siglip_embeds = [
+            #     [se.clone() for se in batch] for batch in condition_siglip_embeds
+            # ]
+            # # TODO: debug remove
+            # negative_condition_siglip_embeds = [
+            #     None if sels == [] else sels + [None]
+            #     for sels in negative_condition_siglip_embeds
+            # ]
+
+            # TODO: review, readonly modify assert
+            negative_condition_siglip_embeds = condition_siglip_embeds
+
+        # TODO: for siglip_feats = pos + neg
+        # in denoising loop
+        batch.condition_siglip_embeds = condition_siglip_embeds
+        # TODO: debug remove
+        batch.negative_condition_siglip_embeds = negative_condition_siglip_embeds
+        return batch
+
+
+class ZImageOmniPipeline(ZImagePipeline):
+    pipeline_name = "ZImageOmniPipeline"
+
+    # TODO: review how to add extra component?
+    _extra_config_module_map = {
+        "siglip": "image_encoder",
+        "siglip_processor": "processor",
+    }
+    _required_config_modules = [
+        "text_encoder",
+        "tokenizer",
+        "vae",
+        "transformer",
+        "scheduler",
+        "siglip",
+        "siglip_processor",
+    ]
+
+    def create_pipeline_stages(self, server_args: ServerArgs):
+        """Set up pipeline stages with proper dependency injection."""
+
+        # copy from diffusers
+        from diffusers.pipelines.flux2.image_processor import Flux2ImageProcessor
+
+        vae_scale_factor = server_args.pipeline_config.vae_config.vae_scale_factor
+        # NOTE: replace vae with Flux in zimage-omni
+        self.image_processor = Flux2ImageProcessor(
+            vae_scale_factor=vae_scale_factor * 2
+        )
+
+        self.add_stage(
+            stage_name="input_validation_stage", stage=InputValidationStage()
+        )
+
+        self.add_stage(
+            stage_name="prompt_encoding_stage_primary",
+            stage=TextEncodingStage(
+                text_encoders=[
+                    self.get_module("text_encoder"),
+                ],
+                tokenizers=[
+                    self.get_module("tokenizer"),
+                ],
+            ),
+        )
+
+        # TODO: dulplicate with InputValidationStage:229
+        # refactory later
+        self.add_stage(
+            stage_name="image_process",
+            stage=_ImageProcessStage(
+                image_processor=self.image_processor,
+                vae_scale_factor=vae_scale_factor,
+            ),
+        )
+
+        self.add_stage(stage_name="conditioning_stage", stage=ConditioningStage())
+
+        self.add_stage(
+            stage_name="timestep_preparation_stage",
+            stage=TimestepPreparationStage(
+                scheduler=self.get_module("scheduler"),
+                prepare_extra_set_timesteps_kwargs=[prepare_mu],
+            ),
+        )
+
+        self.add_stage(
+            stage_name="latent_preparation_stage",
+            stage=LatentPreparationStage(
+                scheduler=self.get_module("scheduler"),
+                transformer=self.get_module("transformer"),
+            ),
+        )
+
+        self.add_stage(
+            stage_name="image_siglip_preparation_stage",
+            stage=_PrepareSiglipStage(
+                transformer=self.get_module("transformer"),
+                siglip=self.get_module("siglip"),
+                siglip_processor=self.get_module("siglip_processor"),
+            ),
+        )
+
+        self.add_stage(
+            stage_name="image_latent_preparation_stage",
+            stage=_PrepareImageLatentsStage(
+                vae=self.get_module("vae"),
+            ),
+        )
+
+        self.add_stage(
+            stage_name="denoising_stage",
+            stage=DenoisingStage(
+                transformer=self.get_module("transformer"),
+                scheduler=self.get_module("scheduler"),
+            ),
+        )
+
+        self.add_stage(
+            stage_name="decoding_stage", stage=DecodingStage(vae=self.get_module("vae"))
+        )
+
+
+EntryClass = [
+    ZImagePipeline,
+    ZImageOmniPipeline,
+]
diff --git a/python/sglang/multimodal_gen/runtime/pipelines_core/composed_pipeline_base.py b/python/sglang/multimodal_gen/runtime/pipelines_core/composed_pipeline_base.py
index 02dd23631..787e8dc05 100644
--- a/python/sglang/multimodal_gen/runtime/pipelines_core/composed_pipeline_base.py
+++ b/python/sglang/multimodal_gen/runtime/pipelines_core/composed_pipeline_base.py
@@ -262,6 +262,10 @@ class ComposedPipelineBase(ABC):
             else:
                 load_module_name = module_name
 
+            # TODO: review !!!
+            # component_model_path should be target module NAME/PATH
+            # while load_module_name is the "loader" type?
+
             # Use custom VAE path if provided, otherwise use default path
             if module_name == "vae" and server_args.vae_path is not None:
                 component_model_path = server_args.vae_path
@@ -271,10 +275,10 @@ class ComposedPipelineBase(ABC):
                 logger.info(
                     "Using custom VAE path: %s instead of default path: %s",
                     component_model_path,
-                    os.path.join(self.model_path, load_module_name),
+                    os.path.join(self.model_path, module_name),
                 )
             else:
-                component_model_path = os.path.join(self.model_path, load_module_name)
+                component_model_path = os.path.join(self.model_path, module_name)
             module, memory_usage = PipelineComponentLoader.load_module(
                 module_name=load_module_name,
                 component_model_path=component_model_path,
diff --git a/python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py b/python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py
index 7c95bc8b1..1f5911800 100644
--- a/python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py
+++ b/python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py
@@ -139,6 +139,17 @@ class Req:
     # results
     output: torch.Tensor | None = None
 
+    #
+    ## TODO: debug member
+    #
+
+    # for prepare_siglip_embeds
+    resized_images = None
+    condition_latents = None
+    negative_condition_latents = None
+    condition_siglip_embeds = None
+    negative_condition_siglip_embeds = None
+
     def __init__(self, **kwargs):
         # Initialize dataclass fields
         for name, field in self.__class__.__dataclass_fields__.items():
diff --git a/python/sglang/multimodal_gen/runtime/pipelines_core/stages/text_encoding.py b/python/sglang/multimodal_gen/runtime/pipelines_core/stages/text_encoding.py
index 4e4da5073..13db07591 100644
--- a/python/sglang/multimodal_gen/runtime/pipelines_core/stages/text_encoding.py
+++ b/python/sglang/multimodal_gen/runtime/pipelines_core/stages/text_encoding.py
@@ -63,6 +63,12 @@ class TextEncodingStage(PipelineStage):
         assert batch.prompt is not None
         prompt_text: str | list[str] = batch.prompt
 
+        # TODO: debug hard code
+        # TODO: review usage
+        num_condition_images: int = (
+            len(batch.image_path) if batch.image_path is not None else 0
+        )
+
         all_indices: list[int] = list(range(len(self.text_encoders)))
 
         prompt_embeds_list, prompt_masks_list, pooler_embeds_list = self.encode_text(
@@ -70,6 +76,7 @@ class TextEncodingStage(PipelineStage):
             server_args,
             encoder_index=all_indices,
             return_attention_mask=True,
+            num_condition_images=num_condition_images,
         )
 
         for pe in prompt_embeds_list:
@@ -89,6 +96,7 @@ class TextEncodingStage(PipelineStage):
                 server_args,
                 encoder_index=all_indices,
                 return_attention_mask=True,
+                num_condition_images=num_condition_images,
             )
 
             assert batch.negative_prompt_embeds is not None
@@ -142,6 +150,7 @@ class TextEncodingStage(PipelineStage):
         max_length: int | None = None,
         truncation: bool | None = None,
         padding: bool | str | None = None,
+        num_condition_images=0,
         return_overflowing_tokens=None,
         return_length=None,
     ):
@@ -244,6 +253,8 @@ class TextEncodingStage(PipelineStage):
                 **text_encoder_extra_arg,
             )
 
+            # TODO: refactor, ugly hard code
+            tok_kwargs["num_condition_images"] = num_condition_images
             text_inputs: dict = server_args.pipeline_config.tokenize_prompt(
                 processed_text_list, tokenizer, tok_kwargs
             ).to(target_device)
-- 
2.52.0

