From 594c1e409109929d851aa3129a5e0a9f1b373a26 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 12:26:36 +0000
Subject: [PATCH] feat: Squash PR #16647 changes

---
 .../runtime/models/dits/qwen_image.py         | 90 +++++++++++++++----
 .../runtime/models/encoders/qwen2_5vl.py      | 75 +++++++++++-----
 2 files changed, 130 insertions(+), 35 deletions(-)

diff --git a/python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py b/python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py
index 73d478dcb..725cb2382 100644
--- a/python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py
+++ b/python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py
@@ -9,7 +9,6 @@ import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from diffusers.models.attention import FeedForward
 from diffusers.models.embeddings import TimestepEmbedding, Timesteps
 from diffusers.models.modeling_outputs import Transformer2DModelOutput
 from diffusers.models.normalization import AdaLayerNormContinuous
@@ -22,7 +21,7 @@ from sglang.multimodal_gen.runtime.layers.layernorm import (
     RMSNorm,
     apply_qk_norm,
 )
-from sglang.multimodal_gen.runtime.layers.linear import ReplicatedLinear
+from sglang.multimodal_gen.runtime.layers.linear import ColumnParallelLinear
 from sglang.multimodal_gen.runtime.layers.rotary_embedding import (
     apply_flashinfer_rope_qk_inplace,
 )
@@ -54,6 +53,57 @@ def _get_qkv_projections(
     return img_query, img_key, img_value, txt_query, txt_key, txt_value
 
 
+class QwenImageTPFeedForward(nn.Module):
+    def __init__(
+        self,
+        dim: int,
+        dim_out: Optional[int] = None,
+        mult: float = 4.0,
+        activation_fn: str = "gelu-approximate",
+        bias: bool = True,
+    ):
+        super().__init__()
+        if dim_out is None:
+            dim_out = dim
+        inner_dim = int(dim * mult)
+
+        # Match diffusers FeedForward naming: net.0.proj, net.1, net.2
+        self.net = nn.ModuleList(
+            [
+                _TPProjLayer(
+                    dim,
+                    inner_dim,
+                    bias=bias,
+                ),
+                _get_ffn_activation(activation_fn),
+                ColumnParallelLinear(inner_dim, dim_out, bias=bias, gather_output=True),
+            ]
+        )
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.net[0](x)
+        x = self.net[1](x)
+        x, _ = self.net[2](x)
+        return x
+
+
+def _get_ffn_activation(activation_fn: str) -> nn.Module:
+    # Keep a simple GELU path for now; extend if new activations are needed.
+    if activation_fn == "gelu":
+        return nn.GELU()
+    return nn.GELU(approximate="tanh")
+
+
+class _TPProjLayer(nn.Module):
+    def __init__(self, in_dim: int, out_dim: int, bias: bool = True):
+        super().__init__()
+        self.proj = ColumnParallelLinear(in_dim, out_dim, bias=bias, gather_output=True)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x, _ = self.proj(x)
+        return x
+
+
 class QwenTimestepProjEmbeddings(nn.Module):
     def __init__(self, embedding_dim, use_additional_t_cond=False):
         super().__init__()
@@ -478,34 +528,44 @@ class QwenImageCrossAttention(nn.Module):
         # Use separate Q/K/V projections
         self.inner_dim = out_dim if out_dim is not None else head_dim * num_heads
         self.inner_kv_dim = self.inner_dim
-        self.to_q = ReplicatedLinear(dim, self.inner_dim, bias=True)
-        self.to_k = ReplicatedLinear(dim, self.inner_dim, bias=True)
-        self.to_v = ReplicatedLinear(dim, self.inner_dim, bias=True)
+        self.to_q = ColumnParallelLinear(
+            dim, self.inner_dim, bias=True, gather_output=True
+        )
+        self.to_k = ColumnParallelLinear(
+            dim, self.inner_dim, bias=True, gather_output=True
+        )
+        self.to_v = ColumnParallelLinear(
+            dim, self.inner_dim, bias=True, gather_output=True
+        )
 
         if self.qk_norm:
             self.norm_q = RMSNorm(head_dim, eps=eps) if qk_norm else nn.Identity()
             self.norm_k = RMSNorm(head_dim, eps=eps) if qk_norm else nn.Identity()
 
         if added_kv_proj_dim is not None:
-            self.add_q_proj = ReplicatedLinear(
-                added_kv_proj_dim, self.inner_dim, bias=True
+            self.add_q_proj = ColumnParallelLinear(
+                added_kv_proj_dim, self.inner_dim, bias=True, gather_output=True
             )
-            self.add_k_proj = ReplicatedLinear(
-                added_kv_proj_dim, self.inner_dim, bias=True
+            self.add_k_proj = ColumnParallelLinear(
+                added_kv_proj_dim, self.inner_dim, bias=True, gather_output=True
             )
-            self.add_v_proj = ReplicatedLinear(
-                added_kv_proj_dim, self.inner_dim, bias=True
+            self.add_v_proj = ColumnParallelLinear(
+                added_kv_proj_dim, self.inner_dim, bias=True, gather_output=True
             )
 
         if context_pre_only is not None and not context_pre_only:
-            self.to_add_out = ReplicatedLinear(self.inner_dim, self.dim, bias=out_bias)
+            self.to_add_out = ColumnParallelLinear(
+                self.inner_dim, self.dim, bias=out_bias, gather_output=True
+            )
         else:
             self.to_add_out = None
 
         if not pre_only:
             self.to_out = nn.ModuleList([])
             self.to_out.append(
-                ReplicatedLinear(self.inner_dim, self.dim, bias=out_bias)
+                ColumnParallelLinear(
+                    self.inner_dim, self.dim, bias=out_bias, gather_output=True
+                )
             )
         else:
             self.to_out = None
@@ -652,7 +712,7 @@ class QwenImageTransformerBlock(nn.Module):
             head_dim=attention_head_dim,
         )
         self.img_norm2 = LayerNorm(dim, eps=eps, elementwise_affine=False)
-        self.img_mlp = FeedForward(
+        self.img_mlp = QwenImageTPFeedForward(
             dim=dim, dim_out=dim, activation_fn="gelu-approximate"
         )
 
@@ -666,7 +726,7 @@ class QwenImageTransformerBlock(nn.Module):
         self.txt_norm1 = LayerNorm(dim, elementwise_affine=False, eps=eps)
         # Text doesn't need separate attention - it's handled by img_attn joint computation
         self.txt_norm2 = LayerNorm(dim, elementwise_affine=False, eps=eps)
-        self.txt_mlp = FeedForward(
+        self.txt_mlp = QwenImageTPFeedForward(
             dim=dim, dim_out=dim, activation_fn="gelu-approximate"
         )
 
diff --git a/python/sglang/multimodal_gen/runtime/models/encoders/qwen2_5vl.py b/python/sglang/multimodal_gen/runtime/models/encoders/qwen2_5vl.py
index 364b72d59..650dc2fd2 100644
--- a/python/sglang/multimodal_gen/runtime/models/encoders/qwen2_5vl.py
+++ b/python/sglang/multimodal_gen/runtime/models/encoders/qwen2_5vl.py
@@ -18,6 +18,7 @@ from transformers.utils import TransformersKwargs, is_torchdynamo_compiling
 from sglang.multimodal_gen.configs.models.encoders.qwen_image import Qwen2_5VLConfig
 from sglang.multimodal_gen.runtime.layers.attention import LocalAttention
 from sglang.multimodal_gen.runtime.layers.linear import (
+    ColumnParallelLinear,
     MergedColumnParallelLinear,
     RowParallelLinear,
 )
@@ -69,7 +70,6 @@ from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (
     Qwen2_5_VLCausalLMOutputWithPast,
     Qwen2_5_VLModelOutputWithPast,
     Qwen2_5_VLRotaryEmbedding,
-    Qwen2MLP,
     apply_multimodal_rotary_pos_emb,
     eager_attention_forward,
 )
@@ -109,17 +109,29 @@ class Qwen2_5_VLAttention(nn.Module):
                 f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                 f" and `num_heads`: {self.num_heads})."
             )
-        self.q_proj = nn.Linear(
-            self.hidden_size, self.num_heads * self.head_dim, bias=True
+        self.q_proj = ColumnParallelLinear(
+            self.hidden_size,
+            self.num_heads * self.head_dim,
+            bias=True,
+            gather_output=True,
         )
-        self.k_proj = nn.Linear(
-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True
+        self.k_proj = ColumnParallelLinear(
+            self.hidden_size,
+            self.num_key_value_heads * self.head_dim,
+            bias=True,
+            gather_output=True,
         )
-        self.v_proj = nn.Linear(
-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True
+        self.v_proj = ColumnParallelLinear(
+            self.hidden_size,
+            self.num_key_value_heads * self.head_dim,
+            bias=True,
+            gather_output=True,
         )
-        self.o_proj = nn.Linear(
-            self.num_heads * self.head_dim, self.hidden_size, bias=False
+        self.o_proj = ColumnParallelLinear(
+            self.num_heads * self.head_dim,
+            self.hidden_size,
+            bias=False,
+            gather_output=True,
         )
         self.sliding_window = (
             config.sliding_window
@@ -156,9 +168,9 @@ class Qwen2_5_VLAttention(nn.Module):
     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
         bsz, q_len, _ = hidden_states.size()
 
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
+        query_states, _ = self.q_proj(hidden_states)
+        key_states, _ = self.k_proj(hidden_states)
+        value_states, _ = self.v_proj(hidden_states)
 
         query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
@@ -201,7 +213,7 @@ class Qwen2_5_VLAttention(nn.Module):
         # )
 
         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
-        attn_output = self.o_proj(attn_output)
+        attn_output, _ = self.o_proj(attn_output)
         return attn_output
 
 
@@ -220,7 +232,12 @@ class Qwen2_5_VLDecoderLayer(nn.Module):
             )
         self.self_attn = Qwen2_5_VLAttention(config, layer_idx)
 
-        self.mlp = Qwen2MLP(config)
+        self.mlp = Qwen2_5_VLMLP(
+            in_features=config.hidden_size,
+            hidden_features=config.intermediate_size,
+            bias=getattr(config, "mlp_bias", False),
+            hidden_act=config.hidden_act,
+        )
         self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
         self.post_attention_layernorm = Qwen2RMSNorm(
             config.hidden_size, eps=config.rms_norm_eps
@@ -1121,6 +1138,11 @@ class Qwen2_5_VLForConditionalGeneration(TextEncoder):
         loaded_params: set[str] = set()
 
         params_dict = dict(self.named_parameters(remove_duplicate=False))
+        stacked_params_mapping = [
+            # (param_name, weight_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
         for name, loaded_weight in weights:
             if "rotary_emb.inv_freq" in name:
                 continue
@@ -1130,14 +1152,27 @@ class Qwen2_5_VLForConditionalGeneration(TextEncoder):
                 if not self.enable_image_understanding:
                     continue
                 name = name.replace("visual.", "model.visual.")
-            try:
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
+            loaded = False
+            for param_name, weight_name, shard_id in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                model_param_name = name.replace(weight_name, param_name)
+                if model_param_name not in params_dict:
                     continue
-                param = params_dict[name]
-            except KeyError:
-                raise
+                param = params_dict[model_param_name]
+                weight_loader = param.weight_loader
+                loaded_weight = loaded_weight.to(param.dtype)
+                weight_loader(param, loaded_weight, shard_id)
+                loaded_params.add(model_param_name)
+                loaded = True
+                break
+            if loaded:
+                continue
 
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+            param = params_dict[name]
             weight_loader = getattr(param, "weight_loader", default_weight_loader)
             loaded_weight = loaded_weight.to(param.dtype)
             weight_loader(param, loaded_weight)
-- 
2.52.0

