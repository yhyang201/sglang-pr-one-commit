From fb09d02167b8c50345710e282b96b5178b21ddf9 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:48:49 +0000
Subject: [PATCH] feat: Squash PR #16621 changes

---
 python/sglang/srt/layers/quantization/awq.py  | 2 --
 python/sglang/srt/layers/quantization/gguf.py | 5 ++---
 2 files changed, 2 insertions(+), 5 deletions(-)

diff --git a/python/sglang/srt/layers/quantization/awq.py b/python/sglang/srt/layers/quantization/awq.py
index 5497900a0..605723de0 100644
--- a/python/sglang/srt/layers/quantization/awq.py
+++ b/python/sglang/srt/layers/quantization/awq.py
@@ -67,8 +67,6 @@ elif _is_hip:
     from sglang.srt.layers.quantization.awq_triton import (
         awq_dequantize_triton as awq_dequantize,
     )
-
-    warnings.warn(f"HIP does not support fused_marlin_moe currently.")
 elif _is_xpu:
     from sgl_kernel import awq_dequantize
 
diff --git a/python/sglang/srt/layers/quantization/gguf.py b/python/sglang/srt/layers/quantization/gguf.py
index cc969557e..f5a7d61ef 100644
--- a/python/sglang/srt/layers/quantization/gguf.py
+++ b/python/sglang/srt/layers/quantization/gguf.py
@@ -3,7 +3,6 @@
 from __future__ import annotations
 
 import logging
-import warnings
 from typing import TYPE_CHECKING, Any, List, Optional
 
 import gguf
@@ -42,8 +41,6 @@ if _is_cuda:
         ggml_mul_mat_a8,
         ggml_mul_mat_vec_a8,
     )
-else:
-    warnings.warn(f"Only CUDA support GGUF quantization currently.")
 
 logger = logging.getLogger(__name__)
 
@@ -53,6 +50,8 @@ class GGUFConfig(QuantizationConfig):
 
     def __init__(self, modules_to_not_convert: list[str] | None = None) -> None:
         super().__init__()
+        if not _is_cuda:
+            raise ValueError("GGUF quantization is only supported on CUDA devices.")
         self.modules_to_not_convert = modules_to_not_convert or []
 
     def __repr__(self) -> str:
-- 
2.52.0

