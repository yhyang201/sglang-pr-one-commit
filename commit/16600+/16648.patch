From abaaa30aedc7ebef6da131fed932bc6337597b50 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 12:26:33 +0000
Subject: [PATCH] feat: Squash PR #16648 changes

---
 .../runtime/layers/attention/turbo_layer.py   | 14 +++--
 .../test/test_sparse_linear_calc.py           | 57 +++++++++++++++++++
 2 files changed, 65 insertions(+), 6 deletions(-)
 create mode 100644 python/sglang/multimodal_gen/test/test_sparse_linear_calc.py

diff --git a/python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py b/python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py
index 4d28601b0..2633953fb 100644
--- a/python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py
+++ b/python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py
@@ -437,12 +437,14 @@ class SparseLinearAttention(nn.Module):
         q = self.feature_map_q(q).contiguous().to(self.dtype)  # c_q
         k = self.feature_map_k(k).contiguous().to(self.dtype)  # c_k
 
-        def calc_linear(q, k, v):
-            kvsum = k.transpose(-1, -2) @ v
-            ksum = torch.sum(k, dim=-2, keepdim=True)
-            return (q @ kvsum) / (1e-5 + (q * ksum).sum(dim=-1, keepdim=True))
-
-        o_l = calc_linear(q, k, v)
+        def torch_calc_linear(q, k, v):
+            kv = torch.matmul(k.transpose(-1, -2), v)
+            k_sum = torch.sum(k, dim=-2, keepdim=True)
+            return torch.matmul(q, kv) / (
+                1e-5 + torch.matmul(q, k_sum.transpose(-1, -2))
+            )
+
+        o_l = torch_calc_linear(q, k, v)
 
         with torch.amp.autocast("cuda", dtype=self.dtype):
             o_l = self.proj_l(o_l)
diff --git a/python/sglang/multimodal_gen/test/test_sparse_linear_calc.py b/python/sglang/multimodal_gen/test/test_sparse_linear_calc.py
new file mode 100644
index 000000000..5d519c284
--- /dev/null
+++ b/python/sglang/multimodal_gen/test/test_sparse_linear_calc.py
@@ -0,0 +1,57 @@
+import torch
+import torch.nn.functional as F
+
+
+def original_calc_linear(q, k, v):
+    kvsum = k.transpose(-1, -2) @ v
+    ksum = torch.sum(k, dim=-2, keepdim=True)
+    return (q @ kvsum) / (1e-5 + (q * ksum).sum(dim=-1, keepdim=True))
+
+
+def torch_calc_linear(q, k, v):
+    kv = torch.matmul(k.transpose(-1, -2), v)
+    k_sum = torch.sum(k, dim=-2, keepdim=True)
+    return torch.matmul(q, kv) / (1e-5 + torch.matmul(q, k_sum.transpose(-1, -2)))
+
+
+for seed in [0, 42, 128, 1024]:
+    # set seed
+    torch.manual_seed(seed)
+
+    # test case
+    # B, H, L, D = 1, 12, 32760, 128
+    B, H, L, D = 1, 40, 32760, 128
+    q = torch.randn(B, H, L, D)
+    k = torch.randn(B, H, L, D)
+    v = torch.randn(B, H, L, D)
+    q = F.softmax(q, dim=-1)
+    k = F.softmax(k, dim=-1)
+
+    # result
+    original = original_calc_linear(q, k, v)
+    optimized = torch_calc_linear(q, k, v)
+
+    # diff
+    abs_error = torch.abs(original - optimized)
+    rel_error = abs_error / (torch.abs(original) + 1e-8)
+
+    print(f"=== seed {seed} result ===")
+    print(f"Maximum Absolute Error: {abs_error.max().item():.2e}")
+    print(f"Mean Absolute Error: {abs_error.mean().item():.2e}")
+    print(f"Maximum Relative Error: {rel_error.max().item():.2e}")
+    print(f"Mean Relative Error: {rel_error.mean().item():.2e}")
+
+    # Cosine Similarity
+    original_flat = original.flatten()
+    optimized_flat = optimized.flatten()
+    cosine_sim = torch.nn.functional.cosine_similarity(
+        original_flat, optimized_flat, dim=0
+    )
+    print(f"Cosine Similarity: {cosine_sim.item():.6f}")
+
+    # range
+    print(f"original range: [{original.min().item():.4f}, {original.max().item():.4f}]")
+    print(
+        f"optimized range: [{optimized.min().item():.4f}, {optimized.max().item():.4f}]"
+    )
+    print(f"=========================")
-- 
2.52.0

