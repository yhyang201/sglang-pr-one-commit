From 4cbe1991947e4351c5db29a58f950473987d5784 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 12:26:43 +0000
Subject: [PATCH] feat: Squash PR #16645 changes

---
 python/sglang/bench_serving.py | 48 +++++++++++++++++++++++++++++++++-
 1 file changed, 47 insertions(+), 1 deletion(-)

diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py
index a178f6f62..628908a38 100644
--- a/python/sglang/bench_serving.py
+++ b/python/sglang/bench_serving.py
@@ -1626,6 +1626,7 @@ def sample_image_requests(
     image_resolution: str,
     backend: str,
     random_image_count: bool = False,
+    skip_special_tokens: bool = False,
 ) -> List[DatasetRow]:
     """Generate requests with images.
 
@@ -1690,6 +1691,46 @@ def sample_image_requests(
 
     dataset: List[DatasetRow] = []
     total_image_bytes = 0
+
+    special_tokens = None
+    if (
+        skip_special_tokens
+        and hasattr(processor.tokenizer, "all_special_ids")
+        and processor.tokenizer.all_special_ids is not None
+    ):
+        special_tokens = set(processor.tokenizer.all_special_ids)
+
+    # Extract special token strings and convert to token_id
+    special_token_strings = []
+    special_token_ids = set()
+
+    if hasattr(processor.tokenizer, "special_tokens"):
+        # Extract all special tokens from tokenizer.special_tokens dictionary
+        for key, value in processor.tokenizer.special_tokens.items():
+            if isinstance(value, str):
+                special_token_strings.append(value)
+            elif isinstance(value, list):
+                special_token_strings.extend(value)
+
+    # Also extract from additional_special_tokens
+    if hasattr(processor.tokenizer, "additional_special_tokens"):
+        special_token_strings.extend(processor.tokenizer.additional_special_tokens)
+
+    # Convert to token_id
+    if special_token_strings and hasattr(processor.tokenizer, "convert_tokens_to_ids"):
+        special_token_ids = set(
+            processor.tokenizer.convert_tokens_to_ids(special_token_strings)
+        )
+
+    # Merge all_special_ids and special_token_ids converted from strings
+    if special_tokens is not None and special_token_ids:
+        merged_special_tokens = special_tokens | special_token_ids
+    elif special_tokens is not None:
+        merged_special_tokens = special_tokens
+    elif special_token_ids:
+        merged_special_tokens = special_token_ids
+    else:
+        merged_special_tokens = None
     for i in range(num_requests):
         # Get the number of images for this request
         request_image_count = int(image_counts[i])
@@ -1699,6 +1740,7 @@ def sample_image_requests(
             processor.tokenizer,
             processor.image_token_id if hasattr(processor, "image_token_id") else None,
             int(input_lens[i]),
+            merged_special_tokens,
         )
 
         # Generate image list
@@ -1748,11 +1790,15 @@ def gen_prompt(tokenizer, token_num):
     return tokenizer.decode(selected_tokens)
 
 
-def gen_mm_prompt(tokenizer, image_pad_id, token_num):
+def gen_mm_prompt(tokenizer, image_pad_id, token_num, special_tokens):
     """Generate a random prompt of specified token length using tokenizer vocabulary."""
     all_available_tokens = list(tokenizer.get_vocab().values())
     if image_pad_id:
         all_available_tokens.remove(image_pad_id)
+    if special_tokens:
+        all_available_tokens = [
+            t for t in all_available_tokens if t not in special_tokens
+        ]
     selected_tokens = random.choices(all_available_tokens, k=token_num)
     return tokenizer.decode(selected_tokens)
 
-- 
2.52.0

