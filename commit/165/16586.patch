From 81ab995817dcded6f98a1357d502bd27f4916979 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:50:40 +0000
Subject: [PATCH] feat: Squash PR #16586 changes

---
 .../runtime/layers/lora/linear.py             | 289 ++++++++++++------
 1 file changed, 201 insertions(+), 88 deletions(-)

diff --git a/python/sglang/multimodal_gen/runtime/layers/lora/linear.py b/python/sglang/multimodal_gen/runtime/layers/lora/linear.py
index ff45b914b..639480b07 100644
--- a/python/sglang/multimodal_gen/runtime/layers/lora/linear.py
+++ b/python/sglang/multimodal_gen/runtime/layers/lora/linear.py
@@ -3,6 +3,8 @@
 # SPDX-License-Identifier: Apache-2.0
 # Code adapted from SGLang https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/lora/layers.py
 
+import math
+from collections import defaultdict
 
 import torch
 from torch import nn
@@ -31,8 +33,11 @@ from sglang.multimodal_gen.runtime.layers.linear import (
 from sglang.multimodal_gen.runtime.layers.vocab_parallel_embedding import (
     VocabParallelEmbedding,
 )
+from sglang.multimodal_gen.runtime.utils.logging_utils import init_logger
 from sglang.multimodal_gen.utils import get_mixed_precision_state
 
+logger = init_logger(__name__)
+
 torch._dynamo.config.recompile_limit = 16
 
 
@@ -43,6 +48,7 @@ class BaseLayerWithLoRA(nn.Module):
         base_layer: nn.Module,
         lora_rank: int | None = None,
         lora_alpha: int | None = None,
+        training_mode: bool = False,
     ):
         super().__init__()
         self.base_layer: nn.Module = base_layer
@@ -51,23 +57,50 @@ class BaseLayerWithLoRA(nn.Module):
         self.cpu_weight = base_layer.weight.to("cpu")
         # indicates adapter weights don't contain this layer
         # (which shouldn't normally happen, but we want to separate it from the case of erroneous merging)
-        # Default to True to prevent using uninitialized weights; set to False when weights are loaded
-        self.disable_lora: bool = True
+        self.disable_lora: bool = False
         self.lora_rank = lora_rank
         self.lora_alpha = lora_alpha
+        self.training_mode = training_mode
         self.lora_path: str | None = None
-        self.strength: float = 1.0
-
-        self.lora_A = None
-        self.lora_B = None
 
-    @property
-    def weight(self):
-        return self.base_layer.weight
-
-    @property
-    def bias(self):
-        return getattr(self.base_layer, "bias", None)
+        # Multi-LoRA batching support
+        self.use_multi_lora: bool = False
+        self.active_lora_indices: torch.Tensor | None = None
+        self.lora_weights_pool: dict[str, tuple[torch.Tensor, torch.Tensor]] = {}
+        self.lora_nickname_to_index: dict[str, int] = {}
+        self.lora_adapter_configs: dict[str, dict] = {}
+        self.layer_name: str | None = None
+
+        if training_mode:
+            assert (
+                self.lora_rank is not None
+            ), "LoRA rank  must be set for training mode"
+            if self.lora_rank is None or self.lora_alpha is None:
+                self.lora_alpha = lora_rank
+            self.base_layer.requires_grad_(False)
+            in_dim = self.base_layer.weight.shape[1]
+            out_dim = self.base_layer.weight.shape[0]
+            self.lora_A = nn.Parameter(
+                torch.zeros(
+                    self.lora_rank,
+                    in_dim,
+                    device=self.base_layer.weight.device,
+                    dtype=self.base_layer.weight.dtype,
+                )
+            )
+            self.lora_B = nn.Parameter(
+                torch.zeros(
+                    out_dim,
+                    self.lora_rank,
+                    device=self.base_layer.weight.device,
+                    dtype=self.base_layer.weight.dtype,
+                )
+            )
+            torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
+            torch.nn.init.zeros_(self.lora_B)
+        else:
+            self.lora_A = None
+            self.lora_B = None
 
     @torch.compile()
     def forward(self, x: torch.Tensor) -> torch.Tensor:
@@ -85,12 +118,11 @@ class BaseLayerWithLoRA(nn.Module):
                 delta = delta * (
                     self.lora_alpha / self.lora_rank  # type: ignore
                 )  # type: ignore
-            delta = delta * self.strength
             out, output_bias = self.base_layer(x)
             return out + delta, output_bias
         else:
             out, output_bias = self.base_layer(x)
-            return out, output_bias
+            return out.to(x), output_bias
 
     def slice_lora_a_weights(self, A: torch.Tensor) -> torch.Tensor:
         return A
@@ -98,27 +130,53 @@ class BaseLayerWithLoRA(nn.Module):
     def slice_lora_b_weights(self, B: torch.Tensor) -> torch.Tensor:
         return B
 
+    def _index_to_nickname(self, lora_idx: int) -> str | None:
+        """Convert LoRA index to nickname using reverse lookup."""
+        for nickname, idx in self.lora_nickname_to_index.items():
+            if idx == lora_idx:
+                return nickname
+        return None
+
+    def set_multi_lora_state(
+        self,
+        active_lora_indices: torch.Tensor,
+        lora_weights_pool: dict[str, tuple[torch.Tensor, torch.Tensor]],
+        lora_nickname_to_index: dict[str, int],
+        lora_adapter_configs: dict[str, dict],
+    ) -> None:
+        """Set the multi-LoRA state for this layer."""
+        self.use_multi_lora = True
+        self.active_lora_indices = active_lora_indices
+        self.lora_weights_pool = lora_weights_pool
+        self.lora_nickname_to_index = lora_nickname_to_index
+        self.lora_adapter_configs = lora_adapter_configs
+
+    def clear_multi_lora_state(self) -> None:
+        """Clear the multi-LoRA state."""
+        self.use_multi_lora = False
+        self.active_lora_indices = None
+        self.lora_weights_pool = {}
+        self.lora_nickname_to_index = {}
+        self.lora_adapter_configs = {}
+
     def set_lora_weights(
         self,
         A: torch.Tensor,
         B: torch.Tensor,
+        training_mode: bool = False,
         lora_path: str | None = None,
-        strength: float = 1.0,
     ) -> None:
         self.lora_A = torch.nn.Parameter(
             A
         )  # share storage with weights in the pipeline
         self.lora_B = torch.nn.Parameter(B)
         self.disable_lora = False
-        self.strength = strength
-        self.merge_lora_weights()
+        if not training_mode:
+            self.merge_lora_weights()
         self.lora_path = lora_path
 
     @torch.no_grad()
-    def merge_lora_weights(self, strength: float | None = None) -> None:
-        if strength is not None:
-            self.strength = strength
-
+    def merge_lora_weights(self) -> None:
         if self.disable_lora:
             return
 
@@ -143,14 +201,9 @@ class BaseLayerWithLoRA(nn.Module):
             data = self.base_layer.weight.data.to(
                 get_local_torch_device()
             ).full_tensor()
-            lora_delta = self.slice_lora_b_weights(self.lora_B).to(
+            data += self.slice_lora_b_weights(self.lora_B).to(
                 data
             ) @ self.slice_lora_a_weights(self.lora_A).to(data)
-            # Apply lora_alpha / lora_rank scaling for consistency with forward()
-            if self.lora_alpha is not None and self.lora_rank is not None:
-                if self.lora_alpha != self.lora_rank:
-                    lora_delta = lora_delta * (self.lora_alpha / self.lora_rank)
-            data += self.strength * lora_delta
             unsharded_base_layer.weight = nn.Parameter(data.to(current_device))
             if isinstance(getattr(self.base_layer, "bias", None), DTensor):
                 unsharded_base_layer.bias = nn.Parameter(
@@ -173,14 +226,9 @@ class BaseLayerWithLoRA(nn.Module):
         else:
             current_device = self.base_layer.weight.data.device
             data = self.base_layer.weight.data.to(get_local_torch_device())
-            lora_delta = self.slice_lora_b_weights(
+            data += self.slice_lora_b_weights(
                 self.lora_B.to(data)
             ) @ self.slice_lora_a_weights(self.lora_A.to(data))
-            # Apply lora_alpha / lora_rank scaling for consistency with forward()
-            if self.lora_alpha is not None and self.lora_rank is not None:
-                if self.lora_alpha != self.lora_rank:
-                    lora_delta = lora_delta * (self.lora_alpha / self.lora_rank)
-            data += self.strength * lora_delta
             self.base_layer.weight.data = data.to(current_device, non_blocking=True)
 
         self.merged = True
@@ -238,8 +286,9 @@ class ColumnParallelLinearWithLoRA(BaseLayerWithLoRA):
         base_layer: ColumnParallelLinear,
         lora_rank: int | None = None,
         lora_alpha: int | None = None,
+        training_mode: bool = False,
     ) -> None:
-        super().__init__(base_layer, lora_rank, lora_alpha)
+        super().__init__(base_layer, lora_rank, lora_alpha, training_mode)
 
     def forward(self, input_: torch.Tensor) -> torch.Tensor:
         # duplicate the logic in ColumnParallelLinear
@@ -273,8 +322,9 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
         base_layer: MergedColumnParallelLinear,
         lora_rank: int | None = None,
         lora_alpha: int | None = None,
+        training_mode: bool = False,
     ) -> None:
-        super().__init__(base_layer, lora_rank, lora_alpha)
+        super().__init__(base_layer, lora_rank, lora_alpha, training_mode)
 
     def slice_lora_a_weights(self, A: torch.Tensor) -> torch.Tensor:
         return A.to(self.base_layer.weight)
@@ -295,8 +345,9 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
         base_layer: QKVParallelLinear,
         lora_rank: int | None = None,
         lora_alpha: int | None = None,
+        training_mode: bool = False,
     ) -> None:
-        super().__init__(base_layer, lora_rank, lora_alpha)
+        super().__init__(base_layer, lora_rank, lora_alpha, training_mode)
 
     def slice_lora_a_weights(self, A: torch.Tensor) -> torch.Tensor:
         return A
@@ -322,14 +373,94 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
 
 
 class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
+    """
+    Row parallel linear layer with LoRA support.
+
+    In tensor parallelism, RowParallel splits the input dimension.
+    For LoRA: A is sliced along input dim, B is full.
+    The LoRA delta must be all-reduced across TP ranks.
+    """
 
     def __init__(
         self,
         base_layer: RowParallelLinear,
         lora_rank: int | None = None,
         lora_alpha: int | None = None,
+        training_mode: bool = False,
     ) -> None:
-        super().__init__(base_layer, lora_rank, lora_alpha)
+        super().__init__(base_layer, lora_rank, lora_alpha, training_mode)
+
+    def _apply_multi_lora(
+        self, x: torch.Tensor, base_out: torch.Tensor, output_bias
+    ) -> tuple[torch.Tensor, torch.Tensor]:
+        """
+        Apply LoRA adaptively per sample in batch for RowParallel.
+
+        For RowParallel:
+        - LoRA_A is sliced along input dim (same as input sharding)
+        - LoRA_B is NOT sliced
+        - delta = x_sharded @ A_sliced.T @ B.T produces a PARTIAL delta
+        - The partial delta is added to the sharded base output
+        - The combined output will be all-reduced in forward() along with
+          all other partial results, effectively summing the LoRA deltas.
+
+        Note: We do NOT all-reduce the delta here. The final all-reduce in
+        forward() handles summing partial deltas from all ranks.
+        """
+        if self.active_lora_indices is None or len(self.lora_weights_pool) == 0:
+            return base_out, output_bias
+
+        device = x.device
+
+        # Group requests by LoRA to minimize computation
+        lora_groups: dict[int, list[int]] = defaultdict(list)
+        for i, lora_idx in enumerate(self.active_lora_indices.cpu().tolist()):
+            lora_groups[lora_idx].append(i)
+
+        delta = torch.zeros_like(base_out)
+
+        for lora_idx, sample_indices in lora_groups.items():
+            if lora_idx < 0:
+                continue
+
+            lora_nickname = self._index_to_nickname(lora_idx)
+            if lora_nickname is None:
+                logger.warning(
+                    "LoRA index %d not found in nickname mapping for layer %s",
+                    lora_idx,
+                    self.layer_name or self.__class__.__name__,
+                )
+                continue
+            if lora_nickname not in self.lora_weights_pool:
+                logger.warning(
+                    "LoRA adapter '%s' not found in weights pool for layer %s",
+                    lora_nickname,
+                    self.layer_name or self.__class__.__name__,
+                )
+                continue
+
+            lora_A, lora_B = self.lora_weights_pool[lora_nickname]
+            lora_A = lora_A.to(device, non_blocking=True)
+            lora_B = lora_B.to(device, non_blocking=True)
+
+            # Slice for tensor parallelism - A is sliced, B is full
+            lora_A_sliced = self.slice_lora_a_weights(lora_A)
+            lora_B_sliced = self.slice_lora_b_weights(lora_B)
+
+            x_group = x[sample_indices]
+            delta_group = x_group @ lora_A_sliced.T @ lora_B_sliced.T
+
+            adapter_config = self.lora_adapter_configs.get(lora_nickname, {})
+            alpha = adapter_config.get("alpha", self.lora_alpha or 16.0)
+            rank = adapter_config.get("rank", self.lora_rank or 16)
+            if alpha != rank:
+                delta_group = delta_group * (alpha / rank)
+
+            delta[sample_indices] = delta_group
+
+        # Do NOT all-reduce here - the final all-reduce in forward() will
+        # sum the partial deltas from all ranks together with base output.
+        return base_out + delta, output_bias
 
     def forward(self, input_: torch.Tensor):
         # duplicate the logic in RowParallelLinear
@@ -341,10 +472,37 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
                 input_, num_partitions=self.base_layer.tp_size
             )
             input_parallel = splitted_input[tp_rank].contiguous()
+
         output_parallel = self.base_layer.quant_method.apply(
             self.base_layer, input_parallel
         )
 
+        # Handle multi-LoRA mode
+        if (
+            self.use_multi_lora
+            and self.active_lora_indices is not None
+            and not self.disable_lora
+        ):
+            # For multi-LoRA in RowParallel, we need to:
+            # 1. Compute the LoRA delta on sharded input
+            # 2. All-reduce the delta across TP ranks
+            # 3. Add to base output (which will also be all-reduced)
+            #
+            # Note: We handle all-reduce in _apply_multi_lora for the delta
+            # The base output will be all-reduced below as normal
+            output_bias = (
+                self.base_layer.bias if self.base_layer.skip_bias_add else None
+            )
+
+            # Apply multi-LoRA with all-reduce for RowParallel
+            output_parallel, output_bias = self._apply_multi_lora(
+                input_parallel, output_parallel, output_bias
+            )
+
+        # Legacy single-LoRA handling
+        elif hasattr(self, "set_lora") and self.set_lora:
+            output_parallel = self.apply_lora(output_parallel, input_parallel)
+
         if self.base_layer.reduce_results and self.base_layer.tp_size > 1:
             output_ = tensor_model_parallel_all_reduce(output_parallel)
         else:
@@ -374,58 +532,13 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
         return B
 
 
-class LinearWithLoRA(BaseLayerWithLoRA):
-    """
-    Wrapper for standard torch.nn.Linear to support LoRA.
-    Unlike custom LinearBase classes, nn.Linear.forward() returns a single tensor,
-    not a tuple of (output, bias).
-    """
-
-    def __init__(
-        self,
-        base_layer: nn.Linear,
-        lora_rank: int | None = None,
-        lora_alpha: int | None = None,
-    ) -> None:
-        super().__init__(base_layer, lora_rank, lora_alpha)
-
-    @torch.compile()
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        lora_A = self.lora_A
-        lora_B = self.lora_B
-        if isinstance(self.lora_B, DTensor):
-            lora_B = self.lora_B.to_local()
-            lora_A = self.lora_A.to_local()
-
-        if not self.merged and not self.disable_lora:
-            lora_A_sliced = self.slice_lora_a_weights(lora_A.to(x, non_blocking=True))
-            lora_B_sliced = self.slice_lora_b_weights(lora_B.to(x, non_blocking=True))
-            delta = x @ lora_A_sliced.T @ lora_B_sliced.T
-            if self.lora_alpha != self.lora_rank:
-                delta = delta * (
-                    self.lora_alpha / self.lora_rank  # type: ignore
-                )  # type: ignore
-            delta = delta * self.strength
-            # nn.Linear.forward() returns a single tensor, not a tuple
-            out = self.base_layer(x)
-            return out + delta
-        else:
-            # nn.Linear.forward() returns a single tensor
-            out = self.base_layer(x)
-            return out
-
-
-def wrap_with_lora_layer(
+def get_lora_layer(
     layer: nn.Module,
     lora_rank: int | None = None,
     lora_alpha: int | None = None,
+    training_mode: bool = False,
 ) -> BaseLayerWithLoRA | None:
-    """
-    transform the given layer to its corresponding LoRA layer
-    """
-    supported_layer_types: dict[
-        type[LinearBase] | type[nn.Linear], type[BaseLayerWithLoRA]
-    ] = {
+    supported_layer_types: dict[type[LinearBase], type[BaseLayerWithLoRA]] = {
         # the order matters
         # VocabParallelEmbedding: VocabParallelEmbeddingWithLoRA,
         QKVParallelLinear: QKVParallelLinearWithLoRA,
@@ -433,14 +546,14 @@ def wrap_with_lora_layer(
         ColumnParallelLinear: ColumnParallelLinearWithLoRA,
         RowParallelLinear: RowParallelLinearWithLoRA,
         ReplicatedLinear: BaseLayerWithLoRA,
-        nn.Linear: LinearWithLoRA,
     }
     for src_layer_type, lora_layer_type in supported_layer_types.items():
-        if isinstance(layer, src_layer_type):  # type: ignore[arg-type]
+        if isinstance(layer, src_layer_type):  # pylint: disable=unidiomatic-typecheck
             ret = lora_layer_type(
                 layer,
                 lora_rank=lora_rank,
                 lora_alpha=lora_alpha,
+                training_mode=training_mode,
             )
             return ret
     return None
-- 
2.52.0

