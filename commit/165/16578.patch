From 092f851492323e359c2aafd68a4b4243fe1463f6 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:51:11 +0000
Subject: [PATCH] feat: Squash PR #16578 changes

---
 python/sglang/srt/layers/dp_attention.py | 6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

diff --git a/python/sglang/srt/layers/dp_attention.py b/python/sglang/srt/layers/dp_attention.py
index 6c2db5f26..395bc0a05 100644
--- a/python/sglang/srt/layers/dp_attention.py
+++ b/python/sglang/srt/layers/dp_attention.py
@@ -61,12 +61,14 @@ class DpPaddingMode(IntEnum):
     def get_dp_padding_mode(
         cls, is_extend_in_batch, global_num_tokens: List[int]
     ) -> DpPaddingMode:
-        if is_extend_in_batch:
+        sum_len = sum(global_num_tokens)
+        avg_len = sum_len / len(global_num_tokens)
+        # Avoid SUM_LEN mode in prefill when token counts are balanced across ranks.
+        if is_extend_in_batch and any(gt < avg_len * 0.8 for gt in global_num_tokens):
             return DpPaddingMode.SUM_LEN
 
         # we choose the mode that minimizes the communication cost
         max_len = max(global_num_tokens)
-        sum_len = sum(global_num_tokens)
         if sum_len * 2 > max_len * get_attention_dp_size():
             return cls.MAX_LEN
         else:
-- 
2.52.0

