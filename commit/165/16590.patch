From 9d4785f9c2d6e4a0e81dfa8632593437b6de5929 Mon Sep 17 00:00:00 2001
From: GitHub Action <action@github.com>
Date: Wed, 7 Jan 2026 05:50:25 +0000
Subject: [PATCH] feat: Squash PR #16590 changes

---
 python/sglang/srt/layers/communicator.py      | 11 ++++++++-
 .../srt/layers/moe/fused_moe_triton/layer.py  | 24 +++++++++++++------
 .../layers/moe/token_dispatcher/standard.py   | 11 ++++++---
 python/sglang/srt/models/deepseek_v2.py       | 19 ++++++++++++---
 python/sglang/srt/server_args.py              | 12 ++++++++++
 5 files changed, 63 insertions(+), 14 deletions(-)

diff --git a/python/sglang/srt/layers/communicator.py b/python/sglang/srt/layers/communicator.py
index 15df851eb..0419cc05b 100644
--- a/python/sglang/srt/layers/communicator.py
+++ b/python/sglang/srt/layers/communicator.py
@@ -224,6 +224,7 @@ class _LayerModeComputationContext:
     is_layer_sparse: bool
     is_previous_layer_sparse: Optional[bool]
     is_next_layer_sparse: Optional[bool]
+    is_nextn: bool = False
 
     def previous_layer(self):
         assert self.is_previous_layer_sparse is not None
@@ -244,6 +245,7 @@ class LayerScatterModes:
     mlp_mode: ScatterMode
     middle_residual_mode: ScatterMode
     layer_output_mode: ScatterMode
+    is_nextn: bool = False
 
     @classmethod
     def init_new(cls, **kwargs):
@@ -254,6 +256,7 @@ class LayerScatterModes:
             mlp_mode=cls._compute_mlp_mode(context),
             middle_residual_mode=cls._compute_middle_residual_mode(context),
             layer_output_mode=cls._compute_layer_output_mode(context),
+            is_nextn=context.is_nextn,
         )
 
     @classmethod
@@ -271,6 +274,7 @@ class LayerScatterModes:
                     # Token dispatch/combine will be handled outside of LayerCommunicator for these modes.
                     not get_moe_a2a_backend().is_none()
                     or should_use_flashinfer_cutlass_moe_fp4_allgather()
+                    or enable_nextn_moe_sparse_fully_dp(is_nextn=context.is_nextn)
                 )
                 else ScatterMode.FULL
             )
@@ -317,6 +321,10 @@ def enable_moe_dense_fully_dp():
     return get_global_server_args().moe_dense_tp_size == 1
 
 
+def enable_nextn_moe_sparse_fully_dp(is_nextn: bool):
+    return is_nextn and get_global_server_args().speculative_moe_tp_ep_size == 1
+
+
 class LayerCommunicator:
     def __init__(
         self,
@@ -327,6 +335,7 @@ class LayerCommunicator:
         allow_reduce_scatter: bool = False,
         is_last_layer: bool = False,
         qkv_latent_func: Optional[Callable] = None,
+        is_nextn: bool = False,
     ):
         self.layer_scatter_modes = layer_scatter_modes
         self.input_layernorm = input_layernorm
@@ -334,7 +343,7 @@ class LayerCommunicator:
         self.allow_reduce_scatter = allow_reduce_scatter
         self.is_last_layer = is_last_layer
         self.qkv_latent_func = qkv_latent_func
-
+        self.is_nextn = is_nextn
         self._context = CommunicateContext.init_new()
         self._post_init_communicate()
         self._speculative_algo = SpeculativeAlgorithm.from_string(
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
index 839463518..c94c8b325 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py
@@ -24,6 +24,7 @@ from sglang.srt.distributed.device_communicators.pynccl_allocator import (
     use_symmetric_memory,
 )
 from sglang.srt.eplb.expert_location import get_global_expert_location_metadata
+from sglang.srt.layers.communicator import enable_nextn_moe_sparse_fully_dp
 from sglang.srt.layers.dp_attention import is_allocation_symmetric
 from sglang.srt.layers.moe import (
     MoeRunnerConfig,
@@ -88,10 +89,12 @@ _use_aiter = get_bool_env_var("SGLANG_USE_AITER") and _is_hip
 logger = logging.getLogger(__name__)
 
 
-def create_moe_dispatcher(moe_runner_config: MoeRunnerConfig) -> BaseDispatcher:
+def create_moe_dispatcher(
+    moe_runner_config: MoeRunnerConfig, is_nextn: Optional[bool] = None
+) -> BaseDispatcher:
     a2a_backend = get_moe_a2a_backend()
     if a2a_backend.is_none():
-        return StandardDispatcher(moe_runner_config)
+        return StandardDispatcher(moe_runner_config, is_nextn)
     elif a2a_backend.is_deepep() or a2a_backend.is_mooncake():
         return MaybeTboDeepEPDispatcher(
             group=get_tp_group().device_group,
@@ -173,6 +176,7 @@ class FusedMoE(torch.nn.Module):
         with_bias=False,
         routing_method_type: Optional[RoutingMethodType] = None,
         is_gated: bool = True,
+        is_nextn: bool = False,
     ):
         super().__init__()
         if params_dtype is None:
@@ -187,10 +191,16 @@ class FusedMoE(torch.nn.Module):
         self.enable_flashinfer_cutlass_moe = (
             get_moe_runner_backend().is_flashinfer_cutlass()
         )
-        self.moe_ep_size = get_moe_expert_parallel_world_size()
-        self.moe_ep_rank = get_moe_expert_parallel_rank()
-        self.moe_tp_size = get_moe_tensor_parallel_world_size()
-        self.moe_tp_rank = get_moe_tensor_parallel_rank()
+        if enable_nextn_moe_sparse_fully_dp(is_nextn):
+            self.moe_ep_size = 1
+            self.moe_ep_rank = 0
+            self.moe_tp_size = 1
+            self.moe_tp_rank = 0
+        else:
+            self.moe_ep_size = get_moe_expert_parallel_world_size()
+            self.moe_ep_rank = get_moe_expert_parallel_rank()
+            self.moe_tp_size = get_moe_tensor_parallel_world_size()
+            self.moe_tp_rank = get_moe_tensor_parallel_rank()
         assert (num_experts - num_fused_shared_experts) % self.moe_ep_size == 0
         self.num_local_experts = (
             num_experts - num_fused_shared_experts
@@ -268,7 +278,7 @@ class FusedMoE(torch.nn.Module):
         )
 
         self.quant_method.create_moe_runner(self, self.moe_runner_config)
-        self.dispatcher = create_moe_dispatcher(self.moe_runner_config)
+        self.dispatcher = create_moe_dispatcher(self.moe_runner_config, is_nextn)
 
         self.should_fuse_routed_scaling_factor_in_topk = isinstance(
             self.quant_method, ModelOptNvFp4FusedMoEMethod
diff --git a/python/sglang/srt/layers/moe/token_dispatcher/standard.py b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
index 2c959c799..ab1997b4c 100644
--- a/python/sglang/srt/layers/moe/token_dispatcher/standard.py
+++ b/python/sglang/srt/layers/moe/token_dispatcher/standard.py
@@ -12,6 +12,7 @@ from sglang.srt.distributed import (
 from sglang.srt.distributed.device_communicators.pynccl_allocator import (
     use_symmetric_memory,
 )
+from sglang.srt.layers.communicator import enable_nextn_moe_sparse_fully_dp
 from sglang.srt.layers.dp_attention import (
     get_dp_global_num_tokens,
     get_local_dp_buffer,
@@ -80,9 +81,14 @@ assert isinstance(StandardCombineInput, CombineInput)
 
 class StandardDispatcher(BaseDispatcher):
 
-    def __init__(self, moe_runner_config: MoeRunnerConfig):
+    def __init__(self, moe_runner_config: MoeRunnerConfig, is_nextn: bool):
         super().__init__()
-        self.moe_ep_size = get_moe_expert_parallel_world_size()
+        if enable_nextn_moe_sparse_fully_dp(is_nextn):
+            self.moe_ep_size = 1
+            self.moe_ep_rank = 0
+        else:
+            self.moe_ep_size = get_moe_expert_parallel_world_size()
+            self.moe_ep_rank = get_moe_expert_parallel_rank()
         self.enable_flashinfer_cutlass_moe = (
             get_moe_runner_backend().is_flashinfer_cutlass()
         )
@@ -91,7 +97,6 @@ class StandardDispatcher(BaseDispatcher):
         self.num_local_routed_experts = (
             moe_runner_config.num_local_experts - self.num_local_shared_experts
         )
-        self.moe_ep_rank = get_moe_expert_parallel_rank()
         self.local_expert_mapping = None
 
     def dispatch(
diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index ed8cc7ada..a28da00f3 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -74,6 +74,7 @@ from sglang.srt.layers.communicator import (
     LayerCommunicator,
     LayerScatterModes,
     enable_moe_dense_fully_dp,
+    enable_nextn_moe_sparse_fully_dp,
     get_attn_tp_context,
 )
 from sglang.srt.layers.communicator_nsa_cp import NSACPLayerCommunicator
@@ -616,8 +617,12 @@ class DeepseekV2MoE(nn.Module):
         is_nextn: bool = False,
     ):
         super().__init__()
-        self.tp_size = get_tensor_model_parallel_world_size()
-        self.moe_ep_size = get_moe_expert_parallel_world_size()
+        if enable_nextn_moe_sparse_fully_dp(is_nextn):
+            self.tp_size = 1
+            self.moe_ep_size = 1
+        else:
+            self.tp_size = get_tensor_model_parallel_world_size()
+            self.moe_ep_size = get_moe_expert_parallel_world_size()
         self.routed_scaling_factor = config.routed_scaling_factor
         self.n_shared_experts = config.n_shared_experts
         self.num_fused_shared_experts = (
@@ -660,7 +665,11 @@ class DeepseekV2MoE(nn.Module):
         self.experts = get_moe_impl_class(quant_config)(
             num_experts=config.n_routed_experts
             + self.num_fused_shared_experts
-            + get_global_server_args().ep_num_redundant_experts,
+            + (
+                0
+                if enable_nextn_moe_sparse_fully_dp(is_nextn)
+                else get_global_server_args().ep_num_redundant_experts
+            ),
             num_fused_shared_experts=self.num_fused_shared_experts,
             top_k=config.num_experts_per_tok + self.num_fused_shared_experts,
             hidden_size=config.hidden_size,
@@ -672,6 +681,7 @@ class DeepseekV2MoE(nn.Module):
                 config, "routing_method_type", RoutingMethodType.DeepSeekV3
             ),
             prefix=add_prefix("experts", prefix),
+            is_nextn=is_nextn,
         )
 
         self.topk = TopK(
@@ -715,6 +725,7 @@ class DeepseekV2MoE(nn.Module):
                     if get_moe_a2a_backend().is_deepep()
                     or get_moe_a2a_backend().is_mooncake()
                     or get_moe_a2a_backend().is_ascend_fuseep()
+                    or enable_nextn_moe_sparse_fully_dp(is_nextn)
                     or should_use_flashinfer_cutlass_moe_fp4_allgather()
                     else {}
                 ),
@@ -2791,6 +2802,7 @@ class DeepseekV2DecoderLayer(nn.Module):
             is_layer_sparse=self.is_layer_sparse,
             is_previous_layer_sparse=is_previous_layer_sparse,
             is_next_layer_sparse=is_next_layer_sparse,
+            is_nextn=is_nextn,
         )
 
         if self.is_layer_sparse:
@@ -2843,6 +2855,7 @@ class DeepseekV2DecoderLayer(nn.Module):
                     is_nextn or (self.layer_id == self.config.num_hidden_layers - 1)
                 ),
                 qkv_latent_func=self.self_attn.prepare_qkv_latent,
+                is_nextn=is_nextn,
             )
 
     def _is_layer_sparse(self, layer_id: int, is_nextn: bool) -> bool:
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index a2b26e0e0..dedf91302 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -439,6 +439,7 @@ class ServerArgs:
     speculative_ngram_branch_length: int = 18
     speculative_ngram_capacity: int = 10 * 1000 * 1000
     enable_multi_layer_eagle: bool = False
+    speculative_moe_tp_ep_size: Optional[int] = None
 
     # Expert parallelism
     ep_size: int = 1
@@ -3523,6 +3524,12 @@ class ServerArgs:
             default=ServerArgs.speculative_draft_model_quantization,
             help="The quantization method for speculative model.",
         )
+        parser.add_argument(
+            "--speculative-moe-tp-ep-size",
+            type=int,
+            default=ServerArgs.speculative_moe_tp_ep_size,
+            help="TP/EP-MOE size for EAGLE speculative decoding MoE layers only. Default is None, which means use the same size as the normal MoE layers.",
+        )
 
         # Speculative decoding (ngram)
         parser.add_argument(
@@ -4639,6 +4646,11 @@ class ServerArgs:
             None,
         }, "moe_dense_tp_size only support 1 and None currently"
 
+        assert self.speculative_moe_tp_ep_size in {
+            1,
+            None,
+        }, "speculative_moe_tp_ep_size only support 1 and None currently"
+
         # Check served model name to not have colon as it is reserved for LoRA adapter syntax
         assert ":" not in self.served_model_name, (
             "served_model_name cannot contain a colon (':') character. "
-- 
2.52.0

